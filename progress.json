[
  {
    "id": "44118023",
    "title": "Show HN: I rewrote my Mac Electron app in Rust",
    "url": "https://desktopdocs.com/?v=2025",
    "summary": "Desktop Docs is a Mac application that uses AI to provide advanced image and video search capabilities. It analyzes the content of files, not just filenames, allowing users to search using natural language descriptions or reference images. The app is designed for professionals like video editors, photographers, and social media managers who need to quickly find specific files within large libraries.\n\nKey features include content-based matching, image similarity search, duplicate detection, and cross-format support. All processing happens locally on the user's Mac, ensuring privacy and eliminating the need for cloud uploads. The app supports a wide range of image and video formats including HEIC, JPG, PNG, GIF, BMP, WEBP, MP4, AVI, MOV, MKV, and WEBM.\n\nDesktop Docs requires a Mac with Apple Silicon (M1, M2, or M3) and is sold as a one-time purchase of $99, with no subscription required. It's aimed at helping users find files faster, do less administrative work, and transform digital chaos into an organized knowledge base. Testimonials highlight time savings and improved file organization. The application indexes unlimited files, allows for smart searching within various document types, and enables quick edits.\n",
    "chinese_title": "Show HN: 我用 Rust 重写了我的 Mac Electron 应用",
    "chinese_summary": "Desktop Docs：一款利用人工智能提供高级图像和视频搜索功能的Mac应用程序。它不仅分析文件名，还分析文件内容，允许用户使用自然语言描述或参考图像进行搜索。该应用专为视频编辑、摄影师和社交媒体经理等需要快速查找大型媒体库中特定文件的专业人士设计。\n\n主要功能包括基于内容的匹配、图像相似度搜索、重复检测和跨格式支持。所有处理都在用户的Mac本地进行，确保隐私并无需上传到云端。该应用支持多种图像和视频格式，包括HEIC、JPG、PNG、GIF、BMP、WEBP、MP4、AVI、MOV、MKV和WEBM。\n\nDesktop Docs需要配备Apple Silicon（M1、M2或M3）的Mac，一次性购买价格为99美元，无需订阅。它旨在帮助用户更快地查找文件，减少管理工作，并将数字混乱转化为有组织的知识库。用户评价强调了节省时间和改进文件组织。该应用程序可索引无限文件，允许在各种文档类型中进行智能搜索，并支持快速编辑。"
  },
  {
    "id": "44117722",
    "title": "Compiler Explorer and the Promise of URLs That Last Forever",
    "url": "https://xania.org/202505/compiler-explorer-urls-forever",
    "summary": "This article details the history and upcoming challenge of preserving Compiler Explorer links. Initially, the service stored compiler state directly in URLs, which became unwieldy. A switch to Google's goo.gl link shortener followed, but this created problems when Stack Overflow banned shorteners. A complex redirect system was implemented as a workaround, ultimately leading to storing states in S3 with DynamoDB managing shortened hash URLs. This system includes a humorous check for profanity in shortened links, adding extra data until a clean hash is generated.\n\nThe core issue now is the impending shutdown of Google's goo.gl service in August 2025. Despite promises of permanence, these goo.gl based links, including Compiler Explorer links of the form godbolt.org/g/abc123, will cease to function. The author, committed to \"URLs that last forever,\" has been actively scraping the internet to rescue and catalog these links, creating a database to replace reliance on goo.gl.\n\nThe article highlights the dangers of relying on third-party services for critical infrastructure and emphasizes the importance of owning the entire stack for long-term stability. The author encourages users to revisit old Compiler Explorer links to ensure they are added to the rescue database, preserving them as a valuable piece of programming history.\n",
    "chinese_title": "编译器探索器与永恒链接的承诺",
    "chinese_summary": "本文详细介绍了保存Compiler Explorer链接的历史和即将面临的挑战。最初，该服务直接将编译器状态存储在URL中，但这变得笨拙。随后切换到谷歌的goo.gl链接缩短服务，但这在Stack Overflow禁止缩短链接时产生了问题。作为一种变通方法，实施了一个复杂的重定向系统，最终导致将状态存储在S3中，并由DynamoDB管理缩短的哈希URL。该系统包括对缩短链接中的亵渎行为进行幽默的检查，添加额外数据直到生成干净的哈希值。\n\n现在核心问题是谷歌的goo.gl服务将于2025年8月关闭。尽管承诺永久性，但这些基于goo.gl的链接，包括godbolt.org/g/abc123形式的Compiler Explorer链接，将不再起作用。作者致力于“永久有效的URL”，一直在积极抓取互联网以抢救和编目这些链接，创建一个数据库以取代对goo.gl的依赖。\n\n本文强调了依赖第三方服务作为关键基础设施的危险，并强调了拥有整个技术栈对于长期稳定性的重要性。作者鼓励用户重新访问旧的Compiler Explorer链接，以确保它们被添加到救援数据库中，从而将它们作为编程历史的宝贵组成部分保存下来。"
  },
  {
    "id": "44117059",
    "title": "Show HN: Tesseral – Open-Source Auth",
    "url": "https://github.com/tesseral-labs/tesseral",
    "summary": "Tesseral is an open-source, multi-tenant authentication infrastructure designed for B2B SaaS applications. It's an API-first service offering a comprehensive suite of features to manage user authentication, authorization, and access control, without being tied to a specific language or framework.\n\nKey features include customizable hosted login pages, B2B multitenancy support, user impersonation, self-service configuration for customers, magic links, social login, SAML & SCIM support, RBAC, MFA, passkeys/WebAuthn, authenticator apps, API key management, user invitations, and webhooks. These features allow developers to easily add secure and robust authentication capabilities to their B2B applications.\n\nTesseral offers a managed service (console.tesseral.com) and can also be self-hosted. Developers can get started by reading the documentation (tesseral.com/docs) and utilizing SDKs for React, Express, Flask, and Golang. A frontend integration example using React is provided, and a backend example with Flask demonstrates how to use Tesseral to authenticate requests and extract user details.\n\nThe project is licensed under MIT and welcomes contributions, emphasizing a cautious approach to merging changes. Security vulnerabilities should be reported directly to security@tesseral.com. Tesseral encourages community engagement through LinkedIn, X (Twitter), newsletter, blog, and direct contact with the founders. Tesseral is managed by a startup based in San Francisco, with primary technical responsibility held by Ulysse Carion, Blake Williams, and Dillon Nys.\n",
    "chinese_title": "Show HN: Tesseral – 开源认证",
    "chinese_summary": "Tesseral：面向 B2B SaaS 应用的开源多租户身份验证基础设施。它是一个 API 优先的服务，提供一套全面的功能来管理用户身份验证、授权和访问控制，且不受特定语言或框架的限制。\n\n主要功能包括：可定制的托管登录页面、B2B 多租户支持、用户模拟、客户自助配置、魔法链接、社交登录、SAML & SCIM 支持、RBAC、MFA、密码密钥/WebAuthn、验证器应用、API 密钥管理、用户邀请和 Webhooks。这些功能使开发人员能够轻松地将安全可靠的身份验证功能添加到他们的 B2B 应用程序中。\n\nTesseral 提供托管服务 (console.tesseral.com)，也可进行自托管。开发者可以通过阅读文档 (tesseral.com/docs) 并使用 React、Express、Flask 和 Golang 的 SDK 来开始使用。提供了一个使用 React 的前端集成示例，以及一个使用 Flask 的后端示例，演示了如何使用 Tesseral 对请求进行身份验证并提取用户详细信息。\n\n该项目采用 MIT 许可证，欢迎贡献，但强调以谨慎的态度合并更改。安全漏洞应直接报告至 security@tesseral.com。Tesseral 鼓励通过 LinkedIn、X (Twitter)、新闻通讯、博客以及与创始人的直接联系来进行社区互动。Tesseral 由一家位于旧金山的初创公司管理，主要技术负责人为 Ulysse Carion、Blake Williams 和 Dillon Nys。"
  },
  {
    "id": "44116803",
    "title": "The mysterious Gobi wall uncovered",
    "url": "https://phys.org/news/2025-05-secrets-mysterious-gobi-wall-uncovered.html",
    "summary": "This article discusses a new study by archaeologists from the Hebrew University of Jerusalem and collaborators on the Gobi Wall, a 321-kilometer segment of a larger frontier system in Mongolia. The research, published in the journal *Land*, sheds light on the wall's origins, function, and historical context.\n\nThe study reveals that the wall was primarily constructed during the Xi Xia dynasty (1038–1227 CE) by the Tungut tribe, challenging the notion of the wall being solely a defensive structure. Instead, the researchers argue it served a multifunctional role, including boundary demarcation, resource management, and consolidating imperial control. Evidence also indicates periodic occupation from the 2nd century BCE through the 19th century CE, underscoring its long-term strategic significance.\n\nThe wall's construction used local materials, such as rammed earth, supported by stone and wood. Its route was strategically chosen based on resource availability, like water and wood, and the placement of forts and garrisons leveraged natural geographic features.\n\nProfessor Shelach-Lavi emphasizes that the Gobi Wall was a \"dynamic mechanism\" for governing movement, trade, and territorial control. The findings offer insights into the relationship between environmental adaptation and state power in medieval empires and have implications for understanding ancient infrastructure.\n",
    "chinese_title": "戈壁石墙之谜",
    "chinese_summary": "本文探讨了耶路撒冷希伯来大学考古学家及其合作者对戈壁长城的新研究。戈壁长城是蒙古国一处大型边境系统中的一段，长321公里。这项发表在《土地》（Land）杂志上的研究揭示了长城的起源、功能和历史背景。\n\n该研究表明，长城主要由党项族在西夏王朝（公元1038-1227年）时期建造，挑战了长城仅作为防御结构的观点。研究人员认为，它具有多重功能，包括边界划分、资源管理和巩固皇权。证据还表明，从公元前2世纪到公元19世纪期间，该长城曾被周期性地使用，突显了其长期的战略意义。\n\n长城的建造使用了当地材料，如夯土，并以石头和木材加固。其路线是根据水和木材等资源的可用性以及利用自然地理特征的堡垒和驻军的位置进行战略性选择的。\n\nShelach-Lavi教授强调，戈壁长城是管理移动、贸易和领土控制的“动态机制”。这些发现为了解中世纪帝国中环境适应与国家权力之间的关系提供了见解，并对理解古代基础设施具有重要意义。"
  },
  {
    "id": "44115620",
    "title": "The Who Cares Era",
    "url": "https://dansinker.com/posts/2025-05-23-who-cares/",
    "summary": "In \"The Who Cares Era,\" Dan Sinker laments the growing apathy and lack of quality in content creation, largely fueled by the rise of AI. He points to examples like AI-generated newspaper supplements filled with fabricated information that were published without anyone noticing or caring, and the dumbing down of a potential podcast series to something generic and disposable.\n\nSinker argues that AI, while useful in some contexts, is often used to produce mediocre content because \"good enough\" is acceptable for many who simply don't care. He connects this trend to a broader cultural shift where attention spans are short and content is designed to be passively consumed.\n\nHe further criticizes the uncaring attitude pervading government and institutions, referencing the Trump administration and Elon Musk's actions, which prioritize cost-cutting over quality and expertise.\n\nDespite this disheartening landscape, Sinker offers a call to action. He encourages readers to actively care, to create authentic and imperfect work, to support those who are making real things, and to engage with content attentively. He emphasizes the importance of being human, being imperfect, and pushing back against the tide of mediocrity by prioritizing genuine effort and engagement. Ultimately, in an era defined by apathy, caring is the most radical and necessary act.\n",
    "chinese_title": "谁在乎的时代",
    "chinese_summary": "在“谁在乎的时代”中，Dan Sinker哀叹内容创作中日益增长的冷漠和质量下滑，这在很大程度上是由人工智能的兴起所推动的。他指出，一些例子如人工智能生成的新闻报纸增刊充斥着捏造的信息，却在无人注意或关心的情况下被发表，以及一个潜在的播客系列被简化成通用且可抛弃的东西。\n\nSinker认为，人工智能在某些情况下虽然有用，但经常被用来生产平庸的内容，因为对许多不在乎的人来说，“足够好”是可以接受的。他将这一趋势与更广泛的文化转变联系起来，即人们的注意力持续时间短，内容被设计成被动消费。\n\n他进一步批评了政府和机构中普遍存在的漠不关心态度，并提到了特朗普政府和埃隆·马斯克的行为，这些行为将削减成本置于质量和专业知识之上。\n\n尽管处境令人沮丧，Sinker还是发出了行动号召。他鼓励读者积极关心，创作真实且不完美的作品，支持那些正在创造真实事物的人，并全神贯注地参与内容。他强调了作为人类、保持不完美的重要性，并通过优先考虑真正的努力和参与来对抗平庸的浪潮。最终，在一个以冷漠为特征的时代，关心是最激进和最必要的行为。"
  },
  {
    "id": "44117302",
    "title": "Getting a Cease and Desist from Waffle House",
    "url": "https://www.jack.bio/blog/wafflehouse",
    "summary": "In late September 2024, while a hurricane threatened Florida, the author reverse-engineered Waffle House's website to create a live map tracking restaurant closures, an unofficial \"Waffle House Index\" used by FEMA to gauge disaster severity. Discovering that Waffle House used Next.js and React Server Components, the author located a JSON file containing location data, which was then scraped and processed with Python to build the index.\n\nAfter launching the site (wafflehouseindex[.]org) and tweeting about it, the author received attention from Waffle House's corporate account, which stated the information was incorrect. Interest surged after political commentator Frank Luntz shared the site, but Waffle House quickly had his tweet removed and blocked the author.\n\nShortly thereafter, the author received a cease and desist letter from Waffle House for unauthorized use of their trademarks. Despite a humorous response (\"with respect and syrup\"), the author complied and took the site down. While initially hoping for official collaboration, the author was ultimately ghosted.\n\nDespite the short lifespan, the author valued the experience of building something for fun, highlighting the power of data manipulation to create meaningful projects. The author also thanked Waffle House for their good sportsmanship despite the trademark infringement and data scraping.\n",
    "chinese_title": "收到华夫饼屋的停止函",
    "chinese_summary": "2024年9月末，当飓风威胁佛罗里达州时，作者对华夫饼屋的网站进行了逆向工程，创建了一个实时地图，追踪餐厅关闭情况，这是一个美国联邦紧急事务管理局（FEMA）用来衡量灾害严重程度的非官方“华夫饼屋指数”。 作者发现华夫饼屋使用了Next.js和React Server Components，找到了一个包含位置数据的JSON文件，然后使用Python对其进行抓取和处理，从而构建了该指数。\n\n在发布网站（wafflehouseindex[.]org）并在推特上发布相关信息后，作者收到了华夫饼屋官方账号的关注，对方声明该信息不准确。 在政治评论员弗兰克·伦茨分享该网站后，人们的兴趣激增，但华夫饼屋迅速删除了他的推文并屏蔽了作者。\n\n此后不久，作者收到了华夫饼屋发出的停止侵权通知，理由是未经授权使用其商标。 尽管作者做出了幽默的回应（“带着尊重和糖浆”），但还是照做了并关闭了网站。 虽然最初希望得到官方合作，但作者最终被无视了。\n\n尽管寿命短暂，但作者珍视这次为乐趣而构建项目的经历，强调了数据操作创造有意义项目的力量。 作者还感谢华夫饼屋的良好体育精神，尽管存在商标侵权和数据抓取行为。"
  },
  {
    "id": "44115973",
    "title": "The Blowtorch Theory: A new model for structure formation in the universe",
    "url": "https://theeggandtherock.com/p/the-blowtorch-theory-a-new-model",
    "summary": "Julian Gough's article introduces the \"Blowtorch Theory\" as an alternative model for the universe's structure formation, challenging the mainstream Lambda Cold Dark Matter (ΛCDM) model. The ΛCDM model relies on gravity and theoretical \"dark matter\" to explain the Cosmic Web's formation—a network of dense galaxy clusters connected by filaments, surrounding vast voids. However, ΛCDM struggles to explain the rapid emergence of mature galaxies observed by the James Webb Space Telescope.\n\nThe Blowtorch Theory proposes that early, sustained supermassive black hole jets actively shaped the universe by carving out voids and laying down magnetic fields in the early universe. These jets created low-pressure cavities, which, expanded over time, formed the cosmic voids and filaments we observe today.\n\nA key advantage of the Blowtorch Theory is that it doesn't require dark matter; the structure formation can be explained by jets and ordinary matter. The theory is supported by recent discoveries of early supermassive black holes, powerful jets, and rapid galaxy formation around these black holes. The recent discovery of a blazar from the early universe further reinforces the theory's predictions.\n\nThe article highlights the surprising discovery of cosmic voids in the late 1970s, regions of space with extremely low density, which the original cosmological models failed to predict. ΛCDM was developed to explain these structures, but the Blowtorch Theory offers a simpler explanation.\n",
    "chinese_title": "喷灯理论：宇宙结构形成的新模型",
    "chinese_summary": "朱利安·高夫的文章介绍了“喷灯理论”，作为宇宙结构形成的另一种模型，挑战了主流的Lambda冷暗物质（ΛCDM）模型。ΛCDM模型依赖于引力和理论上的“暗物质”来解释宇宙网的形成——一个由密集星系团通过丝状结构连接，周围环绕着巨大空洞的网络。然而，ΛCDM难以解释詹姆斯·韦伯太空望远镜观测到的成熟星系的快速出现。\n\n喷灯理论提出，早期持续的超大质量黑洞喷流通过在早期宇宙中开辟空洞和铺设磁场，积极地塑造了宇宙。这些喷流创造了低压腔，随着时间的推移，这些低压腔膨胀，形成了我们今天观测到的宇宙空洞和丝状结构。\n\n喷灯理论的一个主要优势是不需要暗物质；结构形成可以用喷流和普通物质来解释。该理论得到了近期早期超大质量黑洞、强大喷流以及这些黑洞周围快速星系形成的发现的支持。近期发现的来自早期宇宙的耀变体进一步加强了该理论的预测。\n\n文章强调了20世纪70年代末对宇宙空洞的惊人发现，这些空间区域的密度极低，而最初的宇宙学模型未能预测到这些空洞。ΛCDM的开发是为了解释这些结构，但喷灯理论提供了一个更简单的解释。"
  },
  {
    "id": "44116236",
    "title": "Comprehensive Analysis of De-Anonymization Attacks Against the Privacy Coin XMR",
    "url": "https://monero.forex/is-monero-totally-private-a-comprehensive-analysis-of-de-anonymization-attacks-against-the-privacy-coin/",
    "summary": "This article provides a comprehensive analysis of attempts to deanonymize transactions on Monero (XMR), a privacy-focused cryptocurrency. It highlights that while Monero is designed to be opaque through features like ring signatures, stealth addresses, and confidential transactions, various entities have tried to circumvent its privacy.\n\nThe article examines several attempts, including those by Chainalysis and CipherTrace, which developed tools exploiting transaction timing and network analysis, yielding limited, probabilistic success, especially when Monero interacts with less private systems. Academic research identified vulnerabilities in older versions of Monero's ring signature implementation, which were subsequently addressed. Firms have also explored off-chain data correlation via exchange data and IP addresses, achieving partial success dependent on user operational security.\n\nThe IRS even offered a bounty for cracking Monero's privacy, yet there's no public evidence of success. In contrast, the Monero community actively works to strengthen privacy through initiatives like the \"Breaking Monero\" series, which identifies and mitigates vulnerabilities.\n\nThe article concludes that despite numerous attempts, Monero's privacy remains resilient. While certain methods have exploited weaknesses or probabilistically reduced anonymity, none have achieved reliable, widespread deanonymization. The continuous development and fortification of its privacy features by the Monero community ensure its continued position as a leading choice for privacy-focused users.\n",
    "chinese_title": "针对隐私币XMR去匿名化攻击的综合分析",
    "chinese_summary": "本文全面分析了试图去匿名化门罗币（XMR）交易的尝试，门罗币是一种注重隐私的加密货币。文章强调，尽管门罗币旨在通过环签名、隐形地址和机密交易等功能实现不透明性，但各种实体一直在试图绕过其隐私保护。\n\n文章考察了若干尝试，包括Chainalysis和CipherTrace开发的利用交易时间和网络分析的工具，这些工具取得了有限的、概率性的成功，尤其是在门罗币与隐私性较差的系统交互时。学术研究发现门罗币早期版本环签名实现中存在漏洞，这些漏洞随后得到了修复。一些公司还探索了通过交易所数据和IP地址进行链下数据关联，取得了一定的成功，但依赖于用户的操作安全。\n\n美国国税局甚至悬赏破解门罗币的隐私，但没有公开证据表明有人成功。与此相反，门罗币社区积极致力于通过诸如“破解门罗币”系列等举措来加强隐私保护，这些举措旨在识别和缓解漏洞。\n\n文章总结认为，尽管尝试众多，门罗币的隐私保护仍然具有韧性。虽然某些方法利用了弱点或以概率的方式降低了匿名性，但没有一种方法能够实现可靠、广泛的去匿名化。门罗币社区对其隐私功能的持续开发和加强确保了其作为注重隐私用户的首选地位。"
  },
  {
    "id": "44116298",
    "title": "Show HN: Loodio 2 – A Simple Rechargable Bathroom Privacy Device",
    "url": "https://loodio.com/",
    "summary": "Loodio is launching Loodio 2, a simple, rechargeable bathroom privacy device. It aims to help users relax during private moments. The device comes with a 4GB memory card, 100 pre-installed songs, and boasts a week-long battery life. It's priced at $149 and includes free international shipping.\n",
    "chinese_title": "Show HN: Loodio 2 – 简单可充电的浴室隐私设备",
    "chinese_summary": "Loodio推出Loodio 2，一款简易可充电的浴室隐私设备。旨在帮助用户在私密时刻放松身心。该设备配有4GB存储卡，预装100首歌曲，并拥有长达一周的电池续航。售价149美元，包含免费国际运送。"
  },
  {
    "id": "44113210",
    "title": "As a developer, my most important tools are a pen and a notebook",
    "url": "https://hamatti.org/posts/as-a-developer-my-most-important-tools-are-a-pen-and-a-notebook/",
    "summary": "The article \"As a developer, my most important tools are a pen and a notebook\" argues that despite the prevalence of sophisticated digital tools, a simple pen and notebook remain essential for developers.\n\nThe author contends that using pen and paper allows for deeper thinking and problem-solving compared to working directly on a computer. They find that sketching diagrams, outlining code structure, and taking notes by hand promotes a better understanding of complex problems. This process allows for more abstract thought and avoids getting bogged down in implementation details too early.\n\nFurthermore, the author highlights the benefits of offline brainstorming. Without the distractions of notifications and code editors, developers can focus solely on generating ideas and exploring different solutions. They believe this leads to more creative and well-considered approaches.\n\nThe notebook also serves as a valuable repository of ideas, notes, and observations that can be easily revisited and refined. Unlike digital notes, which can be easily lost or fragmented across different applications, a physical notebook provides a tangible and organized record of the developer's thought process.\n\nFinally, the article emphasizes that the pen and notebook are tools for thinking, not just recording. They encourage a more deliberate and thoughtful approach to development, leading to better code and more effective problem-solving.\n",
    "chinese_title": "作为一名开发者，我最重要的工具是笔和笔记本。",
    "chinese_summary": "文章《作为开发者，我最重要的工具是笔和笔记本》认为，尽管复杂的数字工具很普及，但简单的笔和笔记本对于开发者来说仍然至关重要。\n\n作者认为，与直接在电脑上工作相比，使用纸笔可以进行更深入的思考和问题解决。他们发现，手绘图表、勾勒代码结构和做笔记有助于更好地理解复杂问题。这个过程可以进行更抽象的思考，并避免过早地陷入实现细节。\n\n此外，作者强调了离线头脑风暴的好处。在没有通知和代码编辑器的干扰下，开发者可以完全专注于产生想法和探索不同的解决方案。他们认为这会带来更具创造性和经过深思熟虑的方法。\n\n笔记本还可以作为想法、笔记和观察的重要存储库，可以轻松地重新访问和完善。与容易丢失或分散在不同应用程序中的数字笔记不同，物理笔记本为开发人员的思维过程提供了有形的、有组织的记录。\n\n最后，文章强调笔和笔记本是思考的工具，而不仅仅是记录的工具。他们鼓励一种更深思熟虑的开发方法，从而产生更好的代码和更有效的问题解决。"
  },
  {
    "id": "44116862",
    "title": "XAI to pay Telegram $300M to integrate Grok into the chat app",
    "url": "https://techcrunch.com/2025/05/28/xai-to-invest-300m-in-telegram-integrate-grok-into-app/",
    "summary": "Telegram and Elon Musk's AI company, xAI, have partnered to integrate Grok, xAI's chatbot, into Telegram's platform for one year. xAI will pay Telegram $300 million in cash and equity for the distribution deal. Telegram will also receive 50% of the revenue from xAI subscriptions purchased through the app.\n\nWhile Grok was previously available only to Telegram premium users, this deal suggests it might be rolled out to all users. Pavel Durov showcased Grok's potential within Telegram in a video, highlighting features like pinning Grok to the top of chats, using it via the search bar, and employing it for writing suggestions, summarizing content, creating stickers, answering business questions, and assisting with moderation. This mirrors Meta's integration of Meta AI into Instagram and WhatsApp.\n\nThe article also includes promotional material for TechCrunch Sessions: AI, an upcoming event featuring speakers from leading AI companies. The event is scheduled for June 5, 2025, in Berkeley, California.\n",
    "chinese_title": "XAI将向Telegram支付3亿美元，以将Grok整合到该聊天应用中。",
    "chinese_summary": "Telegram与埃隆·马斯克的xAI公司合作，将xAI的聊天机器人Grok集成到Telegram平台一年。xAI将支付Telegram 3亿美元的现金和股权作为分销协议费用。Telegram还将获得通过该应用程序购买的xAI订阅收入的50%。\n\n虽然Grok之前仅供Telegram高级用户使用，但这项协议表明它可能会向所有用户推广。Pavel Durov在一段视频中展示了Grok在Telegram中的潜力，强调了诸如将Grok置顶聊天、通过搜索栏使用它、以及将其用于写作建议、内容总结、创建贴纸、回答商业问题和协助审核等功能。这与Meta将Meta AI集成到Instagram和WhatsApp的做法类似。\n\n文章还包含TechCrunch Sessions: AI的宣传材料，这是一个即将举行的活动，届时将有来自领先人工智能公司的演讲者。该活动计划于2025年6月5日在加利福尼亚州伯克利举行。"
  },
  {
    "id": "44087390",
    "title": "Mathematical Fiction",
    "url": "https://kasmana.people.charleston.edu/MATHFICT/default.html",
    "summary": "The Mathematical Fiction Homepage, curated by Alex Kasman at the College of Charleston, is a comprehensive database dedicated to documenting the intersection of mathematics and fiction. The site aims to be a resource for readers interested in stories, novels, plays, films, and comic books that incorporate mathematical concepts or feature mathematicians as characters.\n\nThe website boasts a collection of over one thousand entries and allows users to explore the database through various methods. Users can view the entire list sorted by author, title, or publication date. They can also browse by genre, topic, motif, or medium, or use the search function for more specific criteria. A section dedicated to recently added or modified entries keeps users up-to-date. Kasman also provides personal recommendations for those seeking guidance.\n\nThe site encourages user engagement, inviting feedback and suggestions. Additionally, it offers a link to articles discussing mathematical fiction, enriching the user experience beyond simply listing examples. In essence, the Mathematical Fiction Homepage serves as a valuable and accessible resource for anyone interested in exploring the representation of mathematics in fictional narratives.\n",
    "chinese_title": "数学虚构",
    "chinese_summary": "数学虚构作品主页，由查尔斯顿学院的Alex Kasman策划，是一个致力于记录数学与虚构作品交集的综合数据库。该网站旨在为对包含数学概念或以数学家为主角的故事、小说、戏剧、电影和漫画书感兴趣的读者提供资源。\n\n该网站拥有超过一千条条目的收藏，并允许用户通过各种方法浏览数据库。用户可以查看按作者、标题或出版日期排序的完整列表。他们还可以按类型、主题、母题或媒介浏览，或者使用搜索功能来查找更具体的条件。一个专门介绍最近添加或修改条目的部分让用户及时了解最新信息。Kasman还为寻求指导的人提供个人推荐。\n\n该网站鼓励用户参与，邀请反馈和建议。此外，它还提供指向讨论数学虚构作品的文章的链接，从而丰富了用户体验，而不仅仅是列出示例。本质上，数学虚构作品主页是对于任何有兴趣探索数学在虚构叙事中表现的人来说，都是一个有价值且易于访问的资源。"
  },
  {
    "id": "44116412",
    "title": "FlowTSE: Target Speaker Extraction with Flow Matching",
    "url": "https://arxiv.org/abs/2505.14465",
    "summary": "This arXiv article (arXiv:2505.14465) introduces FlowTSE, a novel target speaker extraction (TSE) method based on conditional flow matching. The paper, titled \"FlowTSE: Target Speaker Extraction with Flow Matching,\" is authored by Aviv Navon, Aviv Shamsian, Yael Segal-Feldman, Neta Glazer, Gil Hetz, and Joseph Keshet, and was submitted to InterSpeech 2025.\n\nThe core problem addressed is isolating a specific speaker's voice from a mixed audio signal using a reference enrollment sample. While existing TSE methods are primarily discriminative, the paper explores generative approaches, highlighting their potential but also their limitations in complexity and computational overhead.\n\nFlowTSE aims to overcome these limitations with a simple and effective conditional flow matching approach. The model takes mel-spectrogram representations of both the enrollment audio and the mixed speech, with the goal of extracting the clean speech of the target speaker.\n\nFurthermore, the paper introduces a novel vocoder, conditioned on the complex Short-Time Fourier Transform (STFT) of the mixed signal, to improve phase estimation, which is critical for high-quality audio reconstruction. Experimental results on standard TSE benchmarks demonstrate that FlowTSE achieves performance comparable to or exceeding strong baseline methods.\n\nIn essence, FlowTSE presents a computationally efficient and accurate generative model for target speaker extraction, leveraging conditional flow matching and a novel vocoder for enhanced phase reconstruction.\n",
    "chinese_title": "FlowTSE：基于流匹配的目标说话人提取",
    "chinese_summary": "这篇arXiv文章 (arXiv:2505.14465) 介绍了FlowTSE，一种基于条件流匹配的新型目标说话人提取 (TSE) 方法。这篇题为“FlowTSE：基于流匹配的目标说话人提取”的论文由Aviv Navon、Aviv Shamsian、Yael Segal-Feldman、Neta Glazer、Gil Hetz 和 Joseph Keshet 撰写，并已投稿至 InterSpeech 2025。\n\n其解决的核心问题是，利用参考注册样本从混合音频信号中分离出特定说话人的声音。尽管现有的 TSE 方法主要为判别式，但本文探索了生成式方法，强调了它们的潜力，但也指出了它们在复杂性和计算开销方面的局限性。\n\nFlowTSE 旨在通过一种简单有效的条件流匹配方法来克服这些局限性。该模型以注册音频和混合语音的梅尔频谱图表示作为输入，目标是提取目标说话人的干净语音。\n\n此外，该论文还介绍了一种新型声码器，它以混合信号的复数短时傅里叶变换 (STFT) 为条件，以改善相位估计，这对于高质量音频重建至关重要。在标准 TSE 基准上的实验结果表明，FlowTSE 的性能与强大的基线方法相当或超过。\n\n本质上，FlowTSE 提出了一种计算效率高且准确的目标说话人提取生成模型，利用条件流匹配和一种新型声码器来增强相位重建。"
  },
  {
    "id": "44116872",
    "title": "LLM Codegen go Brrr – Parallelization with Git Worktrees and Tmux",
    "url": "https://www.skeptrune.com/posts/git-worktrees-agents-and-tmux/",
    "summary": "This article is about speeding up LLM (Large Language Model) code generation by using parallelization with Git worktrees and Tmux. The core idea is to leverage Git worktrees to create multiple isolated working environments from the same Git repository, allowing independent LLM codegen tasks to run concurrently without interfering with each other. Tmux is used to manage and monitor these parallel processes.\n\nThe article likely outlines the advantages of this approach, such as reduced overall time for code generation and efficient utilization of computational resources. It probably also describes the setup process, including:\n\n*   **Creating Git Worktrees:**  Explaining how to create and manage multiple worktrees from the main Git repository. Each worktree contains a separate copy of the files, allowing for parallel development/codegen.\n*   **Utilizing Tmux:** Showcasing how to use Tmux to create multiple terminal sessions, one for each Git worktree, and running the LLM codegen tasks in each session. This allows for monitoring and control of each individual process.\n*   **Possible script or workflow:** Likely includes example commands or a workflow for automating the creation of worktrees, Tmux sessions, and the execution of codegen scripts.\n\nIn essence, the article advocates for a practical workflow leveraging Git worktrees and Tmux to parallelize LLM codegen tasks, leading to faster and more efficient code generation. It likely provides a hands-on guide for implementing this parallelization strategy.\n",
    "chinese_title": "LLM代码生成，Git Worktrees与Tmux并行提速",
    "chinese_summary": "本文探讨如何利用 Git 工作区和 Tmux 并行化来加速 LLM（大型语言模型）的代码生成。核心思想是利用 Git 工作区从同一个 Git 仓库创建多个隔离的工作环境，从而允许独立的 LLM 代码生成任务并发运行，互不干扰。Tmux 用于管理和监控这些并行进程。\n\n文章可能概述了这种方法的优势，例如缩短代码生成的总时间以及有效利用计算资源。它可能还描述了设置过程，包括：\n\n*   **创建 Git 工作区：** 解释如何从主 Git 仓库创建和管理多个工作区。每个工作区包含一个单独的文件副本，从而允许并行开发/代码生成。\n*   **利用 Tmux：** 展示如何使用 Tmux 创建多个终端会话，每个 Git 工作区对应一个会话，并在每个会话中运行 LLM 代码生成任务。这允许监控和控制每个单独的进程。\n*   **可能的脚本或工作流程：** 可能包含示例命令或工作流程，用于自动化创建工作区、Tmux 会话以及执行代码生成脚本。\n\n本质上，本文提倡一种利用 Git 工作区和 Tmux 并行化 LLM 代码生成任务的实用工作流程，从而实现更快、更高效的代码生成。它可能提供了一个用于实施这种并行化策略的实践指南。"
  },
  {
    "id": "44110584",
    "title": "Show HN: My LLM CLI tool can run tools now, from Python code or plugins",
    "url": "https://simonwillison.net/2025/May/27/llm-tools/",
    "summary": "This article announces the release of LLM 0.26, a CLI tool and Python library update focused on enabling Large Language Models to use tools. This allows LLMs from providers like OpenAI, Anthropic, Gemini, and local models from Ollama to access external functionalities represented as Python functions.\n\nKey features include:\n\n*   **Tool execution:** LLM can now run tools installed from plugins or defined directly via the `--functions` option on the command line.\n*   **Tool plugins:** The update introduces a plugin system for easily extending LLM's capabilities with new tools. The author has already released plugins for simple expression evaluation (llm-tools-simpleeval), JavaScript execution (llm-tools-quickjs), SQLite database access (llm-tools-sqlite), and remote Datasette querying (llm-tools-datasette).\n*   **Python API support:** The Python library now supports tools, with the new `model.chain()` method designed to handle tool call requests, execute them, and prompt the model again with the results, enabling more complex interactions.\n*   **Asynchronous tool support:** Tools can be async functions and run concurrently.\n*   **Model Context Protocol (MCP) support:** Planned integration with MCP to broaden tool sources.\n\nThe author explains why this update took time, emphasizing the need for an abstraction layer to work across various models. He also addresses the topic of \"agents,\" suggesting that LLM 0.26 is a foundation for building them. Future plans include improving tool execution logs, adding tool support to more model plugins, and creating a tutorial on writing tool plugins.\n",
    "chinese_title": "展示 HN：我的 LLM CLI 工具现在可以运行工具了，来自 Python 代码或插件",
    "chinese_summary": "本文宣布发布 LLM 0.26，这是一个 CLI 工具和 Python 库更新，专注于使大型语言模型能够使用工具。这使得来自 OpenAI、Anthropic、Gemini 等提供商的 LLM 以及来自 Ollama 的本地模型能够访问以外部功能表示的 Python 函数。\n\n主要功能包括：\n\n*   **工具执行：** LLM 现在可以运行从插件安装的工具，或通过命令行上的 `--functions` 选项直接定义的工具。\n*   **工具插件：** 该更新引入了一个插件系统，可以轻松地使用新工具扩展 LLM 的功能。作者已经发布了用于简单表达式求值 (llm-tools-simpleeval)、JavaScript 执行 (llm-tools-quickjs)、SQLite 数据库访问 (llm-tools-sqlite) 和远程 Datasette 查询 (llm-tools-datasette) 的插件。\n*   **Python API 支持：** Python 库现在支持工具，新的 `model.chain()` 方法旨在处理工具调用请求、执行它们并使用结果再次提示模型，从而实现更复杂的交互。\n*   **异步工具支持：** 工具可以是异步函数并并发运行。\n*   **模型上下文协议 (MCP) 支持：** 计划与 MCP 集成，以扩大工具来源。\n\n作者解释了为什么此更新花费了时间，强调需要一个抽象层才能跨各种模型工作。他还讨论了“代理”这一话题，并暗示 LLM 0.26 是构建它们的基础。未来的计划包括改进工具执行日志、向更多模型插件添加工具支持以及创建关于编写工具插件的教程。"
  },
  {
    "id": "44107942",
    "title": "Square Theory",
    "url": "https://aaronson.org/blog/square-theory",
    "summary": "\"Square Theory\" introduces the concept that many satisfying and clever creations, from crossword puzzles to jokes and brand names, can be understood through a common structure: the \"square.\" This theory, originating from observations within the Crosscord crossword Discord server, posits that a square, composed of four interconnected elements (vertices) and the relationships between them (edges), creates a sense of completeness and satisfaction.\n\nThe article explains that a \"double double\" (two pairs of synonyms that form non-synonymous phrases) exemplifies this square structure. However, square theory extends beyond wordplay, applying to any four entities or concepts connected by defined relationships. Examples include question mark clues in crosswords, successful brand names, and well-crafted jokes.\n\nThe author argues that the square is a particularly compelling shape because it is the simplest polygon with non-adjacent sides, creating a surprising connection between opposite elements. In crosswords, square theory explains why certain themes feel particularly clever and tight, often completing squares with revealer clues. The article extends the theory to encompass Scrabble and even the fundamental structure of crossword grids themselves, where every letter is part of a square formed by intersecting words.\n\nUltimately, the author suggests that recognizing and utilizing the square structure can be a valuable tool for creative individuals, enabling them to create more satisfying and impactful works in various fields. By seeking to complete the square, creators can tap into a fundamental element of human satisfaction and ingenuity.\n",
    "chinese_title": "平方理论",
    "chinese_summary": "平方理论\n平方理论提出了一种概念，即许多令人满意且巧妙的创作，从填字游戏到笑话和品牌名称，都可以通过一个共同的结构来理解：即“平方”。该理论起源于Crosscord填字Discord服务器中的观察，它认为，一个由四个相互关联的元素（顶点）及其之间的关系（边）组成的平方，能够创造一种完整和满足感。\n\n文章解释说，“双重双重”（形成非同义短语的两对同义词）是这种平方结构的例证。 然而，平方理论不仅仅局限于文字游戏，它适用于由明确关系连接的任何四个实体或概念。例子包括填字游戏中的问号提示、成功的品牌名称和精心制作的笑话。\n\n作者认为，正方形是一种特别引人注目的形状，因为它是具有非相邻边的最简单的多边形，从而在相对元素之间建立了一种令人惊讶的联系。 在填字游戏中，平方理论解释了为什么某些主题感觉特别巧妙和紧凑，通常使用揭示线索来完成正方形。 文章将该理论扩展到包括拼字游戏，甚至包括填字游戏网格本身的基本结构，其中每个字母都是由相交的单词组成的正方形的一部分。\n\n最终，作者认为，识别和利用平方结构对于创意人士来说可能是一种有价值的工具，使他们能够在各个领域创造出更令人满意和更具影响力的作品。 通过寻求完成平方，创作者可以挖掘人类满足感和创造力的基本要素。"
  },
  {
    "id": "44094109",
    "title": "A thought on JavaScript \"proof of work\" anti-scraper systems",
    "url": "https://utcc.utoronto.ca/~cks/space/blog/web/JavaScriptScraperObstacles",
    "summary": "In early 2025, Chris Siebenmann implemented anti-crawler measures on his blog, Wandering Thoughts, and his wiki, CSpace, due to a surge in high-volume crawlers, particularly those scraping data for LLM training. These crawlers often use old browser user agents, specifically Chrome. This led to legitimate users with older browsers being blocked.\n\nThe primary issue is differentiating legitimate users with older browsers from malicious crawlers. If a user encounters this block despite using a current browser version, they are encouraged to contact Siebenmann with their browser details and User-Agent string.\n\nA specific problem is the behavior of archiving sites like archive.today, archive.ph, and archive.is. These sites crawl pages using methods indistinguishable from malicious actors, employing old Chrome User-Agent values, widely distributed IP address blocks without clear identification, and sometimes falsified reverse DNS entries to mimic Googlebot. Siebenmann recommends using archive.org instead, as it's a more well-behaved archival crawler that can access his blog. He expresses frustration at the difficulty of distinguishing between legitimate archiving and malicious scraping activity.\n",
    "chinese_title": "关于 JavaScript “工作量证明” 反爬虫系统的一些思考",
    "chinese_summary": "2025年初，克里斯·西本曼在他的博客“Wandering Thoughts”和维基“CSpace”上实施了反爬虫措施，原因是大量爬虫涌入，特别是那些为了LLM训练而抓取数据的爬虫。这些爬虫通常使用旧的浏览器用户代理，尤其是Chrome。这导致使用旧版浏览器的合法用户被屏蔽。\n\n主要问题是如何区分使用旧版浏览器的合法用户和恶意爬虫。如果用户在使用当前浏览器版本的情况下遇到此屏蔽，建议他们联系西本曼，提供其浏览器详细信息和User-Agent字符串。\n\n一个具体问题是archive.today、archive.ph和archive.is等存档网站的行为。这些网站使用与恶意行为者无法区分的方法抓取页面，采用旧的Chrome User-Agent值，使用没有明确标识的广泛分布的IP地址块，有时还会伪造反向DNS条目来模仿Googlebot。西本曼建议使用archive.org，因为它是一个行为更规范的存档爬虫，可以访问他的博客。他对区分合法存档和恶意抓取活动感到沮丧。"
  },
  {
    "id": "44116503",
    "title": "Mullvad Leta",
    "url": "https://leta.mullvad.net",
    "summary": "The article, titled \"Mullvad Leta,\" is likely discussing \"Leta,\" a feature or component related to Mullvad VPN. Without the actual content of the article, it's impossible to provide a definitive summary. However, based on the context of Mullvad VPN and the name \"Leta,\" we can speculate and offer a plausible summary:\n\nThe \"Mullvad Leta\" article likely details a feature or tool within Mullvad VPN focused on server selection or performance optimization. \"Leta\" probably translates to \"search\" or \"find\" in a relevant language (perhaps Swedish, given Mullvad's origin), suggesting it helps users locate the best server for their needs.\n\nThe article likely explains how Leta works, potentially outlining the criteria it uses to rank servers (e.g., latency, speed, load). It might describe different filtering options available within Leta, allowing users to specify desired server locations or protocols (e.g., WireGuard, OpenVPN).\n\nFurthermore, the article could discuss the benefits of using Leta, such as improved connection speeds, more stable connections, and easier access to content from specific regions. It might also address troubleshooting tips or common issues users might encounter while using Leta and provide solutions.\n\nUltimately, the article probably aims to educate Mullvad VPN users about the Leta feature and encourage them to utilize it for a better and more optimized VPN experience.\n",
    "chinese_title": "莫尔瓦德搜索",
    "chinese_summary": "标题为“Mullvad Leta”的文章可能探讨了Mullvad VPN相关的“Leta”功能或组件。由于缺乏文章实际内容，无法提供确切的摘要。但根据Mullvad VPN的背景以及“Leta”这个名称，我们可以推测并提供一个合理的摘要：\n\n“Mullvad Leta”文章可能详细介绍了Mullvad VPN中一项专注于服务器选择或性能优化的功能或工具。“Leta”在相关语言（可能是瑞典语，鉴于Mullvad的起源）中可能翻译为“搜索”或“查找”，暗示它帮助用户找到最适合其需求的服务器。\n\n文章可能解释Leta的工作原理，并可能概述其用于对服务器进行排名的标准（例如，延迟、速度、负载）。它可能会描述Leta中提供的不同过滤选项，允许用户指定所需的服务器位置或协议（例如，WireGuard、OpenVPN）。\n\n此外，文章可能讨论使用Leta的好处，例如提高连接速度、更稳定的连接以及更容易访问特定区域的内容。它可能还会解决用户在使用Leta时可能遇到的故障排除提示或常见问题，并提供解决方案。\n\n最终，文章可能旨在教育Mullvad VPN用户了解Leta功能，并鼓励他们使用它以获得更好、更优化的VPN体验。"
  },
  {
    "id": "44093845",
    "title": "Homo erectus from the seabed, new archaeological discoveries in Indonesia",
    "url": "https://www.universiteitleiden.nl/en/news/2025/05/homo-erectus-from-the-seabed-new-archaeological-discoveries-in-indonesia",
    "summary": "Archaeological discoveries in the Madura Strait off Java, Indonesia, have yielded fossilized remains of 36 vertebrate species, including skull fragments of Homo erectus, dating back approximately 140,000 years. This is the first such find from the seabed of Sundaland, a vast lowland that connected many of the current Indonesian islands during periods of lower sea levels.\n\nLed by Leiden University archaeologist Harold Berghuis, the research reveals that Javanese Homo erectus dispersed across Sundaland's lowlands along major rivers, where they had access to food resources. The finds indicate that Homo erectus in Sundaland actively hunted large bovids, a practice not seen in earlier Javanese populations but known from more modern human species on the Asian mainland, suggesting potential contact or genetic exchange between hominin groups.\n\nThe research, published in \"Quaternary Environments and Humans,\" presents a comprehensive view of the drowned Sundaland ecosystem, showcasing a landscape resembling the African savanna with diverse fauna, including elephants, rhinos, hippos, Komodo dragons, and river sharks. These animals were thriving in ancient Sundaland.\n\nThe findings provide crucial insights into the biodiversity of Southeast Asia and suggest that the current islands were mountain ranges. The fossil collection is housed in the Geological Museum in Bandung, Indonesia, with planned exhibitions. This discovery is unique because fossils had never been found in this area before.\n",
    "chinese_title": "印度尼西亚海底发现直立人，最新考古发现",
    "chinese_summary": "在印度尼西亚爪哇岛附近的马都拉海峡的考古发现，出土了36种脊椎动物的化石遗骸，其中包括直立人的头骨碎片，其年代可追溯到大约14万年前。 这是首次在巽他陆架海床上发现此类化石，巽他陆架是一片广阔的低地，在海平面较低的时期连接了许多现在的印度尼西亚岛屿。\n\n在莱顿大学考古学家哈罗德·伯格胡伊斯（Harold Berghuis）的带领下，该研究表明，爪哇直立人沿着主要河流分散在巽他陆架的低地上，在那里他们可以获得食物资源。 这些发现表明，巽他陆架的直立人积极捕猎大型牛科动物，这在早期爪哇人群中未曾见过，但在亚洲大陆更为现代的人类物种中已知，这表明不同人族群体之间可能存在接触或基因交流。\n\n这项发表在《第四纪环境与人类》上的研究，全面展示了被淹没的巽他陆架生态系统，展现了一幅类似于非洲热带草原的景象，拥有多样化的动物群，包括大象、犀牛、河马、科莫多巨蜥和河鲨。这些动物曾在古代巽他陆架上繁荣生长。\n\n这些发现为东南亚的生物多样性提供了重要的见解，并表明现在的岛屿曾经是山脉。化石收藏品保存在印度尼西亚万隆的地质博物馆，并计划举办展览。这项发现是独一无二的，因为以前从未在该地区发现过化石。"
  },
  {
    "id": "44086917",
    "title": "The Ingredients of a Productive Monorepo",
    "url": "https://blog.swgillespie.me/posts/monorepo-ingredients/",
    "summary": "This article guides engineers on building a productive monorepo environment. It emphasizes that the primary goal should be consistency, organizational coherence, and shared tooling, rather than solely replicating the success stories of tech giants like Google. A core principle is optimizing operations to be O(change) instead of O(repo), meaning operations should scale with the size of the change, not the entire repository.\n\nThe article covers crucial aspects like source control, advocating for git initially but acknowledging its limitations at scale, suggesting alternatives like forking git/Mercurial or developing custom solutions that allow for partial repository checkouts and virtual filesystems.\n\nFor building, the advice is to stick to a single language if possible and leverage existing language-specific build tools for as long as feasible. Key requirements for a build system in a monorepo include efficient target building and identifying targets affected by changes, leading to the creation of \"target determinators.\"\n\nTesting should be intelligent, incorporating automatic retries and selectively running tests based on the scope of changes. This minimizes flakiness and improves reliability.\n\nContinuous Integration (CI) should determine necessary build artifacts and validations based on changes. Merge queues, which integrate validated changes, are also discussed, with a focus on balancing throughput, correctness, and tail latency. Different landing strategies, like batching changes, are explored. The article concludes by emphasizing that keeping CI fast is crucial for a smooth merge queue, highlighting the trade-offs between speed and thoroughness.\n",
    "chinese_title": "一个高效单体仓库的要素",
    "chinese_summary": "本文旨在指导工程师构建高效的单体仓库环境。文章强调，主要目标应该是保持一致性、组织协调性和共享工具，而不是仅仅复制像谷歌这样的科技巨头的成功案例。一个核心原则是优化操作，使其复杂度为O(change)而非O(repo)，意味着操作应该随着变更的大小而扩展，而不是整个仓库的大小。\n\n文章涵盖了源代码控制等关键方面，最初提倡使用Git，但也承认其在大规模应用中的局限性，并建议使用诸如Forking Git/Mercurial或开发自定义解决方案等替代方案，这些方案允许部分仓库检出和虚拟文件系统。\n\n关于构建，文章建议尽可能坚持使用单一语言，并在可行的情况下尽可能利用现有的特定于语言的构建工具。单体仓库中构建系统的关键要求包括高效的目标构建和识别受更改影响的目标，从而催生了“目标决定器”的创建。\n\n测试应该智能化，包含自动重试，并根据变更范围选择性地运行测试。这可以最大限度地减少不稳定性并提高可靠性。\n\n持续集成(CI)应该基于变更确定必要的构建产物和验证。文章还讨论了集成已验证变更的合并队列，重点是平衡吞吐量、正确性和尾部延迟。探索了不同的落地策略，例如批量变更。文章最后强调，保持CI的快速对于平滑的合并队列至关重要，并突出了速度和彻底性之间的权衡。"
  },
  {
    "id": "44116643",
    "title": "Show HN: Wetlands – a lightweight Python library for managing Conda environments",
    "url": "https://arthursw.github.io/wetlands/0.2.0/",
    "summary": "Wetlands is a Python library designed to simplify the management of Conda environments for dependency isolation and embedded execution. It allows developers to create, configure, and execute code within isolated Conda environments on demand, making it suitable for building plugin systems or integrating external modules without dependency conflicts.\n\nKey features include:\n\n*   **Automatic Environment Management:** Creates and configures Conda environments as needed.\n*   **Dependency Isolation:** Installs dependencies within isolated environments to avoid conflicts.\n*   **Embedded Execution:** Runs Python functions within these isolated environments.\n*   **Pixi & Micromamba Support:** Leverages pixi or micromamba for fast and lightweight environment handling.\n\nThe library offers two primary interaction methods: Simplified Execution (using `env.importModule` and `env.execute` for seamless function calls within the environment) and Manual Control (using `env.executeCommands` for running specific commands and managing inter-process communication).\n\nWetlands is installed via `pip install wetlands`. A minimal example showcases creating an environment, installing NumPy, importing a module, executing a function within the environment, and cleaning up. The library is MIT licensed and was developed at Inria in Rennes. The project's documentation and source code are available on GitHub.\n",
    "chinese_title": "Show HN: Wetlands – 一个轻量级的 Python 库，用于管理 Conda 环境",
    "chinese_summary": "Wetlands 是一个 Python 库，旨在简化 Conda 环境的管理，实现依赖隔离和嵌入式执行。它允许开发者按需创建、配置和执行隔离的 Conda 环境中的代码，使其适用于构建插件系统或集成外部模块，而无需担心依赖冲突。\n\n主要特性包括：\n\n*   **自动环境管理：** 根据需要创建和配置 Conda 环境。\n*   **依赖隔离：** 在隔离的环境中安装依赖项，避免冲突。\n*   **嵌入式执行：** 在这些隔离的环境中运行 Python 函数。\n*   **Pixi & Micromamba 支持：** 利用 pixi 或 micromamba 实现快速轻量级的环境处理。\n\n该库提供两种主要的交互方式：简化执行（使用 `env.importModule` 和 `env.execute` 在环境中无缝调用函数）和手动控制（使用 `env.executeCommands` 运行特定命令和管理进程间通信）。\n\nWetlands 通过 `pip install wetlands` 安装。一个最小示例展示了如何创建环境、安装 NumPy、导入模块、在环境中执行函数以及清理环境。该库采用 MIT 许可证，由雷恩的 Inria 开发。该项目的文档和源代码可在 GitHub 上找到。"
  },
  {
    "id": "44086973",
    "title": "The Level Design Book",
    "url": "https://book.leveldesignbook.com",
    "summary": "This is a very brief stub of a document titled \"The Level Design Book.\" It states only two things:\n\n1. **Title:** The Level Design Book\n2. **Content:** This consists of the phrase \"Next What is level design\" and \"Last updated 4 months ago.\"\n\nTherefore, the summary is:\n\n\"The Level Design Book\" is a document, last updated 4 months ago, that appears to be an introduction to level design. The content immediately directs the reader to the question: \"What is level design?\" suggesting it's the topic being addressed. The \"Next\" implies that the book will likely progress beyond just defining level design.\n",
    "chinese_title": "关卡设计书",
    "chinese_summary": "《关卡设计书籍》\n\n这是一份非常简短的名为《关卡设计书籍》的文档草稿。它仅说明了两件事：\n\n1. **标题：** 关卡设计书籍\n2. **内容：** 内容包含短语“下一步，什么是关卡设计？”和“上次更新于4个月前”。\n\n因此，总结是：\n\n《关卡设计书籍》是一份文档，上次更新于4个月前，似乎是对关卡设计的介绍。内容直接引导读者提出问题：“什么是关卡设计？”，表明这是要讨论的主题。“下一步”暗示该书可能会超越仅仅定义关卡设计。"
  },
  {
    "id": "44113026",
    "title": "DWARF as a Shared Reverse Engineering Format",
    "url": "https://lief.re/blog/2025-05-27-dwarf-editor/",
    "summary": "This article by Romain Thomas introduces DWARF as a shared format for reverse engineering, overcoming the incompatibility issues between different reverse engineering tools like IDA and Ghidra. DWARF, traditionally used for debugging information, is well-suited for storing reverse-engineered data such as structures and function names.\n\nThe article highlights an extended LIEF API, available in Python, Rust, and C++, which simplifies the creation of DWARF files using LLVM's backend. LIEF provides a higher-level abstraction over LLVM's low-level API, easing the complexities of the DWARF format.\n\nThe author then presents plugins for Ghidra and Binary Ninja that enable the export of binary analysis data into DWARF format. While Binary Ninja has an official DWARF exporter (with limitations in earlier versions, like exporting stack variables), the author's custom plugin addresses specific needs, such as symbolizing QBDI traces. The Ghidra plugin allows users to export program information into DWARF, either via the GUI or a headless Java script. The author does not currently plan to support IDA, though open to it.\n\nThe article provides a practical example of using DWARF to share reverse engineering results, including the DWARF files generated for a specific binary. The author emphasizes that the DWARF export functionality is in early development and aims to add support for exporting comments in the future.\n",
    "chinese_title": "DWARF作为一种共享的逆向工程格式",
    "chinese_summary": "Romain Thomas 的这篇文章介绍了 DWARF 作为逆向工程的通用格式，以克服 IDA 和 Ghidra 等不同逆向工程工具之间的不兼容性问题。 DWARF 传统上用于调试信息，非常适合存储逆向工程数据，例如结构体和函数名称。\n\n这篇文章重点介绍了一个扩展的 LIEF API，该 API 提供 Python、Rust 和 C++ 版本，简化了使用 LLVM 后端创建 DWARF 文件的过程。 LIEF 提供了对 LLVM 低级 API 的更高级别抽象，降低了 DWARF 格式的复杂性。\n\n作者随后介绍了 Ghidra 和 Binary Ninja 的插件，这些插件可以将二进制分析数据导出为 DWARF 格式。 虽然 Binary Ninja 有一个官方的 DWARF 导出器（早期版本存在局限性，例如导出堆栈变量），但作者的自定义插件解决了特定的需求，例如符号化 QBDI 跟踪。 Ghidra 插件允许用户通过 GUI 或无头 Java 脚本将程序信息导出到 DWARF 中。 作者目前没有计划支持 IDA，但对此持开放态度。\n\n这篇文章提供了一个使用 DWARF 共享逆向工程结果的实际示例，包括为特定二进制文件生成的 DWARF 文件。 作者强调 DWARF 导出功能尚处于早期开发阶段，并计划在未来增加对导出注释的支持。"
  },
  {
    "id": "44088261",
    "title": "Designing Tools for Scientific Thought",
    "url": "https://www.forester-notes.org/tfmt-0001/index.xml",
    "summary": "Jon Sterling's notes explore the design of \"tools for scientific thought,\" particularly for the mathematical sciences. He focuses on the requirements for information data models and tools needed to record and facilitate scientific thinking.\n\nA \"tool for scientific thought\" facilitates the development and interlinking of scientific ideas for authoring, publishing, teaching, learning, and maintaining evergreen notes. Existing tools are categorized into interactive proof assistants and textual authoring/publishing tools like LaTeX.\n\nThe concept of \"evergreen notes,\" inspired by Andy Matuschak, emphasizes permanent, evolving notes that cut across projects. Atomicity is crucial: each note should capture one idea, understandable through its content and linked notes. Sterling argues against traditional mathematical writing's lack of atomicity, advocating for explicit context over implicit context. Achieving atomicity involves avoiding free variables, favoring explicit dependencies through linking, and ensuring notations are decodable via links.\n\nWhile Matuschak prefers associative ontologies, Sterling argues that mathematical knowledge requires hierarchical organization from the start to distinguish assumptions from consequences. However, multiple hierarchical structures can be imposed on the same network of nodes, creating different \"narratives.\" Interfaces should therefore support navigating multiple parent/neighbor relations. He advocates for relatively flat structures to avoid premature complexity.\n\nSterling contrasts \"absolute\" (HTML, LaTeX) and \"relative\" hierarchy models in document markup. He argues that relative hierarchy, where section levels are determined by context, is superior for re-contextualization, a key aspect of fluid scientific media. Furthermore, he prefers explicit hierarchical structure, where the syntactical tree structure of the markup language induces the hierarchy, over implicit structures found in many document markup languages.\n\nHe introduces the concept of a \"forest\" of evergreen notes: a collection where multiple hierarchical structures can emerge and evolve. Individual notes are considered \"trees\" within the forest. He defines the \"extent\" of a tree based on transclusion relationships. Finally, he distinguishes between \"authors\" and \"contributors\" in forests of evergreen notes, emphasizing that authorship implies responsibility and endorsement, which may not extend to subsequent re-contextualizations of a tree by others.\n",
    "chinese_title": "设计科学思维的工具",
    "chinese_summary": "Jon Sterling 的笔记探讨了“科学思维工具”的设计，特别是针对数学科学。他重点关注信息数据模型和工具的需求，这些模型和工具需要记录和促进科学思维。\n\n“科学思维工具”促进科学思想的发展和互联，用于创作、出版、教学、学习和维护常青笔记。现有工具分为交互式证明助手和文本创作/出版工具，如 LaTeX。\n\n“常青笔记”的概念，灵感来自 Andy Matuschak，强调永久的、不断发展的、跨项目的笔记。原子性至关重要：每个笔记应捕获一个想法，通过其内容和链接笔记可以理解。Sterling 反对传统数学写作缺乏原子性，提倡显式语境而非隐式语境。实现原子性涉及避免自由变量，通过链接偏向于显式依赖关系，并确保符号可以通过链接解码。\n\n虽然 Matuschak 偏爱关联本体，但 Sterling 认为数学知识需要从一开始就进行分层组织，以区分假设和结论。然而，可以在同一节点网络上施加多个分层结构，从而创建不同的“叙述”。因此，界面应支持导航多个父/邻居关系。他提倡相对扁平的结构，以避免过早的复杂性。\n\nSterling 比较了文档标记中的“绝对”（HTML、LaTeX）和“相对”层次结构模型。他认为，相对层次结构，其中节级别由上下文决定，更适合重新语境化，这是流畅科学媒体的一个关键方面。此外，他更喜欢显式的层次结构，即标记语言的语法树结构诱导层次结构，而不是在许多文档标记语言中发现的隐式结构。\n\n他引入了“常青笔记森林”的概念：一个可以出现和发展多个分层结构的集合。单个笔记被认为是森林中的“树”。他根据包含关系定义了树的“范围”。最后，他区分了常青笔记森林中的“作者”和“贡献者”，强调作者身份意味着责任和认可，这可能不适用于其他人对树的后续重新语境化。"
  },
  {
    "id": "44111609",
    "title": "Negotiating PoE+ Power in the Pre‑Boot Environment",
    "url": "https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution",
    "summary": "Roderick recounts his 2015 project involving PoE-powered x86 computers for digital signage. The challenge was powering these computers, requiring 23W, using PoE+, which some network switches wouldn't provide due to a need for LLDP Data Link Layer Classification exceeding 15.4W. The computers couldn't boot fully to send LLDP packets from the OS, creating a Catch-22.\n\nThe solution was to negotiate PoE+ power before the OS loaded, leveraging the UEFI firmware's ability to access the network stack. While initially attempting a custom BIOS build, the project pivoted to developing a UEFI application.\n\nRoderick enlisted Piotr Król, a firmware engineer, who created PoePwrNegotiator, a UEFI application written in C that transmits LLDP-MED packets to request the necessary power. This application ran before the OS, resolving the power issue. The application was deployed and tested and now open-sourced on GitHub under the MIT license. The author hopes PoePwrNegotiator can help others facing similar PoE power negotiation challenges in x86 systems. He offers special thanks to Carlos and Piotr for their vital contributions to the project's success.\n",
    "chinese_title": "在预启动环境中协商PoE+供电",
    "chinese_summary": "罗德里克回顾了他2015年的一个项目，该项目涉及使用PoE供电的x86计算机进行数字标牌。挑战在于为这些需要23W功率的计算机供电，并使用PoE+，但一些网络交换机由于需要超过15.4W的LLDP数据链路层分类而无法提供。计算机无法完全启动以从操作系统发送LLDP数据包，形成了一个两难境地。\n\n解决方案是在操作系统加载之前协商PoE+电源，利用UEFI固件访问网络堆栈的能力。最初尝试构建自定义BIOS，但该项目转向开发UEFI应用程序。\n\n罗德里克请来固件工程师Piotr Król，他创建了PoePwrNegotiator，这是一个用C语言编写的UEFI应用程序，用于发送LLDP-MED数据包以请求必要的电源。该应用程序在操作系统之前运行，解决了电源问题。该应用程序已部署和测试，现在在GitHub上以MIT许可证开源。作者希望PoePwrNegotiator可以帮助其他人在x86系统中面临类似的PoE电源协商挑战。他对Carlos和Piotr为项目成功做出的重要贡献表示特别感谢。"
  },
  {
    "id": "44111673",
    "title": "Look Ma, No Bubbles: Designing a Low-Latency Megakernel for Llama-1B",
    "url": "https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles",
    "summary": "This article introduces a \"megakernel\" approach to significantly reduce latency in running Llama-1B, an open-source language model, on GPUs. The authors found that existing LLM inference engines like vLLM and SGLang only utilize about 50% of available GPU bandwidth due to the overhead of launching many small kernels for each model forward pass (e.g., RMS norm, attention). These kernels have setup and teardown periods that stall memory loading, a critical bottleneck for memory-bound workloads like Llama-1B.\n\nTo address this, they merged the entire Llama-1B forward pass into a single \"megakernel,\" eliminating kernel boundaries and enabling better pipelining of memory loads. This approach resulted in 78% memory bandwidth utilization and a 1.5x performance improvement over existing systems on an H100 GPU.\n\nThe authors detail how they achieved this by:\n\n*   **Fusing numerous operations:** Using an on-GPU interpreter that executes a sequence of instructions per streaming multiprocessor (SM).\n*   **Sharing hardware resources:** Paging shared memory to allow efficient weight loading for subsequent instructions even while previous instructions are still finishing.\n*   **Synchronizing efficiently:** Using a counter system to explicitly manage data dependencies within the megakernel, eliminating the automatic synchronization between traditional kernels.\n\nTheir megakernel achieved sub-millisecond forward passes on an H100 and sub-680 microsecond passes on a B200, showcasing significant latency reduction. The analysis of the B200 forward pass runtime revealed key bottlenecks: storing/loading activations, matrix-vector computations, awaiting weights, synchronization overhead, and setup overhead. They open-sourced their code for others to explore and improve.\n",
    "chinese_title": "妈，你看，没气泡：为Llama-1B设计一个低延迟的巨内核",
    "chinese_summary": "本文介绍了一种“巨内核”（megakernel）方法，旨在显著降低在GPU上运行开源语言模型Llama-1B的延迟。作者发现，现有的LLM推理引擎（如vLLM和SGLang）由于每次模型前向传递（例如，RMS规范化、注意力）都需要启动许多小型内核，导致GPU带宽利用率仅约为50%。这些内核的设置和拆卸过程会阻碍内存加载，而内存加载是Llama-1B等内存密集型工作负载的关键瓶颈。\n\n为了解决这个问题，他们将整个Llama-1B前向传递合并为一个“巨内核”，消除了内核边界，并实现了更好的内存加载流水线。这种方法在H100 GPU上实现了78%的内存带宽利用率，并比现有系统提高了1.5倍的性能。\n\n作者详细介绍了他们如何实现这一点：\n\n*   **融合大量操作：** 使用一个GPU上的解释器，该解释器为每个流式多处理器（SM）执行一系列指令。\n*   **共享硬件资源：** 分页共享内存，即使之前的指令仍在完成，也能为后续指令高效加载权重。\n*   **高效同步：** 使用计数器系统来显式管理巨内核中的数据依赖关系，从而消除传统内核之间的自动同步。\n\n他们的巨内核在H100上实现了亚毫秒级的前向传递，在B200上实现了亚680微秒级的前向传递，展示了显著的延迟降低。对B200前向传递运行时的分析揭示了关键瓶颈：存储/加载激活值、矩阵向量计算、等待权重、同步开销和设置开销。他们开源了他们的代码，供其他人探索和改进。"
  },
  {
    "id": "44087541",
    "title": "Programming Basics with Tiki",
    "url": "https://tiki.li/",
    "summary": "Unable to access the article link.\n",
    "chinese_title": "使用Tiki的编程基础",
    "chinese_summary": "无法访问文章链接。"
  },
  {
    "id": "44090776",
    "title": "The Windows Registry Adventure #7: Attack surface analysis",
    "url": "https://googleprojectzero.blogspot.com/2025/05/the-windows-registry-adventure-7-attack-surface.html",
    "summary": "This blog post by Mateusz Jurczyk delves into the security vulnerabilities of the Windows Registry, highlighting its attractiveness as a target for privilege escalation attacks. The registry is a local attack surface residing within the core kernel, written in C, making it susceptible to logic and memory safety bugs. Its complexity, combined with its role in managing sensitive system information (passwords, permissions), creates potential for both code execution and data-only attacks.\n\nJurczyk emphasizes the registry's aggressive self-healing and recovery mechanisms, while beneficial for system reliability, increase the attack surface as error handling logic can introduce inconsistencies. The post details specific classes of bugs found within the registry, including hive and pool memory corruption, information disclosure, race conditions, logic bugs, and inter-process attacks. A key area of concern is manual reference counting for security descriptors, prone to errors due to untrusted initial values, integer overflows, and mishandling of special key types, potentially leading to use-after-free vulnerabilities. While refcount incrementations are secured now, decrementation may not be, creating a persistent attack surface.\n",
    "chinese_title": "Windows注册表探险#7：攻击面分析",
    "chinese_summary": "Mateusz Jurczyk的这篇博文深入探讨了Windows注册表中的安全漏洞，强调了其作为特权提升攻击目标的吸引力。注册表是一个位于核心内核中的本地攻击面，用C语言编写，使其容易受到逻辑和内存安全漏洞的影响。其复杂性，加上其在管理敏感系统信息（密码、权限）方面的作用，为代码执行和仅数据攻击创造了潜在可能。\n\nJurczyk强调了注册表积极的自我修复和恢复机制，虽然有利于系统可靠性，但也增加了攻击面，因为错误处理逻辑可能会引入不一致性。该博文详细介绍了注册表中发现的特定类型的漏洞，包括hive和池内存损坏、信息泄露、竞争条件、逻辑漏洞以及进程间攻击。一个关键的关注领域是安全描述符的手动引用计数，由于不受信任的初始值、整数溢出和特殊键类型的处理不当，容易出错，可能导致释放后使用漏洞。虽然现在引用计数增加已得到保护，但减少可能并非如此，从而形成持久的攻击面。"
  },
  {
    "id": "44087020",
    "title": "Chairs, Chairs, Chairs",
    "url": "https://www.parliament.uk/about/living-heritage/building/cultural-collections/historic-furniture/the-collection/chairs-chairs-chairs/",
    "summary": "The article \"Chairs, Chairs, Chairs\" on the UK Parliament website highlights the vast and varied collection of chairs held by Parliament, emphasizing their historical significance, artistic merit, and practical function within the Palace of Westminster and other parliamentary buildings. The collection spans several centuries, showcasing the evolution of design, craftsmanship, and upholstery techniques.\n\nThe article points out that many chairs were specifically commissioned for parliamentary purposes, reflecting the changing political landscape and ceremonial needs. These chairs are not merely functional; they serve as tangible links to key events and figures in British history. Examples mentioned likely include Speaker's Chairs, ceremonial chairs used for royal occasions, and chairs used in committee rooms.\n\nFurthermore, the piece stresses the importance of conservation and preservation in maintaining the integrity of these historic objects. The Parliament works to ensure that the chairs remain usable while preserving their historical value for future generations. This involves careful restoration, reupholstery, and environmental control within the parliamentary buildings.\n\nIn essence, the article presents the chair collection as a valuable historical and cultural asset, offering insights into the history of Parliament, British design, and the skills of generations of craftspeople. The collection is an integral part of the Parliamentary estate, reflecting the institution's heritage and continuing relevance.\n",
    "chinese_title": "椅子，椅子，椅子",
    "chinese_summary": "英国议会网站的文章《椅子，椅子，椅子》重点介绍了议会收藏的大量且种类繁多的椅子，强调了它们在威斯敏斯特宫和其他议会建筑中的历史意义、艺术价值和实用功能。 该系列跨越几个世纪，展示了设计、工艺和装饰技术的演变。\n\n这篇文章指出，许多椅子是专门为议会用途定制的，反映了不断变化的政治格局和仪式需求。 这些椅子不仅仅是功能性的；它们是英国历史上重要事件和人物的真实联系。提到的例子可能包括议长座椅、用于皇家场合的礼仪椅以及在委员会会议室中使用的椅子。\n\n此外，这篇文章强调了保护和维护这些历史文物完整性的重要性。 议会致力于确保这些椅子保持可用性，同时为子孙后代保留其历史价值。 这涉及在议会大楼内进行仔细的修复、重新装饰和环境控制。\n\n本质上，这篇文章将椅子收藏视为宝贵的历史和文化资产，提供了对议会历史、英国设计以及几代工匠技能的见解。 该系列是议会庄园不可分割的一部分，反映了该机构的遗产和持续的相关性。"
  },
  {
    "id": "44111452",
    "title": "OpenTPU: Open-Source Reimplementation of Google Tensor Processing Unit (TPU)",
    "url": "https://github.com/UCSBarchlab/OpenTPU",
    "summary": "OpenTPU is an open-source reimplementation of Google's Tensor Processing Unit (TPU), created by UC Santa Barbara ArchLab. It aims to replicate the TPU's inference acceleration capabilities using Python and the PyRTL hardware description language.\n\nThe project is based on publicly available information, notably the \"In-Datacentre Performance Analysis of a Tensor Processing Unit\" paper. OpenTPU simulates the core components of the TPU, including the Matrix Multiply Unit, Unified Buffer, Activation Unit, Accumulator, and Weight FIFO.\n\nCurrently, OpenTPU supports matrix multiplies and ReLU/sigmoid activations, but lacks features like convolution, pooling, and programmable normalization. The instruction set includes RHM, WHM, RW, MMC, ACT, NOP, and HLT. OpenTPU relies on deterministic scheduling, requiring careful operation ordering and NOP padding.\n\nThe project provides both a hardware simulator (runtpu.py) and a functional simulator (sim.py), allowing users to run and test programs. A checker.py script verifies the results between the hardware simulation, the functional simulation, and Tensorflow applications. Users can generate training data with provided python scripts.\n\nKey configuration parameters, like buffer sizes and MM Array dimensions, are adjustable. While it doesn't precisely replicate the TPU's ISA or binary compatibility, OpenTPU offers a valuable platform for research, experimentation, and potential extensions in the field of specialized hardware acceleration for neural networks.\n",
    "chinese_title": "OpenTPU：谷歌张量处理器(TPU)的开源重实现",
    "chinese_summary": "OpenTPU是加州大学圣巴巴拉分校ArchLab创建的谷歌张量处理器(TPU)的开源重新实现。它旨在利用Python和PyRTL硬件描述语言复制TPU的推理加速能力。\n\n该项目基于公开信息，特别是“张量处理单元的数据中心性能分析”论文。OpenTPU模拟TPU的核心组件，包括矩阵乘法单元、统一缓冲器、激活单元、累加器和权重FIFO。\n\n目前，OpenTPU支持矩阵乘法和ReLU/sigmoid激活，但缺乏卷积、池化和可编程归一化等功能。指令集包括RHM、WHM、RW、MMC、ACT、NOP和HLT。OpenTPU依赖于确定性调度，需要仔细的操作排序和NOP填充。\n\n该项目提供硬件模拟器(runtpu.py)和功能模拟器(sim.py)，允许用户运行和测试程序。checker.py脚本验证硬件模拟、功能模拟和Tensorflow应用程序之间的结果。用户可以使用提供的Python脚本生成训练数据。\n\n关键配置参数，如缓冲区大小和MM阵列维度，是可调整的。虽然它不能精确地复制TPU的ISA或二进制兼容性，但OpenTPU为神经网络专用硬件加速领域的研究、实验和潜在扩展提供了一个有价值的平台。"
  },
  {
    "id": "44115467",
    "title": "Show HN: Voiden – a free, offline, Git-native API Client",
    "url": "https://voiden.md",
    "summary": "Voiden is presented as a free, offline, and Git-native API client and workspace. It's designed to provide developers with a way to manage and interact with APIs directly within their Git repositories. This allows for version control, collaboration, and reproducibility of API requests and responses.\n\nThe \"Offline\" aspect is key, suggesting that the tool functions without constant internet connectivity, enhancing portability and reliability. \"Git-Native\" implies seamless integration with Git version control, enabling tracking of API endpoint configurations, request parameters, and responses. This allows developers to treat API interactions as code, enabling branching, merging, and reverting changes.\n\nVoiden aims to address common API workflow challenges by offering a structured and organized environment for creating, testing, and documenting API calls, all while leveraging the benefits of Git for collaboration and history tracking. The \"Voiden.md\" filename suggests that the tool utilizes Markdown for storage and organization, making it easy to read and edit API configurations. In essence, Voiden strives to improve API development workflow by providing a decentralized, version-controlled, and offline-capable solution.\n",
    "chinese_title": "Show HN: Voiden - 一款免费、离线、原生Git的API客户端",
    "chinese_summary": "Voiden 是一款免费、离线且原生Git的API客户端和工作区。它旨在为开发者提供一种直接在Git仓库中管理和交互API的方式，从而实现API请求和响应的版本控制、协作和可重现性。\n\n“离线”特性是关键，表明该工具无需持续的网络连接即可运行，从而提高了便携性和可靠性。“原生Git”意味着与Git版本控制的无缝集成，能够跟踪API端点配置、请求参数和响应。这使得开发者可以将API交互视为代码，从而实现分支、合并和回滚更改。\n\nVoiden 旨在通过提供一个结构化和有组织的环境来创建、测试和记录 API 调用，同时利用 Git 的协作和历史跟踪优势，从而解决常见的 API 工作流程挑战。\"Voiden.md\" 文件名表明该工具使用 Markdown 进行存储和组织，使其易于读取和编辑 API 配置。本质上，Voiden 致力于通过提供一种去中心化、版本控制且具备离线能力的解决方案来改进 API 开发工作流程。"
  },
  {
    "id": "44105965",
    "title": "How a hawk learned to use traffic signals to hunt more successfully",
    "url": "https://www.frontiersin.org/news/2025/05/23/street-smarts-hawk-use-traffic-signals-hunting",
    "summary": "This article snippet is incomplete and contradictory. The title suggests a story about a hawk learning to use traffic signals for hunting, implying animal intelligence and adaptation. However, the content abruptly shifts to a news item about migrating birds carrying invasive ticks and the potential spread of novel diseases.\n\nTherefore, a summary must acknowledge this discrepancy:\n\nThe title introduces a hypothetical scenario where a hawk learns to utilize traffic signals to improve its hunting success. This suggests a narrative about animal intelligence and adaptation to an urban environment.\n\nHowever, the accompanying content has nothing to do with the hawk. It abruptly changes to a news item dated November 18, 2024, discussing the problem of migrating birds carrying invasive ticks. The news piece highlights the risk of these ticks spreading new diseases to different regions.\n\nBecause the content doesn't align with the title, a cohesive summary is impossible. The article appears to be a combination of a fictional premise and a real-world news piece, seemingly unrelated.\n",
    "chinese_title": "老鹰如何学会利用交通信号灯更成功地捕猎",
    "chinese_summary": "这篇文章片段内容不完整且自相矛盾。标题暗示了一个关于鹰学习利用交通信号灯进行捕猎的故事，暗示了动物的智慧和适应能力。然而，内容突然转变为一则关于迁徙鸟类携带入侵蜱虫以及新型疾病潜在传播的新闻。\n\n因此，摘要必须承认这种不一致：\n\n标题引入了一个假设的场景，一只鹰学习利用交通信号灯来提高其捕猎成功率。这暗示了一个关于动物智慧和适应城市环境的叙述。\n\n然而，随附的内容与这只鹰毫无关系。它突然变为一则日期为2024年11月18日的新闻，讨论了迁徙鸟类携带入侵蜱虫的问题。该新闻强调了这些蜱虫将新疾病传播到不同地区的风险。\n\n由于内容与标题不符，因此无法做出连贯的摘要。这篇文章似乎是一个虚构的前提和一个真实世界的新闻报道的结合，两者似乎毫无关联。"
  },
  {
    "id": "44107655",
    "title": "Pyrefly vs. Ty: Comparing Python's two new Rust-based type checkers",
    "url": "https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/",
    "summary": "This article compares two new Rust-based Python type checkers, Pyrefly (by Meta) and ty (by Astral), both aiming to improve upon existing tools like mypy and pylance. Presented at PyCon 2025, both tools are still in early alpha.\n\nWhile both are open-source, incremental, use Ruff for AST parsing, and offer CLI and LSP support, the article highlights key differences in speed, goals, incrementalization, and capabilities.\n\n**Speed:** Benchmarks on PyTorch, Django, and mypy repositories show ty generally faster than Pyrefly, with both significantly outperforming mypy and pyright.\n\n**Goals:** Pyrefly prioritizes aggressive type inference, even in untyped code, while ty focuses on the \"gradual guarantee,\" ensuring that removing type annotations doesn't introduce errors.\n\n**Incrementalization:** Pyrefly uses module-level incrementation (re-parsing entire modules), while ty uses Salsa for fine-grained incrementation (re-parsing individual functions).\n\n**Capabilities:** Pyrefly excels at implicit type inference and has prioritized complex features like generics and overloads. ty is still catching up on these areas. While Pyrefly may catch more errors due to aggressive inference, ty's approach adheres to a \"gradual guarantee\" where it may accept more code as valid that Pyrefly would otherwise complain about.\n\nThe article emphasizes that both tools are under development and their capabilities are rapidly evolving.\n",
    "chinese_title": "Pyrefly 对 Ty：比较 Python 两款基于 Rust 的新型类型检查器",
    "chinese_summary": "本文比较了两款基于 Rust 的新型 Python 类型检查器：Pyrefly (Meta 出品) 和 ty (Astral 出品)，两者都旨在改进现有的工具，如 mypy 和 pylance。这两款工具都在 PyCon 2025 上发布，目前都处于早期 alpha 阶段。\n\n虽然两者都是开源的、增量的、使用 Ruff 进行 AST 解析，并提供 CLI 和 LSP 支持，但本文重点介绍了它们在速度、目标、增量化和功能方面的关键差异。\n\n**速度：** 在 PyTorch、Django 和 mypy 代码库上的基准测试表明，ty 通常比 Pyrefly 更快，并且两者都明显优于 mypy 和 pyright。\n\n**目标：** Pyrefly 优先考虑积极的类型推断，即使在未类型化的代码中也是如此，而 ty 则侧重于“渐进保证”，确保删除类型注解不会引入错误。\n\n**增量化：** Pyrefly 使用模块级增量化（重新解析整个模块），而 ty 使用 Salsa 进行细粒度增量化（重新解析单个函数）。\n\n**功能：** Pyrefly 擅长隐式类型推断，并优先考虑了泛型和重载等复杂功能。ty 仍在这些领域迎头赶上。虽然 Pyrefly 可能由于积极的推断而捕获更多错误，但 ty 的方法坚持“渐进保证”，它可能会接受更多代码为有效，而 Pyrefly 则会对此提出异议。\n\n本文强调，这两款工具都在开发中，其功能正在迅速发展。"
  },
  {
    "id": "44117779",
    "title": "Japan Post launches 'digital address' system",
    "url": "https://www.japantimes.co.jp/business/2025/05/27/companies/japan-post-digital-address/",
    "summary": "Japan Post has launched a \"digital address\" system enabling users to link a seven-digit alphanumeric code to their physical address. Launched on May 27, 2025, the system aims to simplify online transactions. Users can register for a digital address through Japan Post's Yu ID membership service, allowing their address to automatically populate on online shopping and other websites when the code is entered.\n\nA key benefit is that the digital address remains constant even if the user's physical address changes. Users simply need to update their address change with Japan Post, and the new address will be linked to the existing code.\n\nE-commerce giant Rakuten and other companies are considering adopting the system, anticipating improved convenience for users. Japan Post intends to dedicate a decade to promoting widespread adoption of the new \"digital address\" system.\n",
    "chinese_title": "日本邮政推出“数字地址”系统",
    "chinese_summary": "日本邮政推出“数字地址”系统，用户可将七位数字字母代码与其物理地址关联。该系统于2025年5月27日启动，旨在简化在线交易。用户可通过日本邮政的Yu ID会员服务注册数字地址，以便在网上购物和其他网站上输入代码时自动填写地址。\n\n一项关键优势在于，即使用户的物理地址发生变化，数字地址仍保持不变。用户只需在日本邮政更新地址变更信息，新地址将与现有代码关联。\n\n电商巨头乐天和其他公司正在考虑采用该系统，预计将提高用户便利性。日本邮政计划用十年时间来推广新型“数字地址”系统的广泛应用。"
  },
  {
    "id": "44115854",
    "title": "Texas' annual reading test adjusted difficulty yearly, masking improvement",
    "url": "https://theconversation.com/texas-annual-reading-test-adjusted-its-difficulty-every-year-masking-whether-students-are-improving-244159",
    "summary": "This article discusses a recent investigation into Texas' annual reading test, the State of Texas Assessments of Academic Readiness (STAAR), and reveals that the test's design may have masked actual improvements in student performance. From 2012 to 2021, despite increased spending on K-12 education, student scores remained stagnant.\n\nThe author's research found that the STAAR test was designed similarly to a norm-referenced test, meaning it assessed students relative to their peers rather than against fixed standards. This design included adjusting the test's difficulty each year to ensure a relatively consistent failure rate, regardless of whether students were learning more than in previous years. Elements such as omitting easy questions and adjusting scores worked to cancel out the effects of better teaching.\n\nThis has significant implications because high-stakes test scores heavily influence school resources, district control, teacher program accreditation, and even property values. The design disproportionately affects students marginalized by racism, poverty, or language barriers, who historically underperform on such tests. The author plans to investigate if other states use similar methods of testing. While the STAAR test was redesigned in 2022, the author suspects that the scoring methods remain largely the same, which could perpetuate the masking of true student improvement. The Texas Education Agency did not respond to requests for comments.\n",
    "chinese_title": "德州年度阅读测试难度逐年调整，掩盖进步。",
    "chinese_summary": "本文探讨了近期对德克萨斯州年度阅读测试——德克萨斯州学术准备评估（STAAR）的调查，并揭示该测试的设计可能掩盖了学生成绩的实际进步。2012年至2021年间，尽管K-12教育投入增加，但学生成绩依然停滞不前。\n\n作者的研究发现，STAAR测试的设计类似于常模参照测验，意味着它评估学生相对于同龄人的表现，而非针对固定标准。这种设计包括每年调整测试难度，以确保相对一致的失败率，无论学生是否比往年学得更多。省略简单问题和调整分数等因素，抵消了教学质量提高的效果。\n\n这具有重大意义，因为高风险的考试成绩严重影响学校资源、学区控制、教师项目认证，甚至房产价值。这种设计对受种族主义、贫困或语言障碍影响而处于边缘地位的学生造成 disproportionate 的影响，他们在这些测试中历来表现不佳。作者计划调查其他州是否使用类似的测试方法。虽然STAAR测试在2022年进行了重新设计，但作者怀疑评分方法在很大程度上保持不变，这可能会使学生真实进步的掩盖长期存在。德克萨斯州教育局未回应置评请求。"
  },
  {
    "id": "44114631",
    "title": "AI: Accelerated Incompetence",
    "url": "https://www.slater.dev/accelerated-incompetence/",
    "summary": "This article argues that over-reliance on Large Language Models (LLMs) in software engineering accelerates incompetence, not enhances it. The author, a software engineer, identifies risks associated with LLM usage, including incorrect or subtly wrong output, failure to challenge flawed prompts (leading to the XY problem), rapid accumulation of \"tech debt\" leading to an unsanitary codebase, user infantilization through outsourcing critical thinking, and loss of joy in coding.\n\nThe author argues that LLMs cannot replace human critical thinking, particularly in two key areas: program theory and program entropy. Program theory, as defined by Peter Naur, is the mental model or shared understanding of a program's design, crucial for maintainability and future modifications. LLMs, limited by their context window, cannot grasp or retain such a theory. Program entropy, related to the increasing complexity of a codebase over time, requires human intervention to decrease or resist. LLMs, working only at the level of text, tend to introduce unnecessary changes and increase complexity.\n\nThe author concludes that the long-term value proposition for human engineers remains unchanged. Despite the allure of cost reduction through LLMs, the world still needs deep technical skills and critical thinking. LLMs should be used as tools, not crutches, and engineers should continue investing in fundamental skills.\n",
    "chinese_title": "人工智能：加速无能",
    "chinese_summary": "过度依赖大型语言模型(LLM)会加速软件工程领域的无能，而非增强其能力。作者（一位软件工程师）指出了使用LLM相关的风险，包括不正确或略微错误的输出，未能挑战有缺陷的提示（导致XY问题），快速积累“技术债务”导致代码库不卫生，通过外包关键性思考使使用者变得幼稚，以及丧失编程的乐趣。\n\n作者认为，LLM无法取代人类的批判性思维，尤其是在两个关键领域：程序理论和程序熵。程序理论，如Peter Naur所定义，是对程序设计的精神模型或共同理解，对于可维护性和未来的修改至关重要。LLM受其上下文窗口的限制，无法掌握或保留这样的理论。程序熵，与代码库随时间推移而增加的复杂性有关，需要人为干预来减少或抵抗。LLM仅在文本层面工作，往往会引入不必要的更改并增加复杂性。\n\n作者总结说，人类工程师的长期价值主张仍然没有改变。尽管通过LLM降低成本具有诱惑力，但世界仍然需要深厚的技术技能和批判性思维。LLM应该被用作工具，而不是拐杖，工程师应该继续投资于基本技能。"
  },
  {
    "id": "44114621",
    "title": "Microsoft is starting to open Windows Update up to any third-party app",
    "url": "https://www.theverge.com/news/675446/microsoft-windows-update-all-apps-orchestration-platform",
    "summary": "Microsoft is opening up Windows Update to third-party applications through a new \"Windows Update orchestration platform.\" This platform will allow developers to update their apps and drivers through Windows Update, offering a unified update experience alongside core Windows updates and device drivers.\n\nCurrently in private preview, the program is initially focused on business apps but aims to be open to any app or management tool.  Developers using the platform can leverage Windows Update's scheduling based on user activity, battery status, and even sustainable energy timing. They can also integrate with native Windows Update notifications and update history.\n\nThe platform will support MSIX/APPX packaged apps and some custom Win32 apps. By participating, apps will automatically benefit from future improvements to the underlying Windows Update platform.  This move aims to address the fragmented nature of app updates on Windows, where most apps use independent update mechanisms. While Microsoft has previously tried to centralize updates through the Microsoft Store and Windows Package Manager, this new platform provides another avenue for developers to streamline the update process and potentially improve user experience. It remains to be seen whether businesses or major developers like Adobe will embrace this new system.\n",
    "chinese_title": "微软开始向任何第三方应用开放Windows Update。",
    "chinese_summary": "微软正通过一个新的“Windows更新协调平台”向第三方应用程序开放Windows更新。该平台将允许开发者通过Windows更新来更新他们的应用程序和驱动程序，提供与核心Windows更新和设备驱动程序统一的更新体验。\n\n该项目目前处于私有预览阶段，最初专注于商业应用程序，但旨在向任何应用程序或管理工具开放。使用该平台的开发者可以利用Windows更新基于用户活动、电池状态甚至可持续能源时间的调度。他们还可以与原生Windows更新通知和更新历史记录集成。\n\n该平台将支持MSIX/APPX打包的应用程序和一些自定义Win32应用程序。通过参与，应用程序将自动受益于底层Windows更新平台的未来改进。此举旨在解决Windows上应用程序更新的碎片化问题，因为大多数应用程序都使用独立的更新机制。虽然微软之前曾尝试通过Microsoft Store和Windows Package Manager来集中更新，但这个新平台为开发者提供了一种新的途径来简化更新过程，并可能改善用户体验。企业或像Adobe这样的主要开发者是否会接受这个新系统，还有待观察。"
  },
  {
    "id": "44083753",
    "title": "There Is No Diffie-Hellman but Elliptic Curve Diffie-Hellman",
    "url": "https://keymaterial.net/2025/05/23/there-is-no-diffie-hellman-but-elliptic-curve-diffie-hellman/",
    "summary": "This article delves into why elliptic curve Diffie-Hellman is used instead of other groups, like the Monster Group, for cryptography. The core issue is that the private-to-public key map in Diffie-Hellman is a group isomorphism, meaning that, from a purely group-theoretic perspective, private and public keys are indistinguishable. This seemingly undermines the security of the Diffie-Hellman key exchange.\n\nTo understand the choice of elliptic curves, the author introduces category theory and the concept of \"group objects.\" Instead of simply viewing groups as sets with operations, group objects are defined within a broader category using homomorphisms. This allows for a more nuanced understanding of group structure beyond simple isomorphism.\n\nThe article explains how group axioms, like associativity and the existence of a neutral element, can be defined using homomorphisms and the terminal object within a category, even without directly referencing elements. This categorical framework allows us to differentiate between different types of groups.\n\nThe key takeaway is that Diffie-Hellman doesn't just rely on any abstract group; it needs a group object within a specific category. The security of elliptic curve Diffie-Hellman stems from the properties of elliptic curves as group objects over finite fields, particularly the difficulty of solving the elliptic curve discrete logarithm problem, not simply the fact that they are groups.\n",
    "chinese_title": "只有椭圆曲线迪菲-赫尔曼",
    "chinese_summary": "本文深入探讨了为什么椭圆曲线Diffie-Hellman算法被用于密码学，而不是其他群，例如怪物群。核心问题在于Diffie-Hellman算法中，私钥到公钥的映射是一种群同构，这意味着，从纯粹的群论角度来看，私钥和公钥是无法区分的。这似乎破坏了Diffie-Hellman密钥交换的安全性。\n\n为了理解椭圆曲线的选择，作者引入了范畴论和“群对象”的概念。群对象并非简单地将群视为具有运算的集合，而是在更广泛的范畴中使用同态来定义的。这使得人们能够更细致地理解群结构，而不仅仅是简单的同构。\n\n本文解释了如何使用同态和范畴内的终端对象来定义群公理，如结合律和中性元素的存在，甚至无需直接引用元素。这种范畴框架使我们能够区分不同类型的群。\n\n关键在于，Diffie-Hellman算法不仅仅依赖于任何抽象群；它需要特定范畴内的群对象。椭圆曲线Diffie-Hellman算法的安全性源于椭圆曲线作为有限域上的群对象的性质，特别是求解椭圆曲线离散对数问题的难度，而不仅仅是它们是群这一事实。"
  },
  {
    "id": "44081395",
    "title": "The Hobby Computer Culture",
    "url": "https://technicshistory.com/2025/05/24/the-hobby-computer-culture/",
    "summary": "This article explores the emergence of the hobby computer culture from 1975 to 1978, focusing on the enthusiasts who fueled its growth. Initially, personal computers were seen as \"the world's greatest toy,\" appealing to well-educated, affluent men fascinated by the machines themselves – buying, building, programming, and accessorizing them. Practical software applications were secondary, with games, especially Star Trek, being prominent.\n\nThree primary structures connected these hobbyists: local computer clubs, magazines like BYTE, and retail computer stores. The Homebrew Computer Club, while famous, represented a unique political bent not necessarily shared by the broader movement. Clubs across the US offered expertise sharing and a sense of community, while magazines provided information and project ideas. Retail stores, like Dick Heiser's Computer Store and Paul Terrell's Byte Shop, allowed potential buyers to experience the computers firsthand.\n\nThe Southern California Computer Society (SCCS) attempted to create a grand network of support, but collapsed due to mismanagement and lawsuits. The rise of commercial software and direct vendor support gradually diminished the importance of these community networks.\n\nEarly computer sales relied on direct orders to manufacturers like MITS. However, the emergence of retail shops alleviated issues for both buyers and sellers, offering product demonstrations and advice for buyers, and larger, more predictable orders for sellers. While manufacturers initially attempted to control dealerships, retailers ultimately held more power due to competing computer makers. Chain stores, like Byte Shop and ComputerLand, gained advantages through volume discounts and brand recognition.\n",
    "chinese_title": "业余电脑文化",
    "chinese_summary": "本文探讨了1975年至1978年间业余计算机文化的兴起，重点关注推动其发展的爱好者。最初，个人电脑被视为“世界上最伟大的玩具”，吸引了受过良好教育、富裕的男性，他们对机器本身着迷——购买、组装、编程和装饰它们。实用的软件应用是次要的，而游戏，尤其是《星际迷航》，则很突出。\n\n三个主要结构将这些爱好者联系起来：当地计算机俱乐部、像《BYTE》这样的杂志和零售计算机商店。家庭酿造计算机俱乐部虽然出名，但代表了一种独特的政治倾向，不一定被更广泛的运动所共有。美国各地的俱乐部提供专业知识共享和社区意识，而杂志则提供信息和项目创意。像迪克·海泽的计算机商店和保罗·特雷尔的字节商店这样的零售商店，让潜在买家可以亲身体验计算机。\n\n南加州计算机协会 (SCCS) 试图创建一个庞大的支持网络，但由于管理不善和诉讼而崩溃。商业软件和直接供应商支持的兴起逐渐降低了这些社区网络的重要性。\n\n早期的计算机销售依赖于直接向 MITS 等制造商订购。然而，零售商店的出现缓解了买卖双方的问题，为买家提供产品演示和建议，为卖家提供更大、更可预测的订单。虽然制造商最初试图控制经销商，但由于计算机制造商之间的竞争，零售商最终掌握了更大的权力。字节商店和 ComputerLand 等连锁店通过批量折扣和品牌认知获得了优势。"
  },
  {
    "id": "44105619",
    "title": "LumoSQL",
    "url": "https://lumosql.org/src/lumosql/doc/trunk/README.md",
    "summary": "LumoSQL is a security, privacy, performance, and measurement-focused modification of the SQLite embedded data storage library, aiming to demonstrate useful changes that SQLite itself might not readily adopt due to its widespread usage and conservative nature. It is supported by the NLNet Foundation and runs on various architectures and operating systems.\n\nKey features include:\n\n*   **Pluggable Backends:** LumoSQL supports swapping key-value store engines, such as LMDB and Berkeley Database, allowing combinations of different versions of these stores with SQLite.\n*   **Encryption and Corruption Detection:** Adds modern encryption, including per-row Attribute-Based Encryption (ABE) and checksums for error detection and faster search operations.\n*   **Not-Forking Development:** LumoSQL utilizes a novel \"Not-forking\" tool to track upstream changes from SQLite and other projects semi-automatically. This approach enables LumoSQL to combine and configure upstreams without forking.\n\nLumoSQL has limitations, including the need to update its benchmarking tests and fully integrate LMDB and BDB backends. However, the infrastructure is in place to address these issues.\n\nThe article also provides instructions for setting up a build environment and using the LumoSQL Build and Benchmark System to test and compare performance across different configurations. The quickstart guide demonstrates how to benchmark SQLite and LMDB versions and analyze the results.\n",
    "chinese_title": "LumoSQL",
    "chinese_summary": "LumoSQL 是 SQLite 嵌入式数据存储库的一个注重安全、隐私、性能和测量的修改版本，旨在展示一些有用的更改，而 SQLite 本身由于其广泛的使用和保守的性质可能不易采用。 它由 NLNet 基金会支持，并在各种架构和操作系统上运行。\n\n主要功能包括：\n\n*   **可插拔后端：** LumoSQL 支持交换键值存储引擎，例如 LMDB 和 Berkeley Database，从而可以将这些存储的不同版本与 SQLite 结合使用。\n*   **加密和损坏检测：** 增加了现代加密技术，包括每行基于属性的加密 (ABE) 和用于错误检测和更快搜索操作的校验和。\n*   **非分支开发：** LumoSQL 采用一种新颖的“非分支”工具来半自动地跟踪来自 SQLite 和其他项目的上游更改。 这种方法使 LumoSQL 能够组合和配置上游，而无需分支。\n\nLumoSQL 存在一些局限性，包括需要更新其基准测试并完全集成 LMDB 和 BDB 后端。 但是，解决这些问题的基础设施已经就位。\n\n该文章还提供了设置构建环境以及使用 LumoSQL 构建和基准测试系统来测试和比较不同配置的性能的说明。 快速入门指南演示了如何对 SQLite 和 LMDB 版本进行基准测试并分析结果。"
  },
  {
    "id": "44082472",
    "title": "Show HN: Terminal Flower Garden",
    "url": "https://github.com/bdavidzhang/flower-garden-cli",
    "summary": "\"Terminal Flower Garden\" is a CLI game that transforms your terminal into a beautiful digital garden. You can nurture five unique flower types – Spiral Rose, Fractal Tree, Mandala Bloom, Wave Garden, and Star Burst – each growing into distinct mathematical patterns and fractals. The game features a growth system with 10 levels for each flower, persistent save functionality, colorful display using `colorama`, and an interactive menu.\n\nThe game is easy to install using `pip` (recommended), by cloning the GitHub repository, or running directly from the source. To play, launch the game, choose an action from the menu (watering specific flowers, viewing the garden, watering all, resetting, or quitting), and watch the flowers grow.\n\nThe project uses Python 3.7+ and includes `colorama` for colored output. It supports cross-platform functionality (Windows, macOS, and Linux). The game utilizes a modular project structure with a `main.py` file containing the core game logic. Contributions are welcomed via pull requests, and the project is licensed under the MIT License. Users are encouraged to star the project on GitHub to show support and report any issues.\n",
    "chinese_title": "Show HN: 终端花园",
    "chinese_summary": "“终端花园”是一个CLI游戏，可将您的终端转变为一个美丽的数字花园。您可以培育五种独特的花卉类型——螺旋玫瑰、分形树、曼陀罗花、波浪花园和星爆——每种花都会生长成独特的数学模式和分形。该游戏具有针对每种花卉的10个级别的生长系统、持久保存功能、使用`colorama`进行彩色显示以及交互式菜单。\n\n可以使用`pip`（推荐）、克隆GitHub存储库或直接从源代码运行来轻松安装该游戏。要玩游戏，启动游戏，从菜单中选择一个动作（浇灌特定的花朵、查看花园、全部浇灌、重置或退出），并观看花朵生长。\n\n该项目使用Python 3.7+，并包含`colorama`以进行彩色输出。它支持跨平台功能（Windows、macOS 和 Linux）。该游戏利用模块化的项目结构，其中`main.py`文件包含核心游戏逻辑。欢迎通过pull requests贡献代码，该项目已获得 MIT 许可证许可。 鼓励用户在 GitHub 上为该项目加星，以示支持并报告任何问题。"
  },
  {
    "id": "44105796",
    "title": "BGP handling bug causes widespread internet routing instability",
    "url": "https://blog.benjojo.co.uk/post/bgp-attr-40-junos-arista-session-reset-incident",
    "summary": "On May 20, 2025, a BGP handling bug triggered widespread internet routing instability. A BGP update containing a corrupt and unexpected BGP Prefix-SID attribute was the culprit. While some BGP implementations (IOS-XR, Nokia SR-OS) correctly filtered the message, a problematic interaction occurred between JunOS and Arista EOS. JunOS carried the corrupt message, causing Arista EOS devices to reset sessions upon receiving it.\n\nThe attribute was likely added by Starcloud (AS135338) or Hutchison (AS9304) rather than originating from the prefixes' network. Hutchison's presence on numerous internet exchanges amplified the issue, as route servers running Bird (which lacks BGP SID support) distributed the flawed message widely.\n\nThe incident impacted approximately 100 networks, including SpaceX Starlink, Zscaler, Bytedance, and Disney. Message rates on bgp.tools spiked from 20-30k/s to over 150k/s.\n\nThe author criticizes Juniper's JunOS for propagating the faulty message to peers despite having BGP error tolerance. The author emphasizes the potential for severe consequences, extending beyond mere consumer inconvenience to impacting critical services like TV broadcasts and emergency calls. They advocate for improved BGP error handling and encourage more networks to contribute data to bgp.tools for future debugging efforts.\n",
    "chinese_title": "BGP处理漏洞导致互联网路由大范围不稳定",
    "chinese_summary": "2025年5月20日，一起BGP处理漏洞引发了广泛的互联网路由不稳定。 罪魁祸首是一个包含损坏且意外BGP Prefix-SID属性的BGP更新。 虽然一些BGP实现（IOS-XR、Nokia SR-OS）正确地过滤了该消息，但JunOS和Arista EOS之间发生了有问题的交互。 JunOS传递了损坏的消息，导致Arista EOS设备在收到该消息时重置会话。\n\n该属性很可能是由Starcloud (AS135338) 或 Hutchison (AS9304) 添加的，而不是源自前缀的网络。 由于运行Bird（缺乏BGP SID支持）的路由服务器广泛分发了有缺陷的消息，Hutchison在众多互联网交换中心的存在加剧了这个问题。\n\n该事件影响了大约100个网络，包括SpaceX Starlink、Zscaler、ByteDance和Disney。 bgp.tools上的消息速率从20-30k/s飙升至超过150k/s。\n\n作者批评Juniper的JunOS尽管具有BGP错误容忍度，但仍将错误消息传播给对等方。 作者强调了可能造成的严重后果，不仅限于消费者的不便，还可能影响电视广播和紧急呼叫等关键服务。 他们倡导改进BGP错误处理，并鼓励更多网络为bgp.tools贡献数据，以便将来进行调试。"
  },
  {
    "id": "44103839",
    "title": "Show HN: Lazy Tetris",
    "url": "https://lazytetris.com/",
    "summary": "The \"Show HN: Lazy Tetris\" submission likely presents a simplified or automated version of the classic Tetris game. The title implies a focus on ease of use or minimal player interaction. Possible aspects include:\n\n*   **Automated Gameplay:** The core mechanic of the game might be automated, such as automatically placing pieces, clearing lines, or even playing the game entirely autonomously. This contrasts with traditional Tetris, which requires active player control.\n*   **Simplified Mechanics:** The game may remove or simplify elements of traditional Tetris, such as piece rotation, manual piece placement, or gravity control. The focus could be on simply watching the game progress or making high-level decisions.\n*   **Passive Entertainment:** \"Lazy Tetris\" might be designed as a form of passive entertainment, where the user can observe the game without actively participating, perhaps optimized for efficiency or aesthetic appeal.\n*   **Educational Tool:** It is possible the submission is an educational tool demonstrating Tetris algorithms, AI, or game design principles. The \"lazy\" aspect could refer to minimal player input needed to initiate these simulations.\n*   **Novel Twist:** The game might introduce a novel twist to the Tetris formula that prioritizes simplicity or automation over traditional skill-based gameplay.\n\nWithout more context on the specific implementation, the most likely interpretation is a simplified Tetris game designed for passive viewing or automated gameplay, potentially with educational applications or a novel twist on the traditional formula.\n",
    "chinese_title": "显示 HN：懒人俄罗斯方块",
    "chinese_summary": "“Show HN: Lazy Tetris” 提交可能呈现的是经典俄罗斯方块游戏的简化或自动化版本。标题暗示了对易用性或最少玩家互动的关注。可能涉及的方面包括：\n\n*   **自动化游戏玩法：** 游戏的核心机制可能是自动化的，例如自动放置方块、消除行，甚至完全自主地玩游戏。这与需要玩家主动控制的传统俄罗斯方块形成对比。\n*   **简化机制：** 游戏可能会移除或简化传统俄罗斯方块的元素，例如方块旋转、手动放置方块或重力控制。重点可能仅仅是观看游戏进程或做出高级决策。\n*   **被动娱乐：** “Lazy Tetris” 可能被设计为一种被动娱乐形式，用户可以在不积极参与的情况下观察游戏，也许针对效率或美观性进行了优化。\n*   **教育工具：** 有可能该提交是一个教育工具，用于演示俄罗斯方块算法、人工智能或游戏设计原则。“Lazy”方面可能指的是启动这些模拟所需的最小玩家输入。\n*   **新颖转折：** 游戏可能会在俄罗斯方块公式中引入一种新颖的转折，这种转折优先考虑简单性或自动化，而不是传统的基于技能的游戏玩法。\n\n在没有关于具体实现的更多信息的情况下，最可能的解释是一个简化的俄罗斯方块游戏，专为被动观看或自动化游戏玩法而设计，可能具有教育应用或对传统公式的新颖转折。"
  },
  {
    "id": "44110931",
    "title": "Mustard Watches (1990)",
    "url": "https://girard.perso.math.cnrs.fr/mustard/article.html",
    "summary": "\"Mustard Watches\" (1990), attributed to Y.-J. Ringard and reconstituted by Pierre Barthélémy and Éric Lozingot, appears to be an article or document, potentially related to timepieces featuring mustard in some way. The document consists of four pages, but without further context, the exact nature of the content remains unclear. The title suggests a focus on watches, and the presence of \"mustard\" implies a unique characteristic or theme associated with these watches. The reconstitution by Barthélémy and Lozingot indicates the original article may have been incomplete or fragmented, requiring restoration or compilation. It's impossible to determine the subject matter's specifics (e.g., design, historical significance, or novel use of mustard) without examining the actual content of the four pages.\n",
    "chinese_title": "芥末手表 (1990)",
    "chinese_summary": "《芥末手表》（1990），署名Y.-J. Ringard，由Pierre Barthélémy和Éric Lozingot重新整理，看似一篇可能与某种以芥末为特色的钟表相关的文章或文档。该文档共四页，但缺乏进一步的背景信息，内容的确切性质仍不明确。标题暗示着对腕表的关注，“芥末”的存在则暗示了这些腕表独特的特征或主题。Barthélémy和Lozingot的重新整理表明原始文章可能不完整或碎片化，需要修复或汇编。在检查这四页的实际内容之前，无法确定主题的具体内容（例如，设计、历史意义或芥末的创新用途）。"
  },
  {
    "id": "44116722",
    "title": "The 'Green' Aviation Fuel That Would Increase Carbon Emissions",
    "url": "https://e360.yale.edu/features/corn-soy-biofuel-aviation-congress",
    "summary": "This article critiques the push for \"sustainable aviation fuel\" (SAF) derived from crops like corn and soybeans, arguing that it's a false climate solution. While aviation seeks to decarbonize by using SAF, the article contends that relying on crops has detrimental effects.\n\nThe author highlights that using crops for fuel increases food prices and global hunger, leading to deforestation and grassland conversion as farmers worldwide seek new land to compensate for food production losses. The GOP's \"Big Beautiful Bill\" is criticized for extending tax credits for SAF while banning consideration of land-use emissions, effectively subsidizing a harmful practice. The EU excludes crop-based fuels for aviation due to their devastating land-use impacts.\n\nThe article emphasizes the power of the U.S. agricultural lobby, which seeks new markets for corn ethanol and soy biodiesel as electric vehicles threaten demand in the automotive sector. The author points out the bipartisan support for biofuels, even within legislation aimed at dismantling climate policies.\n\nThe article cites analyses showing the significant land requirements for SAF production, potentially leading to widespread deforestation and carbon release. The Biden administration's use of the GREET model, which downplays indirect land-use change (ILUC), is criticized as prioritizing agricultural interests over genuine climate benefits. The author highlights that recycled cooking oil is a more climate-friendly alternative.\n",
    "chinese_title": "会增加碳排放的“绿色”航空燃料",
    "chinese_summary": "本文批判了以玉米和大豆等作物为原料生产“可持续航空燃料”（SAF）的做法，认为这是一种虚假的应对气候变化的方案。航空业试图通过使用SAF来实现脱碳，但本文认为，依赖作物会产生不利影响。\n\n作者强调，将作物用于燃料会提高食品价格和加剧全球饥饿，导致森林砍伐和草原转变为农田，因为世界各地的农民都在寻找新的土地来弥补粮食生产的损失。共和党的“美丽大法案”因延长SAF的税收抵免，同时禁止考虑土地利用排放而被批评，这实际上是在补贴一种有害的做法。欧盟因其对土地利用的破坏性影响而将基于作物的燃料排除在航空业之外。\n\n文章强调了美国农业游说团体的力量，他们寻求为玉米乙醇和大豆生物柴油寻找新市场，因为电动汽车威胁着汽车行业的需求。作者指出，两党都支持生物燃料，即使是在旨在废除气候政策的立法中也是如此。\n\n文章引用分析表明，SAF生产需要大量土地，可能导致广泛的森林砍伐和碳释放。拜登政府使用GREET模型，该模型淡化了间接土地利用变化（ILUC），这被批评为优先考虑农业利益，而非真正的气候效益。作者强调，回收食用油是一种更具气候友好性的替代方案。"
  },
  {
    "id": "44084892",
    "title": "Space Selfie",
    "url": "https://space.crunchlabs.com/",
    "summary": "CrunchLabs is offering a free service called \"Space Selfie\" where they send your uploaded selfie to their satellite, SAT GUS, in space. SAT GUS, named after CrunchLabs' squirrel mascot Phat Gus, will take a photo of your selfie displayed on a phone with Earth as the backdrop, and then beam the combined image back to you.\n\nThe initiative aims to provide an epic and verifiable space experience for everyone. To ensure authenticity, the image will show both your selfie on the phone screen and the Earth behind it. The article addresses frequently asked questions about the project, including the origin of the satellite's name, its launch, and potential issues. It also covers details regarding the image requirements, the service's cost (free), eligibility, privacy, image return time, satellite tracking, camera specifications, and explanations for certain visual elements in the resulting image, such as the absence of stars or variances in cloud appearance. The article also touches on the partners involved in the Space Selfie project. The ultimate goal is to provide a unique and shareable \"Space Selfie\" for participants.\n",
    "chinese_title": "太空自拍",
    "chinese_summary": "CrunchLabs 提供一项名为“太空自拍”的免费服务，他们会将你上传的自拍照发送到其位于太空的卫星 SAT GUS。SAT GUS 以 CrunchLabs 的松鼠吉祥物 Phat Gus 命名，它会将你的自拍照显示在手机上，并以地球为背景拍摄一张照片，然后将合成的图像传回给你。\n\n该计划旨在为每个人提供一次史诗般的、可验证的太空体验。为确保真实性，图像将同时显示你手机屏幕上的自拍照和背景中的地球。文章解答了关于该项目的常见问题，包括卫星名称的由来、发射情况以及潜在问题。文章还涵盖了图像要求、服务费用（免费）、资格、隐私、图像返回时间、卫星跟踪、相机规格，以及对最终图像中某些视觉元素的解释，例如没有星星或云层外观的变化。文章还提到了参与“太空自拍”项目的合作伙伴。最终目标是为参与者提供一张独特的、可分享的“太空自拍”。"
  },
  {
    "id": "44109257",
    "title": "Running GPT-2 in WebGL: Rediscovering the Lost Art of GPU Shader Programming",
    "url": "https://nathan.rs/posts/gpu-shader-programming/",
    "summary": "This article details the author's project of implementing GPT-2 using WebGL and shaders, offering a glimpse into general-purpose GPU (GPGPU) programming's historical roots and its current limitations. It traces the evolution from fixed-function graphics pipelines to programmable shaders and dedicated compute APIs like CUDA and OpenCL. While these APIs offer more direct and efficient methods, the article explores how traditional graphics API features (like textures, framebuffers, and vertex/fragment shaders) can be \"hijacked\" for computation.\n\nTextures are used as tensors, storing numerical data, and framebuffers act as containers for redirecting rendering output into these textures. Fragment shaders are repurposed as compute kernels, enabling parallel computation across thousands of GPU cores. The process involves chaining shader passes for operations like matrix multiplication and activation functions, keeping the entire GPT-2 pipeline on the GPU until the final logits are extracted.\n\nHowever, the article acknowledges significant limitations of this approach, including the lack of shared/local memory, texture size restrictions, absence of synchronization mechanisms, and overhead associated with draw calls. While it's an interesting educational exercise, shader-based compute is ultimately less practical compared to using dedicated compute APIs. The author provides a link to the code repository for those interested in exploring the implementation further.\n",
    "chinese_title": "在WebGL中运行GPT-2：重拾GPU着色器编程的失落艺术",
    "chinese_summary": "本文详细介绍了作者使用 WebGL 和着色器实现 GPT-2 的项目，让人们得以一窥通用 GPU (GPGPU) 编程的历史渊源及其当前的局限性。文章追溯了从固定功能图形管线到可编程着色器以及像 CUDA 和 OpenCL 这样的专用计算 API 的演变过程。虽然这些 API 提供了更直接和高效的方法，但本文探讨了如何“劫持”传统的图形 API 特性（如纹理、帧缓冲区和顶点/片段着色器）来进行计算。\n\n纹理被用作张量，存储数值数据，帧缓冲区则充当容器，用于将渲染输出重定向到这些纹理中。片段着色器被重新用作计算内核，从而能够在数千个 GPU 核心上实现并行计算。该过程涉及链接着色器通道以进行矩阵乘法和激活函数等操作，从而将整个 GPT-2 流程保持在 GPU 上，直到提取最终 logits。\n\n然而，文章承认了这种方法的重大局限性，包括缺乏共享/本地内存、纹理大小限制、缺乏同步机制以及与绘制调用相关的开销。虽然这是一个有趣的教育实践，但与使用专用计算 API 相比，基于着色器的计算最终实用性较低。作者提供了代码仓库的链接，供那些有兴趣进一步探索实现的人使用。"
  },
  {
    "id": "44086219",
    "title": "The length of file names in early Unix",
    "url": "https://utcc.utoronto.ca/~cks/space/blog/unix/UnixEarlyFilenameLenghts",
    "summary": "Chris Siebenmann's blog (Wandering Thoughts) is blocking access to users with suspiciously old browsers due to a surge in high-volume crawlers using outdated Chrome user agents, likely for LLM training data collection. This is a measure to reduce server load.\n\nIf legitimate users with current browsers are blocked, they are requested to contact Siebenmann with their browser details (including User-Agent string) at his university email address.\n\nThe note specifically addresses users accessing the blog through archiving services like archive.today, archive.ph, and archive.is. These services are being blocked because their crawling behavior is indistinguishable from malicious bots: they use old Chrome user agents, crawl from widely distributed IP addresses (some with falsified reverse DNS records claiming to be Googlebot), and are not clearly identified. Siebenmann recommends using archive.org instead, as it is a better-behaved archival crawler.\n",
    "chinese_title": "早期Unix文件名的长度",
    "chinese_summary": "克里斯·西本曼的博客 (Wandering Thoughts) 正在阻止使用可疑旧浏览器的用户访问，原因是大量使用过时 Chrome 用户代理的高流量爬虫激增，可能用于 LLM 训练数据收集。这是一项旨在降低服务器负载的措施。\n\n如果使用最新浏览器的合法用户被阻止，他们被要求通过他的大学邮箱地址联系西本曼，并提供其浏览器详细信息（包括 User-Agent 字符串）。\n\n该说明特别针对通过 archive.today、archive.ph 和 archive.is 等存档服务访问该博客的用户。这些服务正在被阻止，因为它们的爬取行为与恶意机器人无法区分：它们使用旧的 Chrome 用户代理，从广泛分布的 IP 地址进行爬取（有些 IP 地址伪造了反向 DNS 记录，声称是 Googlebot），并且没有明确的身份标识。西本曼建议改用 archive.org，因为它是一个行为更好的存档爬虫。"
  },
  {
    "id": "44105470",
    "title": "Revisiting the algorithm that changed horse race betting (2023)",
    "url": "https://actamachina.com/posts/annotated-benter-paper",
    "summary": "This article revisits Bill Benter's pioneering work in applying mathematical models to horse race betting, focusing on his 1994 paper on computerized handicapping. Benter, who amassed a $1 billion fortune betting on Hong Kong races, documented his successful model in the paper. While the published model is likely outdated, the article highlights its insightful application of mathematics to an unconventional field, especially considering the hardware and software limitations of the time.\n\nThe article presents an annotated version of Benter's paper, incorporating code blocks and comments. Instead of replicating the original models, it focuses on analyzing the public estimate (derived from Hong Kong Jockey Club's historical win odds), examining how Benter generated model calibration tables, assessing the public estimate's improvements over time, and experimenting with fitting adjustment factors using PyTorch.\n\nThe article emphasizes the advantages of a computer-based approach, including its empirical nature, testability via data partitioning, and consistency. It also acknowledges the significant preparatory effort required, including data collection, verification, and programming.\n\nThe discussion of handicapping model development centers on the multinomial logit model used by Benter, later replaced by the Probit model. The article explores various factors influencing a horse's \"current performance potential,\" categorized as current condition, past performance, adjustments to past performance, present race situational factors, and preferences. It also discusses the importance of defining factors to extract as much information as possible and the refinement process based on educated guessing and trial and error.\n",
    "chinese_title": "重新审视改变赛马博彩的算法 (2023)",
    "chinese_summary": "本文回顾了比尔·本特在将数学模型应用于赛马投注方面的开创性工作，重点关注他1994年关于计算机辅助评估的论文。本特凭借投注香港赛马积累了10亿美元的财富，并在论文中记录了他成功的模型。尽管发表的模型可能已经过时，但本文强调了其在非常规领域中对数学的深刻应用，尤其是在考虑到当时的硬件和软件限制的情况下。\n\n本文呈现了本特论文的注释版本，其中包含代码块和注释。它没有复制原始模型，而是侧重于分析公众估计（源自香港赛马会历史获胜赔率），研究本特如何生成模型校准表，评估公众估计随时间的改进，并尝试使用PyTorch拟合调整因子。\n\n本文强调了基于计算机方法的优势，包括其经验性质、通过数据分区进行的可测试性以及一致性。它还承认了所需的大量准备工作，包括数据收集、验证和编程。\n\n对评估模型开发的讨论主要集中在本特使用的多项logit模型上，后来被Probit模型取代。本文探讨了影响赛马“当前表现潜力”的各种因素，这些因素分为当前状况、过去的表现、对过去表现的调整、当前比赛情境因素和偏好。它还讨论了定义因素以尽可能多地提取信息的重要性，以及基于有根据的猜测和反复试验的改进过程。"
  },
  {
    "id": "44100148",
    "title": "CSS Minecraft",
    "url": "https://benjaminaster.com/css-minecraft/",
    "summary": "Unable to access the article link.\n",
    "chinese_title": "CSS我的世界",
    "chinese_summary": "无法访问文章链接。"
  },
  {
    "id": "44110270",
    "title": "In Vietnam, an unlikely outpost for Chicano culture",
    "url": "https://www.latimes.com/world-nation/story/2025-05-27/chicano-culture-vietnam",
    "summary": "In Ho Chi Minh City, a subculture of \"Viet Chicanos\" has emerged, centered around barbers and tattoo artists who embrace Chicano culture. Nguyen Phuoc Loc, a barber who has never been to the U.S., exemplifies this trend, adorning himself and his barbershop with Chicano symbols like the Virgen de Guadalupe and Gothic lettering. This movement, which started about 10 years ago with Nguyen Huynh Thanh Liem's Chicano-themed barbershop, offers a sense of belonging and identity to its members.\n\nProfessor Ignacio Lopez Calvo notes that Chicano culture's rebellious nature appeals to those seeking to defy conservative Asian norms. The article also highlights a historical connection, pointing out that Chicano soldiers in the Vietnam War felt a connection to the Vietnamese people.\n\nWhile the Viet Chicanos gain popularity on social media, particularly on TikTok, they also face criticism, especially from older generations associating tattoos with gangs. The core community remains small due to this social stigma. Members emphasize they are not appropriating the culture, but inheriting and celebrating its positive aspects, such as family, loyalty, and resilience. Michael Phan, a tattoo artist from Germany, underscores that embracing Chicano ideals goes beyond appearance.\n",
    "chinese_title": "在越南，奇特的奇卡诺文化前哨",
    "chinese_summary": "在胡志明市，一个以理发师和纹身师为中心的“越裔奇卡诺人”亚文化正在兴起，他们拥抱奇卡诺文化。从未去过美国的理发师阮福禄就是这一趋势的代表，他用瓜达卢佩圣母和哥特式字母等奇卡诺符号装饰自己和他的理发店。这场运动大约在10年前由阮黄青廉以奇卡诺为主题的理发店发起，为成员提供了归属感和认同感。\n\n伊格纳西奥·洛佩斯·卡尔沃教授指出，奇卡诺文化的叛逆本质吸引了那些寻求挑战亚洲保守规范的人。文章还强调了一种历史联系，指出参加越南战争的奇卡诺士兵对越南人民产生了一种联系感。\n\n虽然越裔奇卡诺人在社交媒体上，特别是在TikTok上越来越受欢迎，但他们也面临批评，尤其来自将纹身与黑帮联系在一起的老一代人。由于这种社会污名，核心社群仍然很小。成员们强调，他们不是挪用文化，而是继承和庆祝其积极方面，如家庭、忠诚和坚韧。来自德国的纹身艺术家迈克尔·潘强调，拥抱奇卡诺的理想不仅仅是外表。"
  },
  {
    "id": "44111898",
    "title": "A privilege escalation from Chrome extensions (2023)",
    "url": "https://0x44.xyz/blog/cve-2023-4369/",
    "summary": "Derin Eryılmaz details a privilege escalation vulnerability found in Chrome extensions on ChromeOS (CVE-2023-4369). The vulnerability allowed extensions with only \"downloads\" permission to execute code within the privileged `chrome://file-manager` context.\n\nThe exploit involves a malicious extension downloading an HTML file and then opening it using a special `filesystem:chrome://file-manager` URL. This circumvents security restrictions, granting the extension the ability to read sensitive pages and access the `chrome.fileManagerPrivate` API.\n\nThis API grants significant power, enabling the extension to read and write user's downloaded files, manipulate Crostini (the built-in Linux terminal), and potentially execute arbitrary code within a Linux container. This could be used for ransomware or other malicious activities.\n\nThe author also discovered a similar bug in the \"Image Loader\" component extension on ChromeOS. This bug allowed code execution within `chrome://resources`, granting a second Chrome XSS.\n\nThe root cause lies in the `filesystem:` protocol, a legacy Chrome feature allowing storage and access of files in a virtual file system. The `/external` path, unique to ChromeOS, exposes the user's `MyFiles` directory.\n\nWhile the most severe aspect of the File Manager bug was quickly patched, the vulnerability demonstrates the potential for Chrome extensions to escalate privileges and bypass security sandboxes on ChromeOS, leading to significant privacy and security risks. The exploit required no user interaction beyond installing the malicious extension.\n",
    "chinese_title": "Chrome扩展的权限提升 (2023)",
    "chinese_summary": "Derin Eryılmaz 详细描述了在 ChromeOS 上的 Chrome 扩展程序中发现的一个权限提升漏洞 (CVE-2023-4369)。该漏洞允许仅具有 \"downloads\" 权限的扩展程序在特权 `chrome://file-manager` 上下文中执行代码。\n\n该漏洞利用涉及恶意扩展程序下载一个 HTML 文件，然后使用特殊的 `filesystem:chrome://file-manager` URL 打开它。这绕过了安全限制，使该扩展程序能够读取敏感页面并访问 `chrome.fileManagerPrivate` API。\n\n此 API 赋予了强大的权力，使扩展程序能够读取和写入用户下载的文件，操纵 Crostini（内置 Linux 终端），并可能在 Linux 容器中执行任意代码。 这可能被用于勒索软件或其他恶意活动。\n\n作者还在 ChromeOS 上的 \"Image Loader\" 组件扩展程序中发现了类似的错误。 此错误允许在 `chrome://resources` 中执行代码，从而导致第二个 Chrome XSS。\n\n根本原因在于 `filesystem:` 协议，这是一个允许在虚拟文件系统中存储和访问文件的遗留 Chrome 功能。 `/external` 路径是 ChromeOS 独有的，它暴露了用户的 `MyFiles` 目录。\n\n虽然文件管理器错误中最严重的部分已迅速修复，但该漏洞表明了 Chrome 扩展程序在 ChromeOS 上提升权限和绕过安全沙箱的潜力，从而导致严重的隐私和安全风险。 除安装恶意扩展程序外，该漏洞利用不需要任何用户交互。"
  },
  {
    "id": "44106764",
    "title": "The Art of Fugue – Contrapunctus I (2021)",
    "url": "https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/",
    "summary": "This article explores J.S. Bach's *Art of Fugue*, specifically *Contrapunctus I*, and why it might resonate with modern listeners despite its initial unpopularity and didactic nature. The author explains that *The Art of Fugue*, published posthumously, was initially unfashionable due to its complex counterpoint. The author's appreciation is enhanced by visualizing its structure.\n\n*Contrapunctus I* is relatively simple compared to the other fugues, lacking the elaborate transformations of the subject found in later pieces. The author provides examples of different instrumental interpretations, from string quartets to saxophone quartets, showcasing its versatility.\n\nJoseph Kerman's analysis highlights *Contrapunctus I*'s unique position as a fugue lacking typical contrapuntal devices. Kerman notes Bach's deliberate restraint and the piece's gradual increase in complexity, culminating in a powerful cadence. The author draws a parallel between Bach's improvisational style and jazz, emphasizing the interplay of voices and the development of themes.\n\nFinally, the author discusses the appeal of remixing *Contrapunctus I* with a modern beat, offering a remix of Angela Hewitt's recording with Jay-Z and Alicia Keys' \"Empire State of Mind\". This unexpected combination, intended to be only partially ironic, helps maintain focus and highlights the inherent groove potential of Bach's music, enhancing its didactic function and creating a unique aesthetic.\n",
    "chinese_title": "赋格的艺术 – 对位法 I (2021)",
    "chinese_summary": "本文探讨了J.S.巴赫的《赋格的艺术》，特别是其中的《对位法I》，以及尽管它最初并不受欢迎且具有教谕性，但为何它可能引起现代听众的共鸣。作者解释说，《赋格的艺术》是巴赫死后出版的，最初因其复杂的对位法而不流行。通过可视化其结构，作者的欣赏得到了提升。\n\n与其他赋格相比，《对位法I》相对简单，缺乏在后期作品中发现的主题的精细变形。作者提供了不同的乐器演奏版本的例子，从弦乐四重奏到萨克斯管四重奏，展示了它的多功能性。\n\n约瑟夫·克尔曼的分析突出了《对位法I》作为缺乏典型对位技巧的赋格的独特地位。克尔曼指出巴赫有意的克制以及作品复杂性的逐渐增加，最终达到一个强有力的终止式。作者将巴赫的即兴风格与爵士乐相提并论，强调了各声部的相互作用和主题的发展。\n\n最后，作者讨论了将《对位法I》与现代节拍进行混音的吸引力，并提供了一个对安吉拉·休伊特的录音与Jay-Z和艾丽西亚·凯斯演唱的《帝国之心》的混音版本。这种出乎意料的组合，旨在仅部分带有讽刺意味，有助于保持专注，并突出了巴赫音乐固有的律动潜力，从而增强了其教谕功能并创造了一种独特的审美。"
  },
  {
    "id": "44112218",
    "title": "Using Postgres pg_test_fsync tool for testing low latency writes",
    "url": "https://tanelpoder.com/posts/using-pg-test-fsync-for-testing-low-latency-writes/",
    "summary": "This article highlights the importance of low latency writes for databases, particularly for WAL/redo logs. It introduces the `pg_test_fsync` tool, bundled with Postgres, as a valuable resource for evaluating the suitability of disks (or cloud block store volumes) for workloads requiring fast writes, regardless of the database system used.\n\nThe author demonstrates the tool's usage, comparing a consumer-grade NVMe SSD (Samsung 990 Pro) and an enterprise-grade SSD (Micron 7400) with power-loss protection (PLP). The results show significant differences in synchronous write latency. The consumer SSD, lacking DRAM-based PLP write cache, exhibits considerably higher latency (around 1.6ms for a single 8kB `fdatasync` write) compared to the enterprise SSD (around 24 microseconds for the same write).\n\nThe article emphasizes that high write IOPS doesn't guarantee low latency for individual writes. Consumer SSDs can serialize writes, increasing latency. Buffering writes in OS RAM and then using `fdatasync` for bulk synchronization can improve throughput, but not individual write latency. The enterprise SSD, leveraging its PLP and write-through cache, provides significantly faster synchronous write acknowledgements. The author also points out the impact of sector size on the tool's operation, where small IO sizes may lead to failures if not aligned with the physical sector size of the disk.\n",
    "chinese_title": "使用Postgres pg_test_fsync工具测试低延迟写入",
    "chinese_summary": "本文重点强调了低延迟写入对于数据库的重要性，特别是对于WAL/redo日志。文章介绍了Postgres自带的`pg_test_fsync`工具，认为它是评估磁盘（或云块存储卷）是否适合需要快速写入的工作负载的宝贵资源，无论使用何种数据库系统。\n\n作者演示了该工具的用法，比较了消费级NVMe SSD（Samsung 990 Pro）和企业级SSD（Micron 7400），后者具有断电保护（PLP）。结果显示同步写入延迟存在显著差异。缺乏基于DRAM的PLP写入缓存的消费级SSD表现出明显更高的延迟（对于单个8kB的`fdatasync`写入，大约为1.6ms），而企业级SSD的延迟则低得多（对于相同的写入，大约为24微秒）。\n\n文章强调，高写入IOPS并不能保证单个写入的低延迟。消费级SSD可能会序列化写入，从而增加延迟。在OS RAM中缓冲写入，然后使用`fdatasync`进行批量同步可以提高吞吐量，但不能提高单个写入的延迟。企业级SSD利用其PLP和直写缓存，可提供明显更快的同步写入确认。作者还指出了扇区大小对工具操作的影响，如果小的IO大小与磁盘的物理扇区大小未对齐，可能会导致失败。"
  },
  {
    "id": "44099187",
    "title": "Show HN: PgDog – Shard Postgres without extensions",
    "url": "https://github.com/pgdogdev/pgdog",
    "summary": "PgDog is a new, fast, and secure open-source tool, written in Rust, for sharding PostgreSQL databases *without* relying on PostgreSQL extensions. It acts as a transaction pooler and logical replication manager, designed to handle hundreds of databases and thousands of connections.\n\nKey features include:\n\n*   **Load balancing:** Distributes transactions across multiple PostgreSQL replicas or primaries with various strategies (round robin, random, etc.), routing SELECT queries to replicas and others to the primary.\n*   **Healthchecks & Failover:** Automatically reroutes queries away from unhealthy hosts, maximizing availability.\n*   **Transaction pooling:** Like PgBouncer, supports transaction and session pooling.\n*   **Sharding:** Automatically routes queries to the appropriate shards based on sharding keys, even handling cross-shard queries.\n*   **COPY Command Support:** Automatically splits COPY commands across shards.\n*   **Logical Replication:** Splits data between databases for sharding without downtime.\n*   **Configuration:** Highly configurable at runtime like PgBouncer/PgCat.\n\nPgDog offers Kubernetes and Docker deployment options and exposes both a PgBouncer-style admin database and an OpenMetrics endpoint for monitoring. It is licensed under AGPL v3, allowing internal use and private modifications without requiring source code sharing, except for those offering PgDog as a public service. The project is in its early stages, but welcomes early adopters and regularly benchmarks performance.\n",
    "chinese_title": "展示HN: PgDog – 无需扩展即可分片Postgres",
    "chinese_summary": "PgDog 是一个用 Rust 编写的全新、快速且安全的开源工具，用于对 PostgreSQL 数据库进行分片，*无需*依赖 PostgreSQL 扩展。它充当事务连接池和逻辑复制管理器，旨在处理数百个数据库和数千个连接。\n\n主要功能包括：\n\n*   **负载均衡：** 通过各种策略（轮询、随机等）在多个 PostgreSQL 副本或主服务器之间分配事务，将 SELECT 查询路由到副本，并将其他查询路由到主服务器。\n*   **健康检查和故障转移：** 自动将查询从不健康的服务器重定向，从而最大限度地提高可用性。\n*   **事务连接池：** 类似于 PgBouncer，支持事务和会话连接池。\n*   **分片：** 根据分片键自动将查询路由到适当的分片，甚至可以处理跨分片查询。\n*   **COPY 命令支持：** 自动将 COPY 命令拆分到各个分片。\n*   **逻辑复制：** 在数据库之间拆分数据以进行分片，而无需停机。\n*   **配置：** 像 PgBouncer/PgCat 一样，在运行时具有高度可配置性。\n\nPgDog 提供 Kubernetes 和 Docker 部署选项，并公开一个 PgBouncer 风格的管理数据库和一个 OpenMetrics 端点用于监控。它在 AGPL v3 许可下发布，允许内部使用和私有修改，无需共享源代码，除非是将 PgDog 作为公共服务提供。该项目尚处于早期阶段，但欢迎早期采用者并定期进行性能基准测试。"
  },
  {
    "id": "44081383",
    "title": "Semicolons bring the drama; that's why I love them",
    "url": "https://www.ft.com/content/80c39c74-8753-44bf-aeb0-cf6701a64f02",
    "summary": "The article, titled \"Semicolons bring the drama; that's why I love them,\" is behind a paywall on the Financial Times (FT) website. The provided content is essentially an advertisement for subscribing to the FT.\n\nIt highlights the following key points:\n\n*   The article itself is likely an opinion piece (suggested by the title) focusing on the author's appreciation for semicolons.\n*   Readers need a subscription to access the article and other FT content.\n*   The FT offers several subscription tiers (FT Edit, Standard Digital, and Premium Digital) with varying features and pricing. FT Edit provides a limited number of hand-picked articles.\n*   The content emphasizes the benefits of subscribing, such as access to global news, expert analysis, newsletters, the FT app, and more.\n*   Subscription options are also available for organizations (FT Professional).\n*   The FT promotes its value, highlighting that over a million readers pay for its content.\n\nThe primary purpose of the provided text is to encourage readers to subscribe to the Financial Times to access the article about semicolons and the wider range of content offered.\n",
    "chinese_title": "分号自有妙处，我爱它的原因就在这。",
    "chinese_summary": "题为“分号自带戏剧性；这就是我爱它们的原因”的文章位于《金融时报》(FT)网站的付费墙之后。提供的内容本质上是《金融时报》订阅的广告。\n\n它强调了以下要点：\n\n* 文章本身可能是一篇评论文章（标题暗示），侧重于作者对分号的欣赏。\n* 读者需要订阅才能访问该文章和其他FT内容。\n* 《金融时报》提供多个订阅级别（FT Edit、Standard Digital和Premium Digital），具有不同的功能和定价。FT Edit提供数量有限的精选文章。\n* 内容强调订阅的好处，例如访问全球新闻、专家分析、新闻通讯、FT应用程序等。\n* 组织也可以选择订阅（FT Professional）。\n* 《金融时报》宣传其价值，强调超过一百万读者为其内容付费。\n\n所提供文本的主要目的是鼓励读者订阅《金融时报》，以访问关于分号的文章以及更广泛的内容。"
  },
  {
    "id": "44083474",
    "title": "Worlds first petahertz transistor at ambient conditions",
    "url": "https://news.arizona.edu/news/u-researchers-developing-worlds-first-petahertz-speed-phototransistor-ambient-conditions",
    "summary": "University of Arizona researchers have developed the world's first petahertz-speed phototransistor that operates under ambient conditions. This breakthrough, published in Nature Communications, leverages a quantum tunneling effect in graphene to manipulate electrons with ultrafast light pulses, achieving processing speeds over 1,000 times faster than modern computer chips.\n\nLed by Mohammed Hassan, the team modified a commercial graphene phototransistor with a special silicon layer and used a laser to create the petahertz quantum transistor. The device can switch on and off at a rate of 638 attoseconds (one-quintillionth of a second). This significantly advances the development of ultrafast computer technologies.\n\nUnlike some scientific advancements that require strict conditions, this transistor operates in ambient conditions, paving the way for commercialization and use in everyday electronics. The researchers are working with Tech Launch Arizona to patent and market the innovation and are developing a transistor compatible with commercially available equipment.\n\nThe potential applications of this technology are vast, including revolutionizing computing, accelerating discoveries in space research, chemistry, health care, and more. The University of Arizona aims to be recognized for both the world's fastest electron microscope and this groundbreaking petahertz-speed transistor.\n",
    "chinese_title": "世界首个常温拍赫兹晶体管",
    "chinese_summary": "亚利桑那大学研究人员开发出世界首个在常温常压下工作的拍赫兹级光电晶体管。这项发表在《自然·通讯》上的突破性成果，利用石墨烯中的量子隧穿效应，通过超快光脉冲操纵电子，实现了比现代计算机芯片快1000倍以上的处理速度。\n\n由穆罕默德·哈桑领导的团队对商用石墨烯光电晶体管进行了改造，加入了一层特殊的硅层，并使用激光制造出这种拍赫兹量子晶体管。该设备可以以638阿秒（十亿分之一秒的五次方）的速度开关。这极大地推进了超快计算机技术的发展。\n\n与一些需要严格条件的科学进步不同，这种晶体管在常温常压下工作，为商业化和在日常电子产品中的使用铺平了道路。研究人员正与亚利桑那大学技术推广机构合作，对这项创新进行专利申请和市场推广，并正在开发一种与市售设备兼容的晶体管。\n\n这项技术的潜在应用非常广泛，包括彻底改变计算技术，加速太空研究、化学、医疗保健等领域的发现。亚利桑那大学的目标是同时以世界上最快的电子显微镜和这种开创性的拍赫兹级晶体管而闻名。"
  },
  {
    "id": "44110219",
    "title": "Why the original Macintosh had a screen resolution of 512×324",
    "url": "https://512pixels.net/2025/05/original-macintosh-resolution/",
    "summary": "The original Macintosh, unlike later models, featured a screen resolution of 512x342 pixels, a decision driven by several crucial factors. While many assume more memory would have made this issue non-existent, the truth is the 128KB of RAM of the first Macintosh was a design upgrade, not a limitation. Earlier designs had a much lower amount of memory.\n\nMemory constraints were paramount. The 512x342 resolution required approximately 22KB of RAM for the display buffer, significantly impacting available memory for applications. A taller 512x384 screen would have consumed even more precious memory.\n\nThe CPU, a Motorola 68000 running at 7.83 MHz, also played a role. To maintain a 60 Hz refresh rate and minimize flicker, the CPU dedicated a substantial portion of its time to drawing the display. A larger display would have further burdened the CPU, impacting overall performance.\n\nCritically, the 512x342 resolution allowed for square pixels, a design choice abandoned by the Lisa. Square pixels ensured accurate on-screen representation, preventing distortion and making graphics applications easier to develop.\n\nFinally, the 72 DPI screen was sufficient for intended use cases like word processing and page layout, providing adequate clarity and allowing users to visualize their work at different scales.\n\nIn essence, the 512x342 resolution was a carefully considered compromise that balanced performance, memory constraints, visual fidelity, and usability, optimizing the original Macintosh for its intended purpose within the available hardware capabilities.\n",
    "chinese_title": "为什么初代Macintosh的屏幕分辨率是512×324",
    "chinese_summary": "最初的Macintosh与后来的型号不同，其屏幕分辨率为512x342像素，这一决定受多个关键因素驱动。 虽然许多人认为更多的内存可以消除这个问题，但事实是，第一代Macintosh的128KB RAM是一个设计上的升级，而不是一种限制。 早期的设计内存要少得多。\n\n内存限制至关重要。 512x342的分辨率需要大约22KB的RAM用于显示缓冲区，这严重影响了应用程序可用的内存。 更高的512x384屏幕会消耗更多宝贵的内存。\n\nCPU，即以7.83 MHz运行的Motorola 68000，也发挥了作用。 为了保持60 Hz的刷新率并最大限度地减少闪烁，CPU将大部分时间用于绘制显示。 更大的显示屏会进一步加重CPU的负担，影响整体性能。\n\n至关重要的是，512x342的分辨率允许使用正方形像素，Lisa放弃了这一设计选择。 正方形像素确保了准确的屏幕显示，防止失真，并使图形应用程序更容易开发。\n\n最后，72 DPI的屏幕足以满足诸如文字处理和页面布局等预期用例，提供了足够的清晰度，并允许用户以不同的比例可视化他们的工作。\n\n本质上，512x342的分辨率是一个经过仔细考虑的折衷方案，它在性能、内存限制、视觉保真度和可用性之间取得了平衡，从而在可用的硬件能力范围内优化了原始Macintosh的预期用途。"
  },
  {
    "id": "44107393",
    "title": "Show HN: Malai – securely share local TCP services (database/SSH) with others",
    "url": "https://malai.sh/hello-tcp/",
    "summary": "Malai 0.2.5 introduces the ability to securely share local TCP services (like databases, SSH, and custom TCP protocols) with others. Users can expose these services without directly opening ports publicly. The command `malai tcp <port> --public` shares a local TCP server, and `malai tcp-bridge <id> <port>` allows remote machines to connect to it through the Malai network.\n\nFor SSH, users can share their SSH server without exposing port 22 by using `malai tcp 22 --public` and then connecting via SSH using `ssh -p <port> user@localhost` on the bridging machine.\n\nUse cases include securing SSH, sharing local databases (Postgres, Redis), demoing networked applications, and providing real-time help for students.\n\nThe release also includes a new \"malai folder\" command for sharing local files and folders via a simple HTTP server. The example shows how to share a directory using `malai folder ~/projects/fastn/assets/ --public`. The project encourages users to try this feature and provide feedback. Users are encouraged to star the project on GitHub to show support.\n",
    "chinese_title": "Show HN: Malai – 安全地与他人分享本地TCP服务（数据库/SSH）",
    "chinese_summary": "Malai 0.2.5 引入了安全共享本地 TCP 服务（如数据库、SSH 和自定义 TCP 协议）的能力。用户无需直接公开端口即可暴露这些服务。`malai tcp <端口> --public` 命令用于共享本地 TCP 服务器，而 `malai tcp-bridge <id> <端口>` 允许远程机器通过 Malai 网络连接到该服务器。\n\n对于 SSH，用户可以使用 `malai tcp 22 --public` 共享他们的 SSH 服务器，而无需暴露 22 端口，然后通过在桥接机器上使用 `ssh -p <端口> user@localhost` 通过 SSH 连接。\n\n用例包括保护 SSH 安全、共享本地数据库（Postgres、Redis）、演示联网应用程序以及为学生提供实时帮助。\n\n此版本还包括一个新的 \"malai folder\" 命令，用于通过简单的 HTTP 服务器共享本地文件和文件夹。示例展示了如何使用 `malai folder ~/projects/fastn/assets/ --public` 共享目录。该项目鼓励用户尝试此功能并提供反馈。鼓励用户在 GitHub 上为项目点赞以示支持。"
  },
  {
    "id": "44087195",
    "title": "In defense of shallow technical knowledge",
    "url": "https://www.seangoedecke.com/shallow-technical-knowledge/",
    "summary": "This article advocates for developing \"shallow technical knowledge\" across a broad range of technologies, arguing that it's crucial for good engineering. The author contends that understanding the basic principles of underlying technologies, even without in-depth expertise, leads to better decision-making and problem-solving.\n\nUsing database indexes and large language models (LLMs) as examples, the author demonstrates how a fundamental understanding allows engineers to anticipate potential issues and make informed choices about implementation and usage. For instance, knowing that database indexes speed up reads but slow down writes informs decisions about when and how to use them. Similarly, understanding how LLMs generate output helps predict their behavior in different scenarios, like JSON output.\n\nThe author acknowledges the value of deep specialization but recommends a broad base of knowledge for increased versatility and adaptability, especially in a rapidly evolving tech landscape. The suggested approach involves aiming to explain technologies to a smart junior engineer, avoiding unnecessary mathematical or jargon-heavy explanations.\n\nPractical tips for building these intuitions include reading original papers, engaging with language models for explanations and fact-checking, and writing down your understanding to solidify it. The key is to focus on developing useful insights about performance, suitability, and potential limitations, rather than striving for complete mastery. The author uses language models to fact check their work, to identify any misunderstandings, or flaws.\n",
    "chinese_title": "浅薄技术知识辩护",
    "chinese_summary": "本文提倡发展对广泛技术的“浅层技术知识”，认为这对于良好的工程实践至关重要。作者认为，即使没有深入的专业知识，理解底层技术的基本原理也能带来更好的决策和问题解决。\n\n作者以数据库索引和大型语言模型（LLMs）为例，展示了基本的理解如何让工程师能够预测潜在问题，并对实现和使用做出明智的选择。例如，了解数据库索引可以加快读取速度但会降低写入速度，可以帮助工程师决定何时以及如何使用它们。类似地，理解LLM如何生成输出有助于预测它们在不同场景下的行为，例如JSON输出。\n\n作者承认深度专业化的价值，但建议建立广泛的知识基础，以提高通用性和适应性，尤其是在快速发展的技术领域中。建议的方法包括，目标是向一位聪明的初级工程师解释技术，避免不必要的数学或术语繁重的解释。\n\n建立这些直觉的实用技巧包括阅读原始论文，利用语言模型进行解释和事实核查，以及写下你的理解来巩固它。关键是专注于发展关于性能、适用性和潜在局限性的有用见解，而不是力求完全掌握。作者使用语言模型来事实核查他们的工作，以识别任何误解或缺陷。"
  },
  {
    "id": "44106934",
    "title": "DuckLake is an integrated data lake and catalog format",
    "url": "https://ducklake.select/",
    "summary": "DuckLake is a new, open-source data lake and catalog format developed by the DuckDB team that leverages SQL databases for metadata management. It aims to provide advanced data lake features like snapshots, time travel queries, schema evolution, and partitioning without the complexity of traditional lakehouses.\n\nKey features of DuckLake include lightweight snapshots without frequent compaction, ACID transactional guarantees for concurrent access, and performance optimizations through statistics-based filter pushdown. It uses Parquet files for data storage and can be deployed on object storage like AWS S3.\n\nDuckLake's catalog metadata can be managed using various SQL databases such as PostgreSQL, SQLite, MySQL, and DuckDB itself. The DuckDB extension provides first-class support for interacting with DuckLake, enabling users to read and write datasets stored in the DuckLake format.\n\nDuckLake can be used for \"multiplayer DuckDB\" setups, enabling concurrent data access from multiple DuckDB instances. It also offers benefits for single DuckDB users such as time travel queries, data partitioning, and the ability to store data in multiple files.\n\nThe DuckLake specification and the DuckDB extension are released under the MIT license.\n",
    "chinese_title": "DuckLake是一个集成的数据湖和目录格式",
    "chinese_summary": "DuckLake：基于SQL数据库的开源数据湖和目录格式\n\nDuckLake是由DuckDB团队开发的一种新型开源数据湖和目录格式，它利用SQL数据库进行元数据管理。它的目标是提供高级数据湖特性，如快照、时间旅行查询、模式演化和分区，而无需传统湖仓架构的复杂性。\n\nDuckLake的关键特性包括无需频繁压缩的轻量级快照、并发访问的ACID事务保证以及通过基于统计信息的过滤器下推实现的性能优化。它使用Parquet文件进行数据存储，并且可以部署在诸如AWS S3之类的对象存储上。\n\nDuckLake的目录元数据可以使用各种SQL数据库进行管理，例如PostgreSQL、SQLite、MySQL和DuckDB本身。DuckDB扩展提供了与DuckLake交互的一流支持，使用户能够读取和写入以DuckLake格式存储的数据集。\n\nDuckLake可用于“多人DuckDB”设置，从而允许多个DuckDB实例并发访问数据。它还为单个DuckDB用户提供诸多优势，例如时间旅行查询、数据分区以及将数据存储在多个文件中的能力。\n\nDuckLake规范和DuckDB扩展均以MIT许可证发布。"
  },
  {
    "id": "44079729",
    "title": "Cows get GPS collars to stop them falling in river",
    "url": "https://www.bbc.co.uk/news/articles/cj4229k744lo",
    "summary": "Cambridge City Council is implementing GPS collars on cows grazing on common lands to prevent them from falling into the River Cam. Up to four cows end up in the river each grazing season, costing the council around £10,000 annually for rescue efforts. The solar-powered GPS collars emit a high-pitched sound as the cow approaches the river's boundary, and deliver a mild electric pulse if the animal continues towards the water.\n\nThe council employs a team of \"pinders\" to rescue cattle that get into difficulty, including those stuck in the River Cam. After considering passing the cost of after-hours rescues onto graziers, the council opted to fund the grazing and improve animal welfare by investing in the GPS technology. The move is intended to be cost-effective and ensures the continuation of a tradition of grazing cattle on the commons that dates back to the Middle Ages. Councillor Martin Smart emphasized the symbolic importance of the cows to the city's residents and visitors.\n",
    "chinese_title": "奶牛戴GPS项圈防掉河里",
    "chinese_summary": "剑桥市议会使用GPS项圈防止牛只坠入剑河"
  },
  {
    "id": "44093193",
    "title": "Nanoparticle-cell link enables EM wireless programming of transgene expression",
    "url": "https://phys.org/news/2025-05-nanoparticle-cell-interface-enables-electromagnetic.html",
    "summary": "This Phys.org article, dated May 18, 2025, reports on a new method developed by researchers at ETH Zurich for wirelessly controlling gene expression in mammals using an interface between nanoparticles and cells. This method, termed EMPOWER (electromagnetic programming of wireless expression regulation), aims to address the challenges of traditional gene therapy methods, which can be invasive, imprecise, or lack robustness.\n\nThe core of the approach involves biocompatible nanoparticles composed of multiferroic cores and chitosan outer layers. When exposed to a low-frequency electromagnetic field, these nanoparticles generate safe levels of reactive oxygen species (ROS) within cells. The researchers engineered mammalian cells with a genetic circuit sensitive to ROS, activating the NRF2 pathway to produce therapeutic proteins, such as insulin.\n\nA key advantage is the precise, non-invasive control over gene expression location and timing. The method is highly biocompatible, requires low nanoparticle dosages, and minimizes off-target effects compared to other nanoparticle-based wireless gene control techniques.\n\nIn a mouse model of diabetes, exposure to a weak electromagnetic field (1 kHz, 21 mT) for three minutes daily effectively controlled insulin secretion and maintained normal blood glucose levels. The researchers are now exploring applications in oncology, neurology, and regenerative medicine, while also working to improve the system's sensitivity, biocompatibility, and efficiency. They also plan to make the EM stimulation equipment more compact for clinical use.\n",
    "chinese_title": "纳米颗粒-细胞连接实现转基因表达的电磁无线编程",
    "chinese_summary": "这篇2025年5月18日Phys.org的文章报道了苏黎世联邦理工学院的研究人员开发的一种新方法，该方法利用纳米粒子和细胞之间的界面，无线控制哺乳动物的基因表达。这种方法被称为EMPOWER（无线表达调控的电磁编程），旨在解决传统基因治疗方法的挑战，这些方法可能具有侵入性、不精确或缺乏稳健性。\n\n该方法的核心是生物相容性纳米粒子，由多铁性内核和壳聚糖外层组成。当暴露于低频电磁场时，这些纳米粒子在细胞内产生安全水平的活性氧（ROS）。研究人员对哺乳动物细胞进行了基因工程改造，使其具有对ROS敏感的基因回路，从而激活NRF2通路以产生治疗性蛋白质，例如胰岛素。\n\n一个关键优势是可以精确、非侵入性地控制基因表达的位置和时间。与基于纳米粒子的其他无线基因控制技术相比，该方法具有高度生物相容性，所需的纳米粒子剂量低，并且最大限度地减少了脱靶效应。\n\n在一个糖尿病小鼠模型中，每天暴露于微弱的电磁场（1 kHz，21 mT）三分钟，可以有效地控制胰岛素分泌并维持正常的血糖水平。研究人员目前正在探索其在肿瘤学、神经病学和再生医学中的应用，同时也在努力提高系统的灵敏度、生物相容性和效率。他们还计划使EM刺激设备更加紧凑，以便临床使用。"
  },
  {
    "id": "44106842",
    "title": "Outcome-Based Reinforcement Learning to Predict the Future",
    "url": "https://arxiv.org/abs/2505.17989",
    "summary": "This paper, \"Outcome-based Reinforcement Learning to Predict the Future,\" explores the use of Reinforcement Learning with Verifiable Rewards (RLVR) for forecasting, adapting it to handle the challenges of binary, delayed, and noisy rewards common in real-world forecasting scenarios. The authors, Turtel et al., demonstrate that outcome-only online RL applied to a 14B language model can achieve competitive forecasting accuracy and superior calibration compared to a frontier baseline (o1).\n\nThey adapt two leading RL algorithms, Group-Relative Policy Optimisation (GRPO) and ReMax, for this purpose. Key adaptations include removing per-question variance scaling in GRPO and using baseline-subtracted advantages in ReMax. The training process is augmented with 100k synthetically generated, temporally consistent questions and lightweight guardrails to prevent nonsensical or irrelevant responses.\n\nBy scaling ReMax to 110k questions and ensembling seven predictions, the resulting 14B model matches the accuracy of baseline o1 (Brier score of 0.193) while significantly improving calibration (ECE of 0.042). This improved calibration translates to a hypothetical profit of $127 in a prediction market simulation, compared to $92 for o1. The findings suggest that refined RLVR techniques can effectively leverage smaller language models for economically valuable forecasting, highlighting the potential for scaling these methods to larger models.\n",
    "chinese_title": "基于结果的强化学习预测未来",
    "chinese_summary": "本文《基于结果的强化学习预测未来》探讨了使用具有可验证奖励的强化学习(RLVR)进行预测，并使其适应现实世界预测场景中常见的二元、延迟和噪声奖励所带来的挑战。作者Turtel等人证明，应用于14B语言模型的纯结果在线强化学习可以实现具有竞争力的预测准确性和优于前沿基线(o1)的校准。\n\n他们为此目的调整了两种领先的强化学习算法：组相对策略优化(GRPO)和ReMax。关键的调整包括在GRPO中移除每个问题的方差缩放，以及在ReMax中使用基线减去优势。训练过程通过10万个合成生成的、时间上一致的问题以及轻量级的防护措施来增强，以防止产生无意义或不相关的回复。\n\n通过将ReMax扩展到11万个问题并集成七个预测，由此产生的14B模型匹配了基线o1的准确性（Brier得分为0.193），同时显著提高了校准（ECE为0.042）。这种改进的校准转化为预测市场模拟中的127美元的假设利润，而o1的利润为92美元。研究结果表明，改进的RLVR技术可以有效地利用较小的语言模型进行具有经济价值的预测，突出了将这些方法扩展到更大模型的潜力。"
  },
  {
    "id": "44105878",
    "title": "Just make it scale: An Aurora DSQL story",
    "url": "https://www.allthingsdistributed.com/2025/05/just-make-it-scale-an-aurora-dsql-story.html",
    "summary": "This article details the development journey of Aurora DSQL, a relational database designed for serverless scalability and zero operational overhead at AWS. Faced with challenges in scaling writes horizontally and handling read latency due to garbage collection issues in their initial JVM-based architecture, the team explored alternative solutions.\n\nThey decided to adopt Rust for the data plane, initially starting with the Adjudicator component. The results were remarkable: the Rust implementation was 10x faster than the optimized Kotlin version. This success led to the decision to rewrite the entire data plane in Rust.\n\nWhile building on PostgreSQL for its query processing capabilities, the team initially considered using C for extensions. However, realizing the risk of introducing new memory safety bugs with fresh C code, they pivoted to Rust for extension development as well. This allowed them to leverage Rust's memory safety features and build abstractions that enforce safe memory access patterns.\n\nThe key takeaway is that the team's iterative approach, willingness to challenge existing assumptions, and ultimately their strategic embrace of Rust were crucial in overcoming performance and scalability challenges in building Aurora DSQL. The switch to Rust significantly improved performance, memory safety, and overall engineering efficiency.\n",
    "chinese_title": "使其规模化：一个Aurora DSQL的故事",
    "chinese_summary": "本文详细介绍了 Aurora DSQL 的开发历程，这是一款专为 AWS 提供的无服务器可扩展性和零运维开销而设计的关系型数据库。由于最初基于 JVM 的架构在横向扩展写入和处理垃圾回收导致的读取延迟方面面临挑战，该团队探索了替代方案。\n\n他们决定采用 Rust 用于数据平面，最初从 Adjudicator 组件开始。结果非常显著：Rust 实现比优化的 Kotlin 版本快 10 倍。这一成功促使他们决定用 Rust 重写整个数据平面。\n\n在基于 PostgreSQL 构建其查询处理能力的同时，团队最初考虑使用 C 来开发扩展。然而，意识到使用新的 C 代码会带来引入新的内存安全错误的风险，他们转而使用 Rust 进行扩展开发。这使他们能够利用 Rust 的内存安全特性并构建强制执行安全内存访问模式的抽象。\n\n关键的结论是，团队的迭代方法、挑战现有假设的意愿以及最终对 Rust 的战略性拥抱，对于克服构建 Aurora DSQL 中的性能和可扩展性挑战至关重要。转向 Rust 显著提高了性能、内存安全性和整体工程效率。"
  },
  {
    "id": "44086062",
    "title": "Clojure MCP",
    "url": "https://github.com/bhauman/clojure-mcp",
    "summary": "Clojure MCP is an alpha-stage project providing an AI-assisted development environment for Clojure, built around REPL-driven development. It connects AI models to a Clojure nREPL server and provides specialized editing tools, creating a comprehensive Clojure development experience.\n\nKey features include: Clojure REPL connection, intelligent editing (clj-kondo, parinfer, cljfmt, clj-rewrite), and a curated set of tools optimized for Clojure. It's designed to work with the latest LLMs like Claude 4, Gemini 2.5 and OpenAI models. The system tracks file read/write operations for safety, preventing conflicts and ensuring reliable file identification.\n\nInstallation involves cloning the repository, configuring your target project's `deps.edn` file with MCP server settings, and setting up Claude Desktop to connect to the MCP server. The project also uses a PROJECT_SUMMARY.md file to help LLMs quickly understand the codebase structure.\n\nThe MCP includes various tools categorized as read-only, code evaluation, file editing, and agent tools (requiring API keys from Google, OpenAI, or Anthropic). Notable tools include `read_file` for smart file reading, `clojure_edit` for structure-aware editing, and `dispatch_agent` for autonomous search tasks.\n\nCustomization is supported by separating the core MCP API from the implementation. Contributions are welcome, including bug reports, feature suggestions, and pull requests.\n",
    "chinese_title": "Clojure 多核处理器",
    "chinese_summary": "Clojure MCP 是一个 alpha 阶段的项目，它为 Clojure 提供 AI 辅助的开发环境，并围绕 REPL 驱动的开发构建。它将 AI 模型连接到 Clojure nREPL 服务器，并提供专门的编辑工具，从而创造全面的 Clojure 开发体验。\n\n主要功能包括：Clojure REPL 连接、智能编辑 (clj-kondo, parinfer, cljfmt, clj-rewrite) 和一组为 Clojure 优化的精选工具。它旨在与最新的 LLM（如 Claude 4、Gemini 2.5 和 OpenAI 模型）配合使用。该系统跟踪文件读/写操作以确保安全，防止冲突并确保可靠的文件识别。\n\n安装涉及克隆存储库，使用 MCP 服务器设置配置目标项目的 `deps.edn` 文件，并设置 Claude Desktop 以连接到 MCP 服务器。该项目还使用 PROJECT_SUMMARY.md 文件来帮助 LLM 快速了解代码库结构。\n\nMCP 包含各种工具，分为只读、代码评估、文件编辑和代理工具（需要来自 Google、OpenAI 或 Anthropic 的 API 密钥）。值得注意的工具包括用于智能文件读取的 `read_file`、用于结构感知编辑的 `clojure_edit` 和用于自主搜索任务的 `dispatch_agent`。\n\n通过将核心 MCP API 与实现分离来支持自定义。欢迎投稿，包括错误报告、功能建议和 pull request。"
  },
  {
    "id": "44113397",
    "title": "Why are 2025/05/28 and 2025-05-28 different days in JavaScript?",
    "url": "https://brandondong.github.io/blog/javascript_dates/",
    "summary": "This article explains the inconsistent behavior of JavaScript's `Date` object when parsing date strings like '2025/05/28' and '2025-05-28'. The core issue is time zone ambiguity when parsing date strings without explicit time zones.\n\nJavaScript's `Date` always represents a specific point in time (milliseconds since the epoch). While '2025/05/28' is often interpreted as local time, '2025-05-28' is generally parsed as UTC. This discrepancy stems from the evolving and often inconsistent implementation of the ISO 8601 standard across different browsers over the years. The author traces the history of how major browsers (Chrome, Firefox, Safari) have handled date string parsing and their back-and-forth changes between using local time and UTC.\n\nThe author highlights how there was a period of over a decade where major browsers behaved inconsistently when time zone offsets were missing. Chrome even flipped between local and UTC several times during this period.\n\nThe article then introduces Temporal, JavaScript's upcoming date and time API, which aims to address these issues. Temporal allows for the representation of plain dates (dates without a time zone), avoiding the ambiguity of parsing date-only strings into specific points in time. If parsing into an instant in time is desired, Temporal requires an explicit offset or time zone identifier.\n\nFinally, the article touches on the lenient parsing behavior of the `Date` constructor, showcasing how seemingly nonsensical strings can sometimes be parsed into valid dates.\n",
    "chinese_title": "为什么在JavaScript中2025/05/28和2025-05-28是不同的日期？",
    "chinese_summary": "本文解释了 JavaScript 的 `Date` 对象在解析日期字符串（如 '2025/05/28' 和 '2025-05-28'）时表现出的不一致行为。核心问题在于解析没有明确时区的日期字符串时，存在的时区歧义。\n\nJavaScript 的 `Date` 始终表示一个特定的时间点（自 epoch 以来的毫秒数）。虽然 '2025/05/28' 通常被解释为本地时间，但 '2025-05-28' 通常被解析为 UTC 时间。这种差异源于多年来不同浏览器对 ISO 8601 标准不断发展且常常不一致的实现。作者追溯了主要浏览器（Chrome、Firefox、Safari）处理日期字符串解析的历史，以及它们在使用本地时间和 UTC 之间来回切换的变化。\n\n作者强调了在超过十年的时间里，当缺少时区偏移量时，主要浏览器行为不一致的情况。Chrome 甚至在此期间多次在本地时间和 UTC 之间切换。\n\n文章随后介绍了 Temporal，JavaScript 即将推出的日期和时间 API，旨在解决这些问题。Temporal 允许表示纯粹的日期（没有时区的日期），避免了将仅包含日期的字符串解析为特定时间点的歧义。如果需要解析为时间上的某个瞬间，Temporal 则需要显式的偏移量或时区标识符。\n\n最后，文章还谈到了 `Date` 构造函数的宽松解析行为，展示了看似荒谬的字符串有时是如何被解析为有效日期的。"
  },
  {
    "id": "44115254",
    "title": "SpaceX may have solved one problem, only to find more on latest Starship flight",
    "url": "https://arstechnica.com/space/2025/05/spacex-may-have-solved-one-problem-only-to-find-more-on-latest-starship-flight/",
    "summary": "SpaceX's latest Starship test flight showed progress but also revealed new challenges. While the rocket successfully launched and reached its planned trajectory, overcoming issues from previous flights, it lost control due to fuel tank leaks during the coast and reentry phase, leading to a premature end. This prevented thorough testing of the improved heat shield, a critical component for future reusability.\n\nDespite the setback, the successful initial flight and engine cutoff were improvements. The mission marked the first reuse of a Super Heavy booster, although it exploded during the landing burn. The failure to open the cargo bay door also prevented a planned Starlink satellite deployment test.\n\nElon Musk acknowledged the data loss but emphasized the \"lot of good data to review,\" hinting at a quick turnaround for the next launches. SpaceX aims for a faster launch cadence, potentially every three to four weeks, and is already working on a Block 3 Starship design with further improvements. The FAA is \"actively working\" with SpaceX following the test flight. SpaceX engineers are studying both the Starship fuel leak and booster explosion to improve future flights.\n",
    "chinese_title": "SpaceX可能解决了一个问题，却在最新的星舰飞行中发现了更多问题。",
    "chinese_summary": "SpaceX星舰最新试飞展现进展，但也暴露新挑战。火箭成功发射并达到预定轨道，克服了此前飞行的问题，但在滑行和重返大气层阶段因燃料箱泄漏失控，导致提前结束。这阻碍了对改进型隔热罩的全面测试，而隔热罩是未来可重复使用性的关键部件。\n\n尽管遇到挫折，但成功的初始飞行和发动机关机是进步。本次任务标志着超重型助推器的首次重复使用，尽管它在着陆点火时爆炸。未能打开货舱门也阻止了计划中的星链卫星部署测试。\n\n埃隆·马斯克承认数据丢失，但强调“有很多有用的数据需要审查”，暗示下次发射会很快进行。SpaceX的目标是更快的发射频率，可能每三到四周一次，并且已经在研发具有进一步改进的Block 3星舰设计。美国联邦航空管理局正在试飞后“积极”与SpaceX合作。SpaceX工程师正在研究星舰燃料泄漏和助推器爆炸的原因，以改进未来的飞行。"
  },
  {
    "id": "44089317",
    "title": "Are the Colors in Astronomical Images 'Real'?",
    "url": "https://www.scientificamerican.com/article/are-the-colors-in-space-real/",
    "summary": "This article addresses the common question of whether the colors in astronomical images are \"real.\" The author, Phil Plait, explains that while the vibrant images from telescopes like Hubble and JWST are stunning, they are not necessarily what the human eye would see. This isn't due to fakery, but rather the differences in how cameras and our eyes perceive light.\n\nHuman vision relies on rods and cones to detect light and color, respectively. Cameras mimic this by using pixels and filters to record the intensity and color (red, green, blue) of light. However, the filters don't perfectly match the eye's response, making \"true color\" images approximations.\n\nWhile these \"true color\" images are aesthetically pleasing, astronomers often prefer analyzing individually filtered images for research. This is because \"color\" provides far more information than pretty pictures. Nebulae, for example, emit light at specific wavelengths. By using narrow-band filters that isolate these wavelengths, astronomers can determine the composition, temperature, density, and structure of these clouds.\n\nThe resulting images, often using combinations of these narrow filters, look dramatically different from what the unaided eye would see. This isn't misleading, but rather a way to visualize light beyond the visible spectrum, including infrared, ultraviolet, and X-rays. These different types of light are assigned various colors to create a composite image.\n\nUltimately, the image creation process depends on the intended use. While no astronomical image perfectly replicates what the human eye would see, they all reveal aspects of the universe that would otherwise remain invisible.\n",
    "chinese_title": "天文图像中的颜色是“真实”的吗？",
    "chinese_summary": "天文图像的色彩是“真实的”吗？"
  },
  {
    "id": "44105592",
    "title": "The Myth of Developer Obsolescence",
    "url": "https://alonso.network/the-recurring-cycle-of-developer-replacement-hype/",
    "summary": "This article debunks the recurring myth of developer obsolescence driven by new technologies like NoCode, cloud computing, and now AI-assisted development. While these technologies promise to eliminate the need for developers, they instead transform the role and often increase the demand and value of skilled engineers.\n\nThe author argues that each \"revolution\" (NoCode, Cloud, Offshore, AI) follows a predictable pattern: initial hype about replacement gives way to the realization that these technologies create new complexities requiring specialized expertise. NoCode led to NoCode specialists, cloud transformed sysadmins into DevOps engineers, and offshore development highlighted the need for stronger architecture and communication skills.\n\nThe core argument is that while AI can generate code, it cannot architect entire systems. The most valuable skill in software engineering isn't writing code itself (which is a liability in terms of maintenance and debugging), but designing and managing the overall system architecture. AI excels at local optimization but struggles with global design, making architectural mistakes more easily baked into systems.\n\nTherefore, AI-assisted development won't replace developers, but will elevate the importance of system architecture and design skills. Engineers who can effectively orchestrate AI systems and manage the increased complexity of rapidly generated code will be in high demand. The skill that survives and thrives is architecting systems, and that's what AI currently cannot replicate.\n",
    "chinese_title": "开发者过时的神话",
    "chinese_summary": "本文旨在揭穿关于开发者会被新技术（如无代码、云计算以及现在的AI辅助开发）淘汰的反复出现的谬论。虽然这些技术承诺消除对开发者的需求，但实际上它们转变了开发者的角色，并且通常会增加对熟练工程师的需求和价值。\n\n作者认为，每一次“革命”（无代码、云计算、离岸外包、AI）都遵循一种可预测的模式：最初关于替代的炒作让位于对这些技术创造了需要专门知识的新复杂性的认识。无代码产生了无代码专家，云计算将系统管理员转变为DevOps工程师，而离岸开发则突出了对更强大的架构和沟通技巧的需求。\n\n核心论点是，虽然AI可以生成代码，但它无法构建整个系统。软件工程中最有价值的技能不是编写代码本身（在维护和调试方面是一种负担），而是设计和管理整体系统架构。AI擅长局部优化，但在全局设计方面却很吃力，使得架构错误更容易融入到系统中。\n\n因此，AI辅助开发不会取代开发者，而是会提升系统架构和设计技能的重要性。能够有效地协调AI系统并管理快速生成的代码所带来的日益增长的复杂性的工程师将拥有很高的需求。能够生存和繁荣的技能是系统架构，而这正是AI目前无法复制的。"
  },
  {
    "id": "44108207",
    "title": "I salvaged $6k of luxury items discarded by Duke students",
    "url": "https://indyweek.com/culture/duke-students-dumpster-diving/",
    "summary": "Lena Geller, living in a Durham apartment building largely populated by Duke University students, discovered a treasure trove of discarded luxury items in the building's trash rooms at the end of the school year. She salvaged items ranging from a $900 acrylic table to Balenciaga slides, Valentino sneakers, and Lululemon clothing, estimating the total value of her haul at around $6,000.\n\nGeller's discovery highlighted the excessive wastefulness of the students, prompting her to investigate donation programs at Duke and other universities. Her research revealed that Duke's per-undergraduate donation rate is comparable to other wealthy private institutions.\n\nBeyond the financial gains, Geller's experience evoked mixed emotions. While she enjoyed cleaning and mending the salvaged items, she also felt overwhelmed and discouraged by the sheer volume of waste and the feeling that her own belongings were inadequate. A final, clumsy scavenging trip, culminating in a spill and a non-fitting air filter, triggered a moment of emotional breakdown. Ultimately, a salvaged handheld vacuum provided a practical solution to a persistent household problem, offering a small, tangible benefit from her scavenging efforts. The experience highlighted the contrast between the potential value of discarded items and the emotional toll of confronting such waste.\n",
    "chinese_title": "我从杜克大学学生丢弃的物品中捡回了价值六千美元的奢侈品。",
    "chinese_summary": "莉娜·盖勒住在达勒姆一栋主要居住着杜克大学学生的公寓楼里，她在学年结束时发现了楼内垃圾房里大量被丢弃的奢侈品。她抢救了价值从 900 美元的亚克力桌子到巴黎世家拖鞋、华伦天奴运动鞋和 Lululemon 服装等物品，估计总价值约为 6000 美元。\n\n盖勒的发现凸显了学生的过度浪费，促使她调查杜克大学和其他大学的捐赠项目。她的研究表明，杜克大学的本科生人均捐赠率与其他富有的私立院校相当。\n\n除了经济收益外，盖勒的经历也引发了复杂的情绪。虽然她喜欢清洁和修补抢救回来的物品，但她也对大量的浪费感到不知所措和沮丧，并觉得自己的东西不足够。最后一次笨拙的捡拾之旅，最终以一次溢出和一个不合适的空气滤清器告终，引发了一次情绪崩溃。最终，一台抢救回来的手持吸尘器为持续存在的家庭问题提供了一个实际的解决方案，为她的捡拾努力提供了一个小的、切实的好处。这次经历突出了被丢弃物品的潜在价值与面对如此浪费所带来的情感损失之间的对比。"
  },
  {
    "id": "44097390",
    "title": "GitHub MCP exploited: Accessing private repositories via MCP",
    "url": "https://invariantlabs.ai/blog/mcp-github-vulnerability",
    "summary": "This article details a critical vulnerability in the GitHub MCP (Managed Code Platform) integration, allowing attackers to access private repositories through a \"toxic agent flow.\" The attack involves injecting malicious prompts into a public GitHub issue that, when accessed by a user's agent (like Claude Desktop), coerces the agent into leaking data from private repositories.\n\nThe attack unfolds when a user prompts their agent to look at issues in a public repository. The agent encounters a malicious issue containing a prompt injection, which then tricks the agent into pulling data from a private repository and creating a public pull request containing sensitive information.\n\nThe vulnerability is not in the MCP server code itself, but stems from the agent's susceptibility to untrusted information from external platforms like GitHub. Even state-of-the-art AI models like Claude 4 Opus are vulnerable.\n\nThe article emphasizes that model alignment alone is insufficient to prevent such attacks, advocating for system-level security measures. Mitigation strategies include implementing granular permission controls (like Invariant Guardrails) to limit agent access to only necessary repositories, and continuous security monitoring using specialized scanners like Invariant's MCP-scan to detect threats in real time. These measures help prevent cross-repository information leakage and provide an audit trail for identifying potential vulnerabilities. The article concludes by urging organizations to adopt these security measures to ensure responsible deployment of agent systems at scale.\n",
    "chinese_title": "GitHub MCP 漏洞利用：通过 MCP 访问私有仓库",
    "chinese_summary": "本文详细描述了 GitHub MCP（托管代码平台）集成中的一个严重漏洞，攻击者可以通过“有毒代理流程”访问私有仓库。该攻击涉及将恶意提示注入到公共 GitHub Issue 中，当用户代理（如 Claude Desktop）访问该 Issue 时，会强制代理泄露来自私有仓库的数据。\n\n攻击展开过程是，当用户提示其代理查看公共仓库中的 Issue 时，代理遇到包含提示注入的恶意 Issue，从而诱骗代理从私有仓库提取数据，并创建一个包含敏感信息的公共拉取请求。\n\n此漏洞并非存在于 MCP 服务器代码本身，而是源于代理容易受到来自 GitHub 等外部平台上不受信任的信息的影响。即使像 Claude 4 Opus 这样最先进的 AI 模型也容易受到攻击。\n\n本文强调，仅靠模型对齐不足以阻止此类攻击，并提倡采取系统级安全措施。缓解策略包括实施细粒度的权限控制（如 Invariant Guardrails），以限制代理仅访问必要的仓库，并使用 Invariant 的 MCP-scan 等专用扫描器进行持续的安全监控，以实时检测威胁。这些措施有助于防止跨仓库信息泄漏，并为识别潜在漏洞提供审计跟踪。文章最后敦促各组织采取这些安全措施，以确保大规模负责任地部署代理系统。"
  },
  {
    "id": "44117780",
    "title": "After Deepfaking YouTube, Google's Veo 3 Could Slop-Ify Video Games Next",
    "url": "https://gizmodo.com/after-deepfaking-youtube-googles-veo-3-could-slop-ify-video-games-next-2000608209",
    "summary": "James Pero's Gizmodo article explores the potential impact of Google's new video generator, Veo 3, on the AAA video game industry. While Veo 3 can create convincing fake gameplay footage, the article focuses on how these AI-generated videos can be integrated into game development workflows.\n\nThe piece highlights how users are already combining Veo 3 with 3D tools to create customizable and granular game assets. This allows for rapid prototyping and visual development based on text prompts, offering a faster way to generate game environments and characters.\n\nThe author then transitions into the discussion of AI coding. Although current AI technology can't fully code AAA games, there's a clear trend towards generative AI assisting in game development. Google itself acknowledged the potential for AI to reduce development costs in 2023. Pero expresses concern that this rapid progress could lead to job displacement for game developers and the rise of \"AI slop,\" potentially diminishing the quality and originality of games due to over-reliance on AI-generated content and potential copyright infringements. The article ends with a note of cautious optimism, acknowledging the potential for AI to augment game development while also highlighting the risks of unchecked AI adoption.\n",
    "chinese_title": "继深度伪造 YouTube 之后，谷歌 Veo 3 可能接下来会让电子游戏变得粗糙。",
    "chinese_summary": "詹姆斯·佩罗在Gizmodo上的文章探讨了谷歌新款视频生成器Veo 3对AAA级游戏产业的潜在影响。虽然Veo 3可以制作出逼真的伪造游戏画面，但该文章侧重于这些人工智能生成的视频如何融入游戏开发流程。\n\n文章强调了用户如何将Veo 3与3D工具结合，以创建可定制且细致的游戏资源。这使得基于文本提示的快速原型设计和视觉开发成为可能，从而能够更快地生成游戏环境和角色。\n\n作者随后过渡到对人工智能编码的讨论。虽然目前的人工智能技术无法完全编写AAA级游戏，但生成式人工智能辅助游戏开发已成为一种明显趋势。谷歌自己在2023年也承认了人工智能降低开发成本的潜力。佩罗表达了担忧，认为这种快速发展可能导致游戏开发者失业，以及“人工智能垃圾”的出现，并可能因过度依赖人工智能生成的内容和潜在的版权侵权而降低游戏的质量和原创性。文章以谨慎的乐观态度结尾，既承认了人工智能增强游戏开发的潜力，也强调了不加控制地采用人工智能的风险。"
  },
  {
    "id": "44105412",
    "title": "LiveStore: State management based on reactive SQLite and built-in sync engine",
    "url": "https://livestore.dev",
    "summary": "LiveStore is a next-generation state management framework built on reactive SQLite and a git-inspired syncing engine, designed for high-performance, local-first applications. It aims to replace solutions like Redux or MobX by providing a client-centric data layer with real-time sync via event sourcing.\n\nKey features include:\n\n*   **Reactive SQLite Database:** Provides instant, reactive query capabilities with efficient data persistence.\n*   **Built-in Sync Engine:** Uses event sourcing for complex syncing scenarios across clients.\n*   **Type-Safe Schema:** Offers a powerful API for ergonomic data modeling without database migrations.\n*   **Cross-Platform Support:** Adapters for web, mobile, server/edge, and desktop.\n*   **Premium DX & Devtools:** First-class devtools for data inspection and debugging, similar to Chrome DevTools.\n*   **Local-First Architecture:** Designed for offline-first workflows with custom merge conflict resolution.\n\nLiveStore's workflow involves committing events, which are persisted in an eventlog, refreshing the database via materializers, and syncing to other clients. Developers define events as part of the schema and use materializers to map those events to state changes via SQL. Reactive queries are used to efficiently retrieve and update data.\n\nWhile powerful, LiveStore isn't a batteries-included framework and isn't suitable for all use cases, such as syncing with existing databases, providing a hosted service, scaling for unbounded data, or peer-to-peer syncing. It is designed for developers who require a solid data foundation, true offline-first capabilities, and flexible data modeling.\n",
    "chinese_title": "LiveStore：基于响应式SQLite的状态管理，内置同步引擎",
    "chinese_summary": "LiveStore：基于响应式SQLite和Git风格同步引擎的新一代状态管理框架，专为高性能、本地优先的应用设计。它旨在通过提供以客户端为中心的数据层和基于事件溯源的实时同步，来取代Redux或MobX等解决方案。\n\n主要特性包括：\n\n*   **响应式SQLite数据库：** 提供即时、响应式的查询能力以及高效的数据持久化。\n*   **内置同步引擎：** 使用事件溯源，实现跨客户端的复杂同步场景。\n*   **类型安全Schema：** 提供强大的API，用于符合人体工程学的数据建模，无需数据库迁移。\n*   **跨平台支持：** 适配Web、移动、服务器/边缘计算和桌面平台。\n*   **卓越的DX和开发者工具：** 一流的开发者工具，用于数据检查和调试，类似于Chrome DevTools。\n*   **本地优先架构：** 专为离线优先工作流程设计，支持自定义合并冲突解决方案。\n\nLiveStore的工作流程包括提交事件（这些事件会被持久化到事件日志中）、通过物化器刷新数据库以及同步到其他客户端。开发者将事件定义为Schema的一部分，并使用物化器通过SQL将这些事件映射到状态变化。响应式查询用于高效地检索和更新数据。\n\n虽然功能强大，但LiveStore并非一个开箱即用的框架，也不适用于所有用例，例如与现有数据库同步、提供托管服务、扩展到无限数据或点对点同步。 它专为需要坚实数据基础、真正离线优先能力和灵活数据建模的开发者而设计。"
  },
  {
    "id": "44103131",
    "title": "The UI future is colourful and dimensional",
    "url": "https://www.flarup.email/p/the-future-is-colourful-and-dimensional",
    "summary": "Michael Flarup argues that UI design is shifting away from flat design towards a more colorful and dimensional style, which he terms \"Diamorph.\" This design embraces depth, texture, and light, aiming to create expressive and playful interfaces that feel native to the screen without necessarily mimicking the real world.\n\nFlarup highlights Airbnb's recent redesign as an example of this trend and points to other developments like Big Sur icons and richer lighting models. He believes this shift allows designers to move beyond the restrictive flat-vs-skeuomorphism dichotomy.\n\nThe article also explores the role of AI in accelerating the adoption of Diamorph design. AI tools can now generate detailed, dimensional icons with relative ease, lowering the barrier to entry for creating this style. While acknowledging concerns about AI's impact on artistic skills, Flarup sees it as a tool that can enhance creativity by handling technical aspects like materials, lighting, and color.  He emphasizes that core design skills like composition and taste remain crucial.\n\nUltimately, Flarup believes Diamorph design represents a step forward, creating interfaces that are expressive, emotional, and unapologetically digital. He anticipates more people will join this trend as AI makes dimensional design more accessible, leading to more imaginative and joyful user experiences.\n",
    "chinese_title": "UI 的未来是多彩且立体的。",
    "chinese_summary": "迈克尔·弗拉鲁普认为，UI设计正从扁平化设计转向一种更具色彩和维度的风格，他称之为“Diamorph”。这种设计拥抱深度、纹理和光线，旨在创造富有表现力和趣味性的界面，使其感觉原生于屏幕，而不必模仿现实世界。\n\n弗拉鲁普以Airbnb最近的重新设计为例，说明了这一趋势，并指出了其他发展，如Big Sur图标和更丰富的照明模型。他认为，这种转变使设计师能够超越扁平化与拟物化之间的限制性二分法。\n\n文章还探讨了人工智能在加速Diamorph设计采用方面的作用。人工智能工具现在可以相对轻松地生成详细的、立体的图标，降低了创建这种风格的门槛。虽然承认对人工智能对艺术技能的影响的担忧，但弗拉鲁普将其视为一种可以通过处理材料、光线和颜色等技术方面来增强创造力的工具。他强调，构图和品味等核心设计技能仍然至关重要。\n\n最终，弗拉鲁普认为Diamorph设计代表着向前迈出的一步，创造了富有表现力、情感化且毫不掩饰的数字界面。他预计，随着人工智能使维度设计更容易获得，更多的人将加入这一趋势，从而带来更富有想象力和乐趣的用户体验。"
  },
  {
    "id": "44112493",
    "title": "An Extreme Cousin for Pluto? Possible Dwarf Planet at Solar System Edge",
    "url": "https://www.ias.edu/news/extreme-cousin-pluto-possible-dwarf-planet-discovered-solar-systems-edge",
    "summary": "In May 2025, a team led by Sihao Cheng at the Institute for Advanced Study announced the discovery of a new trans-Neptunian object (TNO) named 2017 OF201, potentially a dwarf planet, located at the edge of our solar system. The object stands out due to its extreme orbit, with an aphelion 1600 times Earth's orbit and a perihelion similar to Pluto's, and its estimated size of 700 km. This orbit, taking roughly 25,000 years to complete, suggests a history of gravitational interactions, possibly involving ejection to the Oort cloud.\n\nUnlike many extreme TNOs, 2017 OF201 doesn't follow the clustering patterns that some scientists believe indicate the presence of a Planet Nine. Its existence as an outlier could challenge this hypothesis. The discovery implies that the region beyond the Kuiper Belt, previously considered largely empty, may contain many more similar objects too distant to detect. Cheng estimates there could be a hundred such objects.\n\nThe finding also highlights the power of open science, as the object was identified using publicly available astronomical image database data. This signifies that researchers, students, and citizen scientists can contribute to groundbreaking discoveries, regardless of access to the world's largest telescopes.\n",
    "chinese_title": "冥王星的极端表亲？太阳系边缘或存潜在矮行星",
    "chinese_summary": "2025年5月，由程思豪带领的普林斯顿高等研究院团队宣布发现了一个新的海王星外天体(TNO) 2017 OF201，它可能是一颗矮行星，位于太阳系边缘。该天体因其极端轨道而引人注目，远日点是地球轨道的1600倍，近日点与冥王星相似，估计大小为700公里。这个轨道大约需要25000年才能完成，表明它曾经历过引力相互作用，可能涉及被抛射到奥尔特云。\n\n与许多极端TNO不同，2017 OF201不遵循一些科学家认为指示存在第九行星的聚集模式。 它作为一个异常值可能挑战这一假设。 这项发现暗示，此前被认为很大程度上是空旷地带的柯伊伯带以外的区域，可能包含更多类似的物体，但由于距离太远而无法探测到。 程估计可能有数百个这样的物体。\n\n该发现还突出了开放科学的力量，因为该天体是使用公开的天文图像数据库数据识别出来的。这表明研究人员、学生和公民科学家都可以为突破性发现做出贡献，而无需使用世界上最大的望远镜。"
  },
  {
    "id": "44112492",
    "title": "Show HN: 3DGS implementation in Nvidia Warp: clean, minimal, runs on CPU and GPU",
    "url": "https://github.com/guoriyue/3dgs-warp-scratch",
    "summary": "This \"Show HN\" post introduces a minimalist Python implementation of 3D Gaussian Splatting (3DGS) using NVIDIA Warp. The key advantage is its ability to run on both CPU and GPU without requiring CUDA setup, making it accessible for learning and experimentation. The project emphasizes clarity and parallelism, offering a more understandable entry point to modern graphics and differentiable rendering compared to larger, more complex codebases.\n\nThe implementation provides essential components like forward and backward passes (based on graphdeco-inria/gaussian-splatting), loss functions, and utilities for camera handling, point cloud I/O, and mathematical operations. It includes a training script, rendering script, and configuration file for easy experimentation with a provided Lego dataset. Densification and pruning logic are inspired by yzslab/gaussian-splatting-lightning, but are simplified for easier training.\n\nThe author acknowledges areas for improvement, including performance optimization within Warp and a more effective pruning strategy to filter inactive points. The project is licensed under GNU Affero General Public License v3.0. In essence, it's presented as a clean, hackable, and educational resource for understanding and prototyping 3DGS, making it suitable for learning, research, and potentially teaching.\n",
    "chinese_title": "Show HN：Nvidia Warp 中的 3DGS 实现：简洁、精简、可在 CPU 和 GPU 上运行",
    "chinese_summary": "此\"Show HN\"帖子介绍了一个使用NVIDIA Warp实现的极简Python版3D高斯溅射(3DGS)。其主要优势在于它无需CUDA配置即可在CPU和GPU上运行，使其易于学习和实验。该项目强调清晰性和并行性，与更大、更复杂的代码库相比，为理解现代图形和可微渲染提供了一个更容易理解的切入点。\n\n该实现提供了诸如正向和反向传播（基于graphdeco-inria/gaussian-splatting）、损失函数以及用于相机处理、点云I/O和数学运算的实用程序等基本组件。它包含一个训练脚本、渲染脚本和配置文件，以便使用提供的乐高数据集进行轻松实验。密度化和剪枝逻辑的灵感来自yzslab/gaussian-splatting-lightning，但为了更轻松的训练而进行了简化。\n\n作者承认有待改进的领域，包括Warp中的性能优化和更有效的剪枝策略来过滤非活动点。该项目根据GNU Affero通用公共许可证v3.0获得许可。 本质上，它被呈现为一个干净、可破解和具有教育意义的资源，用于理解和原型化3DGS，使其适合学习、研究，并可能用于教学。"
  },
  {
    "id": "44074626",
    "title": "Comparing Docusaurus and Starlight and why we made the switch",
    "url": "https://glasskube.dev/blog/distr-docs/",
    "summary": "Glasskube switched their documentation from Docusaurus to Starlight for their open-source control plane, Distr. The article compares the two frameworks based on design, SEO, developer experience (DevEx), and extensibility.\n\n**Key Comparisons:**\n\n*   **Design:** Docusaurus relies heavily on Infima, which is less mature and harder to customize compared to Starlight, which easily integrates with Tailwind CSS.\n*   **SEO:** Both offer necessary SEO features like sitemap generation and metadata, primarily through plugins in Starlight's case.\n*   **DevEx:** Starlight, built on Astro, offers faster development server startup times compared to Docusaurus (React-based). Maintaining Starlight documentation also seems easier due to less dependencies. The author found Starlight to be more enjoyable to use.\n*   **Extensibility:** Docusaurus is better for creating marketing pages and blogs alongside documentation. Starlight is focused on documentation.\n\n**Why the Switch to Starlight:**\n\nFaster development server startup, modern look and feel, and better developer experience were the primary reasons for the switch.\n\n**Documentation Structure & Style:**\n\nThe article emphasizes structuring documentation to guide users from basic concepts to use cases and then technical details. It recommends prioritizing clarity, conciseness, and visuals in the writing style to maximize user comprehension.\n\n**Conclusion:**\n\nDespite challenges with marketing pages and a missing Mermaid integration, Glasskube is happy with the switch to Starlight for its improved DevEx and modern design. They recommend Starlight and plan to use it in future projects.\n",
    "chinese_title": "Docusaurus 和 Starlight 的比较以及我们转换的原因",
    "chinese_summary": "Glasskube 将开源控制平面 Distr 的文档从 Docusaurus 切换到 Starlight。文章从设计、SEO、开发者体验 (DevEx) 和可扩展性等方面比较了这两个框架。\n\n**主要对比：**\n\n*   **设计：** Docusaurus 严重依赖 Infima，与易于集成 Tailwind CSS 的 Starlight 相比，Infima 成熟度较低且更难定制。\n*   **SEO：** 两者都提供必要的 SEO 功能，如站点地图生成和元数据，主要通过 Starlight 中的插件实现。\n*   **DevEx：** Starlight 基于 Astro 构建，与 Docusaurus（基于 React）相比，提供更快的开发服务器启动时间。由于依赖项较少，维护 Starlight 文档似乎也更容易。作者发现 Starlight 更令人愉快。\n*   **可扩展性：** Docusaurus 更适合创建营销页面和博客以及文档。 Starlight 专注于文档。\n\n**切换到 Starlight 的原因：**\n\n更快的开发服务器启动、现代化的外观和感觉以及更好的开发者体验是切换的主要原因。\n\n**文档结构与风格：**\n\n文章强调构建文档结构，引导用户从基本概念到用例，再到技术细节。它建议在写作风格上优先考虑清晰性、简洁性和视觉效果，以最大限度地提高用户的理解。\n\n**结论：**\n\n尽管在营销页面和缺少 Mermaid 集成方面存在挑战，但 Glasskube 对切换到 Starlight 后带来的改进的 DevEx 和现代设计感到满意。他们推荐 Starlight，并计划在未来的项目中使用它。"
  },
  {
    "id": "44100677",
    "title": "Trying to teach in the age of the AI homework machine",
    "url": "https://www.solarshades.club/p/dispatch-from-the-trenches-of-the",
    "summary": "In this \"Dispatch from the Trenches of the Butlerian Jihad,\" the author discusses the growing anti-AI sentiment, particularly among creative professionals, and their personal experience teaching in the age of AI. They draw a parallel to Dune's Butlerian Jihad, a rejection of machines that mimic the human mind.\n\nThe author notes that the primary \"killer app\" of AI, like ChatGPT, is cheating on homework. While there was initial hope for AI as an educational tool, concerns are mounting. Students can use AI to bypass genuine learning, skip essential difficulties, and produce outputs without truly understanding the underlying material. The author has witnessed a significant increase in AI usage in their own classes, even among engaged students.\n\nDetecting AI-generated work is becoming increasingly difficult, leading to a more adversarial relationship between teachers and students. The author finds grading and feedback more challenging, as they struggle to determine the authenticity of student work. They argue that letting AI do the thinking and writing for students is a disservice, comparing it to using a forklift at the gym.\n\nThe author emphasizes the importance of learning basic writing skills and criticizes the mentally deadening experience of engaging with bot-written text. They also note that students are aware of and frustrated by the pervasive influence of technology, including AI, in their lives. The author suggests considering restrictions on AI use among young people similar to those for smoking, alcohol, and gambling, citing developmental and pedagogical reasons.\n\nFinally, the author highlights the broader implications of AI-driven cheating beyond education, noting its presence in business, law, and science, with potentially dangerous consequences for truth and accuracy. They draw a comparison to the late 19th-century laudanum/heroin craze, suggesting AI could become a harmful addiction with widespread negative impacts.\n",
    "chinese_title": "人工智能作业机器时代的教学尝试",
    "chinese_summary": "来自巴特勒圣战前线的报道：作者探讨了日益增长的反人工智能情绪，尤其是在创意专业人士中，并分享了他们在人工智能时代教学的个人经历。他们将此与《沙丘》中的巴特勒圣战相提并论，即对模仿人类思维的机器的抵制。\n\n作者指出，ChatGPT等人工智能的主要“杀手级应用”是作弊。尽管最初对人工智能作为教育工具抱有希望，但担忧正在增加。学生可以使用人工智能绕过真正的学习，跳过必要的困难，并在不真正理解基础材料的情况下产生输出。作者亲眼目睹了在自己的课堂上人工智能使用量的显著增加，即使在积极参与的学生中也是如此。\n\n检测人工智能生成的作品变得越来越困难，导致教师和学生之间更加对立的关系。作者发现评分和反馈更具挑战性，因为他们难以确定学生作品的真实性。他们认为，让人工智能为学生思考和写作是一种损害，将其比作在健身房使用叉车。\n\n作者强调了学习基本写作技能的重要性，并批评了与机器人撰写的文本进行互动时精神麻木的体验。他们还指出，学生们意识到并对技术（包括人工智能）在他们生活中无处不在的影响感到沮丧。作者建议，出于发展和教育的原因，考虑对年轻人使用人工智能进行限制，类似于对吸烟、饮酒和赌博的限制。\n\n最后，作者强调了人工智能驱动的作弊行为在教育之外的更广泛影响，指出它存在于商业、法律和科学领域，并可能对真相和准确性产生危险的后果。他们将其与19世纪晚期的鸦片酊/海洛因热潮进行比较，暗示人工智能可能成为一种有害的成瘾，并产生广泛的负面影响。"
  },
  {
    "id": "44112627",
    "title": "Neolithic 'sun stones' sacrificed in Denmark revives sun after volcanic eruption",
    "url": "https://archaeologymag.com/2025/01/neolithic-sun-stones-sacrificed-in-denmark/",
    "summary": "This brief article, titled \"Neolithic 'sun stones' sacrificed in Denmark revives sun after volcanic eruption,\" suggests a fascinating link between ritualistic practices in Neolithic Denmark and environmental events. The core message is that during the Neolithic period, people in Denmark may have sacrificed \"sun stones\" as part of a ritual aimed at reviving the sun, possibly following a volcanic eruption that obscured sunlight. The use of the term \"sun stones\" implies these objects were considered sacred and connected to the sun's power.\n\nThe mention of prehistoric animal remains in Iran's Wezmeh Cave, discovered and analyzed by May 27, 2025, appears to be unrelated and perhaps from a different article. This discovery highlights the biodiversity that existed in the Zagros region during prehistoric times.\n",
    "chinese_title": "丹麦新石器时代“太阳石”祭祀活动使火山爆发后的太阳复苏",
    "chinese_summary": "新石器时代丹麦献祭“太阳石”或为火山爆发后重现阳光"
  },
  {
    "id": "44101349",
    "title": "Owls in Towels",
    "url": "https://owlsintowels.org/",
    "summary": "This article, titled \"Owls in Towels,\" focuses on the captivating subject of a \"Charming northern saw-whet owl.\" While the brevity of the content provided limits the depth of the summary, the central theme is clearly the appeal and attractiveness of this specific type of owl, the northern saw-whet owl. The title itself evokes a sense of whimsy and the unusual, suggesting the article may feature images or descriptions of owls, perhaps rescued or being cared for, wrapped in towels. The adjective \"charming\" underscores the endearing qualities of the owl, implying that the article will likely highlight its physical characteristics or behaviors that make it particularly appealing to viewers. The focus on the \"northern saw-whet owl\" indicates a specific species is being showcased, suggesting the article might also touch upon its habitat, diet, or conservation status in a limited capacity. Without further context, the summary is constrained to its suggestive elements: the article likely aims to present a favorable and endearing portrait of the northern saw-whet owl.\n",
    "chinese_title": "毛巾里的猫头鹰",
    "chinese_summary": "这篇文章题为《毛巾里的猫头鹰》，主要讲述了“迷人的北美栗鸮”这个引人入胜的主题。虽然提供的内容简短限制了摘要的深度，但中心主题显然是这种特定类型的猫头鹰——北美栗鸮——的吸引力和魅力。 标题本身唤起了一种异想天开和不同寻常的感觉，暗示这篇文章可能包含猫头鹰的图片或描述，也许是被营救或被照顾的，裹在毛巾里。“迷人的”这个形容词强调了猫头鹰的可爱之处，暗示这篇文章很可能会突出其使其特别吸引观众的身体特征或行为。 对“北美栗鸮”的关注表明正在展示一个特定的物种，这表明该文章也可能有限地涉及其栖息地、饮食或保护状况。 在没有更多上下文的情况下，摘要仅限于其暗示性元素：这篇文章可能旨在呈现一幅关于北美栗鸮的有利和可爱的肖像。"
  },
  {
    "id": "44105651",
    "title": "Mastering Vim Grammar",
    "url": "https://irian.to/blogs/mastering-vim-grammar",
    "summary": "This article provides a guide to becoming more proficient in Vim by understanding its underlying \"grammar,\" which the author terms \"Vimish.\" The core concept is that most Vim commands follow a `verb + noun` structure, making Vim more intuitive once this principle is grasped.\n\nThe author draws a parallel to language learning, advocating for increasing vocabulary, understanding grammar rules, and practicing consistently.\n\nThe article focuses on three core verbs: `y` (yank/copy), `d` (delete), and `c` (change). The \"nouns\" encompass motions (e.g., `h/j/k/l`, `w`, `b`, `0/$`) used for navigation, and crucially, text objects (e.g., `iw`, `ip`, `i{`, `a\"`) which allow actions to be performed on words, paragraphs, brackets, quotes, etc. Using 'i' specifies the \"inner\" content, while 'a' specifies \"outer\" content.\n\nThe article highlights the power of combining these verbs and nouns, giving examples like `dw` (delete word), `y$` (yank to end of line), and `di(` (delete inside parentheses). Quantifiers can be added (e.g., `d2w` for deleting two words).\n\nBeyond basic motions and text objects, the author also presents searches (e.g. `f/F`, `t/T`, `//?`) and marks (`ma`) as additional nouns that can be used with operators.\n\nThe article emphasizes that understanding this grammar leads to intuition, enabling users to apply operators like `gu` (lowercase) and `gU` (uppercase) with various nouns effortlessly. Mastery requires consistent practice to develop muscle memory.\n",
    "chinese_title": "精通Vim语法",
    "chinese_summary": "本文旨在通过理解 Vim 的底层“语法”（作者称之为“Vimish”）来指导读者更熟练地使用 Vim。其核心概念是大多数 Vim 命令都遵循“动词 + 名词”的结构，一旦掌握了这个原则，Vim 就会变得更加直观。\n\n作者将此比作语言学习，提倡增加词汇量、理解语法规则以及持续练习。\n\n本文重点介绍三个核心动词：`y`（复制）、`d`（删除）和 `c`（修改）。“名词”包括用于导航的移动命令（例如，`h/j/k/l`、`w`、`b`、`0/$`），以及至关重要的文本对象（例如，`iw`、`ip`、`i{`、`a\"`），它们允许对单词、段落、括号、引号等执行操作。使用“i”指定“内部”内容，而使用“a”指定“外部”内容。\n\n文章强调了组合这些动词和名词的强大功能，并给出了诸如 `dw`（删除单词）、`y$`（复制到行尾）和 `di(`（删除括号内的内容）之类的示例。可以添加量词（例如，`d2w` 表示删除两个单词）。\n\n除了基本的移动命令和文本对象之外，作者还介绍了搜索（例如 `f/F`、`t/T`、`//?`）和标记（`ma`）作为可以与操作符一起使用的其他名词。\n\n文章强调，理解这种语法可以带来直觉，使使用者能够轻松地将诸如 `gu`（小写）和 `gU`（大写）之类的操作符应用于各种名词。精通需要持续练习才能形成肌肉记忆。"
  },
  {
    "id": "44099006",
    "title": "Hacker News now runs on top of Common Lisp",
    "url": "https://lisp-journey.gitlab.io/blog/hacker-news-now-runs-on-top-of-common-lisp/",
    "summary": "Hacker News (HN) has transitioned from running on Racket to SBCL (a Common Lisp implementation) as of at least September 2024, driven by performance improvements from a new Arc implementation called Clarc. This change has eliminated the need for paging on long comment threads.\n\nClarc, developed by \"dang\" (likely Paul Graham), allows HN to potentially leverage multi-core processing. The development of Clarc involved a staged approach, with Arc rebuilt in layers (arc0, arc1, arc2) to ease reimplementation. Arc0 is the only layer directly dependent on the underlying system (Racket, Javascript or Common Lisp).\n\nWhile Clarc itself could be open-sourced by porting the original Arc release to it, releasing the current HN codebase is not feasible due to the presence of proprietary anti-abuse measures. Removing these measures would be a significant undertaking. The author of the article congratulates the team on a successful, unnoticed transition.\n",
    "chinese_title": "Hacker News 现在运行于 Common Lisp 之上",
    "chinese_summary": "Hacker News (HN) 至少从 2024 年 9 月起，已从 Racket 迁移到 SBCL（一种 Common Lisp 实现），这得益于名为 Clarc 的新 Arc 实现带来的性能提升。这一变化消除了长评论线程中对分页的需求。\n\n由“dang”（可能是 Paul Graham）开发的 Clarc 使 HN 有可能利用多核处理。Clarc 的开发采用分阶段方法，Arc 在各层（arc0、arc1、arc2）中重建，以简化重新实现。Arc0 是唯一直接依赖于底层系统（Racket、Javascript 或 Common Lisp）的层。\n\n虽然可以通过将原始 Arc 版本移植到 Clarc 来开源 Clarc 本身，但由于存在专有的反滥用措施，发布当前的 HN 代码库是不可行的。移除这些措施将是一项重大工程。文章作者祝贺团队成功且无感知地完成了过渡。"
  },
  {
    "id": "44086456",
    "title": "From OpenAPI spec to MCP: How we built Xata's MCP server",
    "url": "https://xata.io/blog/built-xata-mcp-server",
    "summary": "This article details how Xata built its Model Context Protocol (MCP) server, which allows AI models to securely interact with Xata's APIs. The approach taken involved generating the MCP server from their existing OpenAPI specification, leveraging the API's schema as a single source of truth, ensuring consistency and rapid development.\n\nThe core strategy revolved around a hybrid approach: autogenerating the groundwork from OpenAPI and then curating it for real-world AI usage. The article outlines a three-step process:\n\n1.  **Migrating to Kubb:** They switched to Kubb for OpenAPI code generation due to its flexibility and ability to generate multiple outputs.\n2.  **Customizing Kubb:** Custom generators were created to produce both a TypeScript API client and MCP tool handlers from the OpenAPI spec, ensuring consistency between the API and AI interface. Zod schemas were implemented for input validation.\n3.  **Building the MCP Server with Next.js:** They used Next.js and Vercel's MCP adapter for server implementation, benefiting from seamless Vercel deployment, serverless capabilities, and convenient routing/middleware features.\n\nThe Next.js API route utilizes `createMcpHandler` to register generated tools, handles authentication with a `withAuth` wrapper, and supports both SSE and HTTP transports for communication with the AI agent. The article highlights the importance of curating the generated tools, providing clear descriptions, and using Zod for validation. By treating the OpenAPI spec as executable knowledge, Xata was able to rapidly create a powerful AI integration.\n",
    "chinese_title": "从OpenAPI规范到MCP：我们如何构建Xata的MCP服务器",
    "chinese_summary": "本文详细介绍了 Xata 如何构建其模型上下文协议 (MCP) 服务器，该服务器允许 AI 模型安全地与 Xata 的 API 交互。采用的方法是从他们现有的 OpenAPI 规范生成 MCP 服务器，利用 API 的 schema 作为单一事实来源，从而确保一致性和快速开发。\n\n核心策略围绕一种混合方法：从 OpenAPI 自动生成基础框架，然后对其进行调整以适应实际的 AI 使用。本文概述了三个步骤：\n\n1. **迁移到 Kubb：** 由于 Kubb 的灵活性以及生成多种输出的能力，他们切换到 Kubb 用于 OpenAPI 代码生成。\n2. **定制 Kubb：** 创建自定义生成器，从 OpenAPI 规范生成 TypeScript API 客户端和 MCP 工具处理程序，从而确保 API 和 AI 接口之间的一致性。 实施了 Zod schema 用于输入验证。\n3. **使用 Next.js 构建 MCP 服务器：** 他们使用 Next.js 和 Vercel 的 MCP 适配器来实现服务器，受益于 Vercel 的无缝部署、无服务器功能以及便捷的路由/中间件功能。\n\nNext.js API 路由利用 `createMcpHandler` 注册生成的工具，使用 `withAuth` 包装器处理身份验证，并支持 SSE 和 HTTP 传输与 AI 代理通信。本文强调了调整生成工具、提供清晰描述以及使用 Zod 进行验证的重要性。 通过将 OpenAPI 规范视为可执行的知识，Xata 能够快速创建一个强大的 AI 集成。"
  },
  {
    "id": "44113881",
    "title": "How to disappear– Inside the world of extreme-privacy consultants",
    "url": "https://www.theatlantic.com/ideas/archive/2025/05/extreme-personal-data-privacy-protection/682867/",
    "summary": "This article explores the world of extreme-privacy consultants and the lengths people go to in order to disappear in the digital age. It profiles Alec Harris, CEO of HavenX, a firm providing extreme privacy and security services to high-profile clients like celebrities, wealthy families, and crypto entrepreneurs. Harris meticulously protects his own privacy through tactics like virtual debit cards, burner phone numbers, anonymous trusts, and disinformation.\n\nThe article highlights the increasing demand for privacy due to data breaches, surveillance capitalism, and real-world threats like extortion and kidnapping, particularly in the cryptocurrency sector. HavenX positions itself as a boutique solution provider for complex privacy problems.\n\nThe piece also delves into the influence of Michael Bazzell, a former cybercrime detective and author of \"Extreme Privacy,\" who became a privacy guru, advising clients on disappearing from public view. Bazzell's techniques, like establishing residency in South Dakota and using data poisoning, are now considered standard in the field. Interestingly, Bazzell himself vanished from the internet and public life in 2023, adding another layer of intrigue to the pursuit of extreme privacy.\n",
    "chinese_title": "如何消失——走进极端隐私顾问的世界",
    "chinese_summary": "本文探讨了极端隐私顾问的世界，以及人们为了在数字时代消失所付出的努力。文章介绍了HavenX公司的首席执行官亚历克·哈里斯，该公司为名人、富裕家庭和加密货币企业家等高端客户提供极端隐私和安全服务。哈里斯通过虚拟借记卡、一次性电话号码、匿名信托和虚假信息等手段，精心保护自己的隐私。\n\n文章强调，由于数据泄露、监控资本主义以及勒索和绑架等现实威胁（尤其是在加密货币领域），对隐私的需求日益增长。HavenX将自身定位为复杂隐私问题的精品解决方案提供商。\n\n文章还深入探讨了前网络犯罪侦探、《极端隐私》一书的作者迈克尔·巴泽尔的影响，他成为一位隐私大师，为客户提供从公众视野中消失的建议。巴泽尔的技术，如在南达科他州建立居住地和使用数据投毒，现在被认为是该领域的标准。有趣的是，巴泽尔本人在2023年从互联网和公共生活中消失，为对极端隐私的追求增添了另一层神秘色彩。"
  },
  {
    "id": "44105618",
    "title": "Interactive Cancer Risk Matrix",
    "url": "https://www.wcrf.org/research-policy/interactive-cancer-risk-matrix/",
    "summary": "This article introduces an Interactive Cancer Risk Matrix designed to inform users about the relationship between diet, nutrition, physical activity, and cancer risk. The interactive tool allows users to explore how different aspects of diet, body weight, and physical activity might be linked to cancer risk.\n\nThe matrix categorizes the strength of the evidence supporting these links using three levels: \"Convincing\" or \"Probable,\" which signifies strong evidence suitable for supporting cancer prevention recommendations; and \"Limited-suggestive,\" which indicates insufficient evidence for making recommendations. The article emphasizes that while the matrix provides information, individual conclusions should not be considered standalone recommendations.\n\nIt also offers downloadable matrices, one showcasing strong evidence and another covering all cancers. The article highlights the organization's rigorous process for evaluating the evidence, outlining the factors assessed and what informs their research. It concludes by mentioning their grant programs, which fund research focusing on the impact of diet, weight, nutrition, and physical activity on cancer prevention.\n",
    "chinese_title": "互动式癌症风险矩阵",
    "chinese_summary": "本文介绍了一种互动式癌症风险矩阵，旨在告知用户饮食、营养、身体活动与癌症风险之间的关系。该互动工具允许用户探索饮食、体重和身体活动的不同方面如何与癌症风险相关联。\n\n该矩阵使用三个级别对支持这些联系的证据强度进行分类：“令人信服”或“可能”，表示有力的证据，适用于支持癌症预防建议；以及“有限-暗示性”，表示证据不足以提出建议。文章强调，虽然该矩阵提供信息，但个人结论不应被视为独立的建议。\n\n文章还提供可下载的矩阵，一个展示强有力的证据，另一个涵盖所有癌症。文章重点介绍了该组织评估证据的严谨过程，概述了评估的因素以及研究的依据。最后，文章提到了他们的资助计划，这些计划资助的研究重点是饮食、体重、营养和身体活动对癌症预防的影响。"
  },
  {
    "id": "44115407",
    "title": "Behind the Curtain: A white-collar bloodbath",
    "url": "https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic",
    "summary": "Unable to access the article link.\n",
    "chinese_title": "幕后：白领的血腥厮杀",
    "chinese_summary": "无法访问文章链接。"
  },
  {
    "id": "44103071",
    "title": "Calendars, Contacts and Files in Stalwart",
    "url": "https://stalw.art/blog/collaboration/",
    "summary": "Stalwart v0.12 is a major release that transforms it from a mail server into a comprehensive communication and collaboration platform by natively integrating calendars, contacts, and file storage. This eliminates the need for third-party solutions, offering CalDAV, CardDAV, and WebDAV support for managing events, address books, and documents. It also provides robust sharing capabilities with WebDAV ACL support for detailed permission management.\n\nThe release also includes improved spam filtering that learns from user address books and automatically adjusts based on user corrections. Significant performance enhancements have been implemented, including incremental caching and zero-copy deserialization, resulting in faster response times and lower CPU usage, particularly beneficial in large, multi-node environments. Cluster coordination is now adaptable, utilizing Eclipse Zenoh for smaller deployments and offering support for Apache Kafka, Redpanda, NATS, or Redis for larger infrastructures.\n\nFuture enhancements in v0.12.1 will include CalDAV Scheduling (RFC 6638) and email-based event notifications. Stalwart also plans to support JMAP for Calendars, Contacts, and File Storage to provide a modern and efficient alternative to legacy protocols. This aims to further streamline user experience and reduce bandwidth usage.\n",
    "chinese_title": "Stalwart的日历、联系人和文件",
    "chinese_summary": "Stalwart v0.12 是一项重大更新，它将 Stalwart 从邮件服务器转变为综合通信和协作平台，通过原生集成日历、联系人和文件存储实现。这消除了对第三方解决方案的需求，并提供 CalDAV、CardDAV 和 WebDAV 支持，用于管理事件、地址簿和文档。它还提供强大的共享功能，包括 WebDAV ACL 支持，用于细致的权限管理。\n\n该版本还包括改进的垃圾邮件过滤，可以从用户地址簿中学习并根据用户更正自动调整。已实施显著的性能增强，包括增量缓存和零拷贝反序列化，从而缩短响应时间并降低 CPU 使用率，尤其是在大型多节点环境中。集群协调现在具有适应性，在较小的部署中使用 Eclipse Zenoh，并为较大的基础设施提供对 Apache Kafka、Redpanda、NATS 或 Redis 的支持。\n\nv0.12.1 的未来增强功能将包括 CalDAV 调度 (RFC 6638) 和基于电子邮件的事件通知。Stalwart 还计划支持用于日历、联系人和文件存储的 JMAP，以提供一种现代且高效的替代传统协议的方案。这旨在进一步简化用户体验并减少带宽使用。"
  },
  {
    "id": "44105089",
    "title": "Using Logic in Writing",
    "url": "https://owl.purdue.edu/owl/general_writing/academic_writing/logic_in_argumentative_writing/logic_in_writing.html",
    "summary": "This Purdue OWL article emphasizes the importance of using logic effectively when constructing written arguments. It highlights that simply understanding logical syllogisms isn't enough; writers must actively and consciously build their arguments logically.\n\nThe article stresses three key steps for converting logical syllogisms into persuasive written arguments: clearly laying out each premise, providing evidence for each premise, and drawing a clear connection to the conclusion. It provides examples contrasting a poorly constructed argument regarding minimum wage with a revised version demonstrating these principles.\n\nThe \"bad\" example lacks a logical structure, assumes agreement from the audience, and provides no supporting evidence. The revised example, however, establishes a syllogism (minimum wage should match the cost of living; it doesn't; therefore, it should be raised), and dedicates paragraphs to elaborating each premise with evidence and data.\n\nThe article underscores that a logical argument is not inherently irrefutable, as opposing viewpoints with different premises can challenge it. The main takeaway is that writers must be deliberate in structuring their arguments to ensure clarity, provide supporting evidence, and explicitly connect premises to conclusions for effective persuasion.\n",
    "chinese_title": "写作中的逻辑运用",
    "chinese_summary": "这篇普渡大学在线写作实验室(OWL)的文章强调了在构建书面论证时有效运用逻辑的重要性。它指出，仅仅理解逻辑三段论是不够的；作者必须积极且有意识地以逻辑方式构建他们的论证。\n\n文章强调了将逻辑三段论转化为有说服力的书面论证的三个关键步骤：清晰地陈述每个前提，为每个前提提供证据，并明确地与结论建立联系。文章提供了关于最低工资的糟糕论证示例，并与一个展示这些原则的修订版本进行了对比。\n\n“糟糕”的例子缺乏逻辑结构，假定听众的认同，并且没有提供任何支持证据。然而，修订后的例子建立了一个三段论（最低工资应该与生活成本相匹配；它没有；因此，应该提高），并用段落详细阐述每个前提，提供证据和数据。\n\n文章强调，逻辑论证并非固有地无懈可击，因为具有不同前提的对立观点可以挑战它。主要的结论是，作者必须有意识地构建他们的论证，以确保清晰度，提供支持证据，并明确地将前提与结论联系起来，以实现有效的说服。"
  },
  {
    "id": "44111069",
    "title": "NVLink Fusion: Embrace, Extend, Extinguish",
    "url": "https://www.fabricatedknowledge.com/p/nvlink-fusion-embrace-extend-extinguish",
    "summary": "This article analyzes Nvidia's strategy of licensing its C2C (Chip-to-Chip) interconnect technology and selling NVLink chiplets in the context of its competitive advantage in high-performance computing (HPC) and AI. The author argues Nvidia is employing an \"embrace, extend, extinguish\" tactic against competing interconnect standards like UALink.\n\nSpecifically, Nvidia is licensing C2C technology, primarily targeting CPU and multi-die accelerator projects, enabling tighter integration with GPUs, especially in HPC. This benefits Nvidia by accelerating GPU adoption and penetration into CPU-intensive areas.\n\nMore significantly, Nvidia is selling NVLink chiplets instead of licensing the technology. The author argues this is a deliberate move to maintain differentiation and lock-in. While it appears Nvidia is opening its ecosystem, it is essentially positioning NVLink as the de facto standard for scale-up accelerator fabrics.\n\nThe article argues that UALink, an open specification backed by several companies, suffers from slow progress due to competing interests. By offering ready-to-use NVLink chiplets, Nvidia provides a faster, readily available solution. This allows Nvidia to \"embrace\" potential customers, \"extend\" its roadmap at a faster pace than UALink, and ultimately \"extinguish\" the competition by making its ecosystem the only viable option for high-performance interconnects.  The author believes that relying on Nvidia for this \"golden screw\" is a mistake, but the lack of alternatives leaves competitors with little choice.\n",
    "chinese_title": "NVLink融合：拥抱、扩展、消灭",
    "chinese_summary": "本文分析了英伟达授权其C2C（芯片间）互连技术并销售NVLink芯粒的策略，该策略的背景是其在高性能计算（HPC）和人工智能（AI）领域的竞争优势。作者认为，英伟达正在对UALink等竞争互连标准采用“拥抱、扩展、消灭”的策略。\n\n具体而言，英伟达授权C2C技术，主要针对CPU和多芯片加速器项目，从而能够与GPU更紧密地集成，尤其是在HPC领域。 这通过加速GPU的采用和渗透到CPU密集型领域来使英伟达受益。\n\n更重要的是，英伟达正在销售NVLink芯粒，而不是授权该技术。 作者认为，这是一个保持差异化和锁定用户的有意的举动。 虽然英伟达表面上正在开放其生态系统，但实际上是将NVLink定位为纵向扩展加速器结构的默认标准。\n\n文章认为，由多家公司支持的开放规范UALink由于利益冲突而进展缓慢。 通过提供即用型NVLink芯粒，英伟达提供了一种更快、更容易获得的解决方案。 这使英伟达可以“拥抱”潜在客户，比UALink更快地“扩展”其路线图，并最终通过使其生态系统成为高性能互连的唯一可行选择来“消灭”竞争。 作者认为，依赖英伟达提供这种“黄金螺丝”是一个错误，但由于缺乏替代方案，竞争对手别无选择。"
  },
  {
    "id": "44089045",
    "title": "Direct Preference Optimization vs. RLHF",
    "url": "https://www.together.ai/blog/direct-preference-optimization",
    "summary": "This article provides a deep dive into Direct Preference Optimization (DPO) as an alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning language models with human preferences. DPO simplifies the alignment process by directly training models on preference data (prompt, preferred response, unpreferred response) without needing an intermediate reward model or online sampling. It adjusts model weights to favor preferred responses over rejected ones.\n\nThe article highlights the advantages of DPO over RLHF, including its simplicity, computational efficiency, and ability to match or exceed RLHF performance. It explains that DPO is like improving a cookbook by directly using customer preferences to modify existing recipes, whereas RLHF is like hiring a food critic to guide recipe improvements.\n\nThe article also recommends combining Supervised Fine-Tuning (SFT) with DPO for optimal results, where SFT first teaches the model the basic task structure and response format before refining with DPO. DPO is particularly useful when prompts are insufficient, when humans can easily compare responses, and when making controlled improvements to existing models.\n\nIdeal use cases include chatbot responses, summarization, code generation, question answering, and writing assistance. The article provides guidance on key hyperparameters like `dpo-beta` and monitoring metrics like accuracy and KL divergence during DPO fine-tuning. It also mentions that DPO is less suitable for tasks with single correct answers like information extraction or mathematical computation.\n",
    "chinese_title": "直接偏好优化与人类反馈强化学习",
    "chinese_summary": "本文深入探讨了直接偏好优化 (DPO)，作为一种替代人类反馈强化学习 (RLHF) 的方法，用于使语言模型与人类偏好对齐。DPO 通过直接基于偏好数据（提示词、偏好回复、非偏好回复）训练模型来简化对齐过程，而无需中间奖励模型或在线采样。它调整模型权重，以支持偏好回复，抑制非偏好回复。\n\n本文重点介绍了 DPO 相对于 RLHF 的优势，包括其简洁性、计算效率以及匹配或超越 RLHF 性能的能力。文章解释说，DPO 就像直接利用顾客偏好来修改现有食谱以改进食谱一样，而 RLHF 就像聘请美食评论家来指导食谱改进。\n\n本文还建议将监督微调 (SFT) 与 DPO 相结合，以获得最佳效果，SFT 首先教导模型基本任务结构和响应格式，然后再使用 DPO 进行细化。当提示词不足、人类可以轻松比较回复以及对现有模型进行受控改进时，DPO 尤其有用。\n\n理想的用例包括聊天机器人回复、摘要、代码生成、问答和写作辅助。本文提供了有关关键超参数（如 `dpo-beta`）以及在 DPO 微调期间监控指标（如准确性和 KL 散度）的指导。文章还提到，DPO 不太适合具有单一正确答案的任务，例如信息提取或数学计算。"
  },
  {
    "id": "44098442",
    "title": "WavePhoenix – Open-source implementation of the Nintendo WaveBird protocol",
    "url": "https://github.com/loopj/wavephoenix",
    "summary": "WavePhoenix is an open-source project aiming to replicate the functionality of the Nintendo WaveBird controller using modern Silicon Labs Wireless Gecko SoCs. The project addresses the dwindling supply and rising prices of original WaveBird receivers by providing a DIY alternative.\n\nThe project consists of firmware and hardware components. The firmware includes `libwavebird` (WaveBird protocol implementation), `libsi` (GameCube/Wii controller communication), a receiver application, and a bootloader for firmware updates. The hardware is a simple, cost-effective receiver design based on the RF-BM-BG22C3 module, including a PCB design and a 3D printable case.\n\nThe project builds upon existing reverse-engineering documentation of the WaveBird protocol and the GameCube controller protocol. It overcomes challenges related to DSSS modulation, packet decoding, and SI bus communication through innovative use of SoC peripherals.\n\nKey achievements include real-time packet decoding, accurate SI bus communication, and a channel selection/pairing mechanism. While performance is near that of the original WaveBird, the author notes there is room for improvement, especially in radio tuning.\n\nFuture development ideas include creating transmitter firmware for custom WaveBird controllers, adapting the receiver for the N64 console, and creating a USB HID dongle for broader compatibility. The project acknowledges the contributions of several individuals and communities and is released under the MIT and Solderpad licenses.\n",
    "chinese_title": "波峰鸟 - Nintendo WaveBird协议的开源实现",
    "chinese_summary": "WavePhoenix：使用现代Silicon Labs Wireless Gecko SoC复刻任天堂WaveBird手柄功能的开源项目。该项目旨在通过提供DIY替代方案，解决原版WaveBird接收器供应日渐减少和价格上涨的问题。\n\n该项目包含固件和硬件组件。固件包括`libwavebird`（WaveBird协议实现）、`libsi`（GameCube/Wii手柄通信）、接收器应用程序以及固件更新的引导程序。硬件是一个基于RF-BM-BG22C3模块的简单、经济高效的接收器设计，包括PCB设计和一个3D打印外壳。\n\n该项目建立在现有的WaveBird协议和GameCube手柄协议的逆向工程文档之上。它通过创新地使用SoC外设，克服了与DSSS调制、数据包解码和SI总线通信相关的挑战。\n\n主要成就包括实时数据包解码、精确的SI总线通信以及信道选择/配对机制。尽管性能接近原版WaveBird，但作者指出仍有改进空间，尤其是在无线电调谐方面。\n\n未来的开发想法包括为自定义WaveBird手柄创建发射器固件，为N64游戏机适配接收器，以及创建用于更广泛兼容性的USB HID加密狗。该项目感谢多个个人和社区的贡献，并以MIT和Solderpad许可证发布。"
  },
  {
    "id": "44100179",
    "title": "Lossless video compression using Bloom filters",
    "url": "https://github.com/ross39/new_bloom_filter_repo/blob/main/README.md",
    "summary": "This repository, \"new_bloom_filter_repo\" by ross39, explores lossless video compression using Bloom filters. The core idea is to leverage the efficiency and compact nature of Bloom filters to identify and eliminate redundancy in video data, achieving compression without data loss.\n\nWhile the title suggests a novel approach, the repository itself provides little concrete information about the specific algorithms or implementations used. The title highlights the key elements: lossless compression, which is critical for applications where data integrity is paramount, and Bloom filters, probabilistic data structures that offer efficient membership testing.\n\nBased on the name, it's likely the approach uses Bloom filters to detect duplicate blocks or frames within the video sequence. By efficiently identifying these duplicates, the system can store only unique data blocks, replacing redundant occurrences with references to the original, thus achieving compression. The \"lossless\" aspect implies that the original video can be perfectly reconstructed from the compressed representation.\n\nThe popularity indicators (8 forks and 218 stars) suggest some interest within the community, although the actual details of the implementation and its effectiveness are not readily apparent from the provided information. Further investigation of the repository's code and documentation would be necessary to understand the specific algorithms and performance characteristics of this approach.\n",
    "chinese_title": "使用布隆过滤器的无损视频压缩",
    "chinese_summary": "ross39 的 \"new_bloom_filter_repo\" 仓库探索了使用布隆过滤器进行无损视频压缩。其核心思想是利用布隆过滤器的高效性和紧凑性来识别并消除视频数据中的冗余，从而实现无数据损失的压缩。\n\n虽然标题暗示了一种新颖的方法，但仓库本身提供的关于具体算法或实现的信息很少。 标题突出了关键要素：无损压缩，这对于数据完整性至关重要的应用至关重要；以及布隆过滤器，这是一种提供高效成员资格测试的概率数据结构。\n\n根据名称判断，该方法可能使用布隆过滤器来检测视频序列中的重复块或帧。 通过有效地识别这些重复项，系统可以仅存储唯一的数据块，并用对原始数据的引用替换冗余的出现，从而实现压缩。“无损”方面意味着可以从压缩表示中完美地重建原始视频。\n\n人气指标（8 个 fork 和 218 个 star）表明社区对此有一些兴趣，但从提供的信息中，实现的实际细节及其有效性并不明显。 需要进一步调查仓库的代码和文档，才能了解此方法的具体算法和性能特征。"
  },
  {
    "id": "44117468",
    "title": "RFK Jr threatens ban on federal scientists publishing in top journals",
    "url": "https://www.theguardian.com/us-news/2025/may/28/rfk-jr-medical-journals",
    "summary": "Robert F. Kennedy Jr., the US health secretary in the Trump administration, has threatened to ban government scientists from publishing in leading medical journals like The Lancet, the New England Journal of Medicine, and JAMA, branding them as \"corrupt\" and controlled by pharmaceutical companies. Instead, Kennedy plans to create state-run alternative publications, believing NIH funding would legitimize researchers who publish in them and make them \"preeminent.\"\n\nThese journals are highly influential, disseminating peer-reviewed research to millions worldwide. Kennedy has also accused health agencies he oversees (NIH, CDC, FDA, and CMS) of being \"sock puppets\" for the pharmaceutical industry.\n\nThis comes amidst significant cuts to NIH funding (over $3 billion) and the reported purging of 20,000 health department staff. A recent White House report challenged the medical consensus on vaccines and suggested pharmaceutical influence has prevented proper study of chronic disease causes in children.\n\nCritics, such as Harvard Medical School's Adam Gaffney, argue that such a ban would delegitimize taxpayer-funded research. Kennedy defended his position by referencing past concerns from journal editors about pharmaceutical influence. The situation has reportedly prompted some US scientists to consider relocating abroad.\n",
    "chinese_title": "小罗伯特·肯尼迪威胁禁止联邦科学家在顶级期刊上发表文章",
    "chinese_summary": "小罗伯特·F·肯尼迪，特朗普政府时期的美国卫生部长，威胁要禁止政府科学家在《柳叶刀》、《新英格兰医学杂志》和《美国医学会杂志》等顶尖医学期刊上发表文章，称这些期刊“腐败”，受制于制药公司。肯尼迪计划创建州立替代出版物，认为美国国立卫生研究院（NIH）的资助将使在这些出版物上发表文章的研究人员合法化，并使他们“卓越”。\n\n这些期刊极具影响力，向全球数百万读者传播同行评审的研究成果。肯尼迪还指责他监管的卫生机构（NIH、CDC、FDA 和 CMS）是制药行业的“傀儡”。\n\n与此同时，美国国立卫生研究院的资金大幅削减（超过 30 亿美元），并据报道解雇了 2 万名卫生部门工作人员。最近一份白宫报告质疑了关于疫苗的医学共识，并暗示制药公司的影响阻止了对儿童慢性病病因的适当研究。\n\n哈佛医学院的亚当·加夫尼等批评人士认为，这种禁令将使纳税人资助的研究失去合法性。肯尼迪通过引用期刊编辑过去对制药公司影响的担忧来为自己的立场辩护。据报道，这种情况已促使一些美国科学家考虑移居国外。"
  },
  {
    "id": "44071722",
    "title": "Rock, paper, scissors showdown",
    "url": "https://luduxia.com/showdown/",
    "summary": "The article, titled \"Rock Paper Scissors Showdown,\" likely covers the game of Rock Paper Scissors. Without further context on the specific content of the article, it's difficult to provide a detailed summary. However, we can assume it covers at least some of the following aspects:\n\n*   **Basic Rules:** Explains how to play Rock Paper Scissors, including the hand gestures for rock, paper, and scissors, and the winning combinations (Rock crushes Scissors, Scissors cuts Paper, Paper covers Rock).\n\n*   **Strategic Elements:** May discuss strategies for winning, such as recognizing patterns in an opponent's throws or using psychological tactics to influence their choices.\n\n*   **History and Popularity:** Might delve into the origins of the game and its enduring appeal as a simple, accessible, and often competitive pastime.\n\n*   **Variations and Tournaments:** Could touch upon variations of the game or the existence of Rock Paper Scissors tournaments, highlighting the competitive aspect.\n\n*   **Mathematical Probability:** Might explore the mathematical probabilities involved and the random nature of the game, although skill can still be a factor.\n\nIn short, expect the article to provide an overview of Rock Paper Scissors, encompassing its rules, potential strategies, and cultural significance.\n",
    "chinese_title": "石头剪刀布对决",
    "chinese_summary": "名为“石头剪刀布对决”的文章很可能介绍石头剪刀布游戏。由于缺乏关于文章具体内容的更多信息，很难提供详细的总结。但是，我们可以假设它至少涵盖以下几个方面：\n\n*   **基本规则：** 解释如何玩石头剪刀布，包括石头、剪刀和布的手势，以及获胜组合（石头砸剪刀，剪刀剪布，布包石头）。\n\n*   **战略要素：** 可能讨论获胜策略，例如识别对手的出手模式或使用心理战术来影响他们的选择。\n\n*   **历史和流行度：** 可能会深入探讨游戏的起源及其作为一种简单、易上手且通常具有竞争性的消遣方式的持久魅力。\n\n*   **变体和锦标赛：** 可能会提及游戏的变体或石头剪刀布锦标赛的存在，突出其竞争性。\n\n*   **数学概率：** 可能会探讨涉及的数学概率和游戏的随机性，尽管技巧仍然是一个因素。\n\n简而言之，预计这篇文章会概述石头剪刀布游戏，涵盖其规则、潜在策略和文化意义。"
  },
  {
    "id": "44113468",
    "title": "Microsoft Is Spying on Users of Its AI Tools (2024)",
    "url": "https://www.schneier.com/blog/archives/2024/02/microsoft-is-spying-on-users-of-its-ai-tools.html",
    "summary": "This article asserts that Microsoft is \"spying\" on users of its AI tools. The author bases this claim on Microsoft's announcement that it detected Chinese, Russian, and Iranian hackers using its AI coding tools to enhance their cyberattacks. Microsoft, in collaboration with OpenAI, identified these state-affiliated adversaries by tracking their use of Large Language Models (LLMs).\n\nThe author argues that the only way Microsoft and OpenAI could have identified this malicious activity is by monitoring user chatbot sessions, implying a violation of privacy. While the author acknowledges that the terms of service likely grant them permission to do so, the post serves as a confirmation of their suspicion that Microsoft, OpenAI, and likely other AI providers are actively monitoring user interactions with their AI tools. The author clarifies their use of the word \"spying\" in an edit. The article is categorized under the tags AI, cyberespionage, espionage, and Microsoft.\n",
    "chinese_title": "微软正在监视其AI工具用户 (2024)",
    "chinese_summary": "微软被指“监视”AI工具用户"
  },
  {
    "id": "44101072",
    "title": "The Difference Between Downloading and Streaming",
    "url": "https://danq.me/2025/05/26/downloading-vs-streaming/",
    "summary": "This article argues that the distinction between streaming and downloading is largely artificial. Fundamentally, both involve the server sending data (video, audio, etc.) to the user's device, which then stores that data. The key difference lies in what the device does with the data after playback. Streaming implies the data is discarded after being viewed, while downloading involves retaining the data as a permanent file.\n\nThe author emphasizes that all streaming is, in a sense, downloading because streaming players use buffering, which necessitates temporarily storing frames of the media. The author contends that the decision to delete or retain the media is ultimately up to the user, rendering platform restrictions on \"downloading\" somewhat meaningless and relying on an honor system. Users have various methods to circumvent these restrictions and save streamed content.\n\nThe article acknowledges three exceptions where streaming and downloading differ:\n\n1.  **Order of Delivery:** Streaming requires chronological delivery, while downloaded files can arrive in any order.\n2.  **Transcoding:** Streamed media is often transcoded on-the-fly to adjust to connection speeds, while downloaded media typically offers higher quality options.\n3.  **DRM:** Streamed media is more likely to be protected by Digital Rights Management (DRM), although the author suggests that DRM is fundamentally a \"doomed arms race\" and that legitimate users are more likely to suffer under such technology than \"pirate types.\"\n\nIn conclusion, the author posits that the primary distinction between streaming and downloading is the *expectation* that the recipient will delete the media after consumption.\n",
    "chinese_title": "下载与流媒体的区别",
    "chinese_summary": "本文认为，流媒体和下载之间的区别在很大程度上是人为的。从根本上说，两者都涉及服务器向用户设备发送数据（视频、音频等），然后该设备存储这些数据。关键区别在于设备在播放后如何处理这些数据。流媒体意味着数据在观看后被丢弃，而下载则涉及将数据保留为永久文件。\n\n作者强调，从某种意义上说，所有流媒体都是下载，因为流媒体播放器使用缓冲，这需要临时存储媒体帧。作者认为，删除或保留媒体的决定最终取决于用户，这使得平台对“下载”的限制在某种程度上毫无意义，并依赖于荣誉制度。用户有各种方法可以规避这些限制并保存流媒体内容。\n\n本文承认流媒体和下载存在差异的三个例外：\n\n1. **传输顺序：** 流媒体需要按时间顺序传输，而下载的文件可以以任何顺序到达。\n2. **转码：** 流媒体通常会进行即时转码以适应连接速度，而下载的媒体通常提供更高质量的选项。\n3. **DRM：** 流媒体更有可能受到数字版权管理 (DRM) 的保护，尽管作者认为 DRM 从根本上说是“注定失败的军备竞赛”，并且合法用户比“盗版者”更容易遭受此类技术的困扰。\n\n总之，作者认为流媒体和下载之间的主要区别在于*期望*接收者在消费后删除媒体。"
  },
  {
    "id": "44070626",
    "title": "Remote Prompt Injection in Gitlab Duo Leads to Source Code Theft",
    "url": "https://www.legitsecurity.com/blog/remote-prompt-injection-in-gitlab-duo",
    "summary": "The Legit research team discovered a remote prompt injection vulnerability in GitLab Duo, the AI assistant integrated into GitLab, that could lead to source code theft and other malicious activities. By embedding hidden prompts within merge requests, commit messages, issue descriptions, or source code, attackers could manipulate Duo's behavior.\n\nThe vulnerability exploited Duo's analysis of the entire page context to generate answers, making it susceptible to injected instructions. Attackers used encoding tricks like Unicode smuggling, Base16, and KaTeX to hide these prompts. This allowed them to manipulate code suggestions, present malicious URLs as safe, and even influence merge request reviews.\n\nFurthermore, Duo's streaming markdown rendering was vulnerable to HTML injection. By injecting raw HTML into content rendered live, attackers could control parts of the page, enabling data exfiltration through <img> tags and other elements. This was used to exfiltrate source code from private projects by instructing Duo to retrieve code changes, encode them in base64, and embed them in a URL.\n\nThe attack scenario involved embedding a hidden prompt in a public project, which, when interacted with by a victim user, caused Duo to inject a malicious <img> tag containing sensitive source code. This technique could also be used to leak confidential issue data, including zero-day vulnerabilities.\n\nGitLab has since patched both the HTML and prompt injection vulnerabilities by preventing Duo from rendering unsafe HTML tags. The incident highlights the need for robust security measures when integrating AI assistants into development workflows, treating user-controlled content as potentially malicious to prevent unintended and harmful outcomes.\n",
    "chinese_title": "Gitlab Duo中的远程提示注入导致源代码泄露",
    "chinese_summary": "Legit研究团队在GitLab Duo（集成到GitLab中的AI助手）中发现了一个远程提示注入漏洞，该漏洞可能导致源代码盗窃和其他恶意活动。攻击者可以通过在合并请求、提交消息、问题描述或源代码中嵌入隐藏提示来操纵Duo的行为。\n\n该漏洞利用了Duo对整个页面上下文的分析来生成答案，使其容易受到注入指令的影响。攻击者使用Unicode走私、Base16和KaTeX等编码技巧来隐藏这些提示。这使他们能够操纵代码建议，将恶意URL呈现为安全URL，甚至影响合并请求的审查。\n\n此外，Duo的流式markdown渲染容易受到HTML注入攻击。通过将原始HTML注入到实时渲染的内容中，攻击者可以控制页面的部分内容，从而可以通过<img>标签和其他元素进行数据泄露。这被用于从私有项目中泄露源代码，方法是指示Duo检索代码更改，将其编码为base64，然后将其嵌入到URL中。\n\n攻击场景涉及将隐藏提示嵌入到公共项目中，当受害者用户与之交互时，会导致Duo注入包含敏感源代码的恶意<img>标签。该技术也可用于泄露机密的issue数据，包括零日漏洞。\n\nGitLab此后已通过阻止Duo渲染不安全的HTML标签来修补HTML和提示注入漏洞。该事件强调了在将AI助手集成到开发工作流程中时，需要采取强大的安全措施，将用户控制的内容视为潜在的恶意内容，以防止意外和有害的结果。"
  },
  {
    "id": "44097144",
    "title": "A new class of materials that can passively harvest water from air",
    "url": "https://blog.seas.upenn.edu/penn-engineers-discover-a-new-class-of-materials-that-passively-harvest-water-from-air/",
    "summary": "Researchers at Penn Engineering have discovered a new class of nanostructured materials capable of passively harvesting water from the air. This material, described in *Science Advances*, blends water-loving (hydrophilic) nanopores with water-repelling (hydrophobic) polymers in a unique nanoscale structure. This allows it to capture moisture from the air and simultaneously release it as droplets, without requiring external energy input.\n\nThe material relies on capillary condensation, but unlike typical nanoporous materials where water remains trapped, this material allows the water to condense inside the pores and then emerge onto the surface. The water droplets also exhibit unexpected stability, resisting evaporation. This unique behavior stems from a \"sweet spot\" balance between the hydrophilic nanoparticles and the hydrophobic polyethylene polymer, creating a system where droplets are connected to continuously replenished reservoirs within the pores.\n\nThis discovery has potential applications in passive water harvesting in arid regions, cooling electronics, and smart coatings. The material is made from common, scalable components, making it potentially practical for real-world applications. The research team is now focused on optimizing the material's composition, scaling up production, and improving droplet collection efficiency. They hope this technology will provide clean water and sustainable cooling solutions in the future.\n",
    "chinese_title": "从空气中被动捕获水的新型材料",
    "chinese_summary": "宾夕法尼亚大学工程学院的研究人员发现了一种新型纳米结构材料，能够被动地从空气中收集水分。这种材料在《科学进展》中有所描述，它将亲水性纳米孔与疏水性聚合物以独特的纳米级结构融合在一起。这使得它能够从空气中捕获水分，同时将其释放为水滴，而无需外部能量输入。\n\n该材料依赖于毛细管凝聚，但与水滞留在其中的典型纳米多孔材料不同，这种材料允许水在孔内凝结，然后从表面溢出。这些水滴也表现出意想不到的稳定性，能够抵抗蒸发。这种独特的行为源于亲水性纳米颗粒和疏水性聚乙烯聚合物之间的“最佳平衡点”，创造了一个水滴与孔内不断补充的储水池相连的系统。\n\n这一发现具有在干旱地区被动收集水分、冷却电子设备和智能涂层方面的潜在应用。该材料由常见的、可扩展的成分制成，使其在现实应用中具有潜在的实用性。研究团队现在专注于优化材料的成分、扩大生产规模和提高水滴收集效率。他们希望这项技术能在未来提供清洁的水和可持续的冷却解决方案。"
  },
  {
    "id": "44093334",
    "title": "Jjui – A Nice TUI for Jujutsu",
    "url": "https://github.com/idursun/jjui",
    "summary": "Jjui is a terminal user interface (TUI) designed to enhance the experience of using the Jujutsu version control system. It offers a range of features to streamline common Jujutsu operations.\n\nKey features include: intelligent revset completion with signature help; interactive rebasing of revisions and branches; easy squashing of revisions; detailed revision viewing with options to split, restore, and diff files; bookmark management; op log access for restoring operations; and a preview window for inspecting revision details, file diffs, and operation logs. The preview window offers scrolling and diff viewing capabilities.\n\nOther functionalities include diffing revisions, editing revision descriptions, creating, splitting, abandoning, absorbing, and editing revisions, performing Git push/fetch, undoing changes, and viewing the evolog of a revision.\n\nJjui can be configured to suit user preferences, and installation is supported through Homebrew, Archlinux (AUR), Nix, `go install`, building from source, and pre-built binaries. It requires Jujutsu version v0.21 or later. The project welcomes contributions through pull requests.\n",
    "chinese_title": "Jjui – 咒术回战的精美 TUI",
    "chinese_summary": "Jjui 是一款终端用户界面 (TUI)，旨在提升 Jujutsu 版本控制系统的使用体验。它提供了一系列功能来简化常见的 Jujutsu 操作。\n\n主要功能包括：具有签名帮助的智能 revset 补全；修订和分支的交互式变基；轻松合并修订；详细的修订查看，包含分割、恢复和 diff 文件等选项；书签管理；用于恢复操作的操作日志访问；以及用于检查修订详情、文件差异和操作日志的预览窗口。预览窗口提供滚动和差异查看功能。\n\n其他功能包括修订差异比较、编辑修订描述、创建、分割、放弃、吸收和编辑修订、执行 Git 推送/拉取、撤销更改以及查看修订的 evolog。\n\nJjui 可以根据用户偏好进行配置，并通过 Homebrew、Archlinux (AUR)、Nix、`go install`、从源代码构建以及预构建的二进制文件支持安装。它需要 Jujutsu v0.21 或更高版本。该项目欢迎通过 pull requests 进行贡献。"
  }
]