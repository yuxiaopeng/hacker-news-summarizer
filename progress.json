[
  {
    "id": "44041738",
    "title": "Deep Learning Is Applied Topology",
    "url": "https://theahura.substack.com/p/deep-learning-is-applied-topology",
    "summary": "Unable to access the article link.\n",
    "chinese_title": "深度学习是应用拓扑学",
    "chinese_summary": "无法访问文章链接。"
  },
  {
    "id": "44042371",
    "title": "Show HN: 90s.dev - game maker that runs on the web",
    "url": "https://90s.dev/blog/finally-releasing-90s-dev.html",
    "summary": "90s.dev is a new web-based game maker inspired by tools like Pico-8, Tic-80, and Love2D, aiming to provide a unique development experience. It features a 320x180 canvas, runs apps in web workers for security and performance, and provides full WebGL2 access for 60 fps games. The platform allows publishing and loading apps from GitHub or NPM.\n\nA key feature is its innovative GUI API, built around an auto-layout system, \"refs\" (watchable pointers for view properties), and \"composites\" (abstract/concrete view separation using JSX). The \"refs\" are designed to avoid manual value setting by automatically updating when properties change. \"Composites\" enable users to override default app implementations.\n\nOriginally, the platform was designed with a shared file system drive, \"net/\", but this approach was replaced in favor of importing files from NPM or GitHub using CDN, creating a more convenient way to import files.\n\nThe author emphasizes the importance of community contribution, envisioning users creating and sharing apps, game assets, and games. The platform includes built-in apps for pixel art data creation (paint, sprite-maker, map-maker), but the author hopes the community will contribute apps, with sharing facilitated through a dedicated issue tracker, wiki, and discussion forum on GitHub. The author recognizes the initial release is too soon and plans to focus on adding more working apps, sample games, and tutorials.\n",
    "chinese_title": "Show HN: 90年代.dev - 在网页上运行的游戏制作器",
    "chinese_summary": "90s.dev 是一款全新的基于 Web 的游戏制作工具，灵感来源于 Pico-8、Tic-80 和 Love2D 等工具，旨在提供独特的开发体验。它拥有 320x180 的画布，在 Web Workers 中运行应用程序以确保安全和性能，并提供完整的 WebGL2 访问权限以实现 60 fps 的游戏。该平台允许从 GitHub 或 NPM 发布和加载应用程序。\n\n一个关键特性是其创新的 GUI API，围绕自动布局系统、“refs”（用于观察视图属性的可观察指针）和“composites”（使用 JSX 的抽象/具体视图分离）构建。“refs”旨在避免手动设置值，通过在属性更改时自动更新。“Composites”允许用户覆盖默认的应用程序实现。\n\n最初，该平台设计了一个共享文件系统驱动器 \"net/\"，但这种方法被取代，取而代之的是使用 CDN 从 NPM 或 GitHub 导入文件，从而创建了一种更方便的文件导入方式。\n\n作者强调社区贡献的重要性，设想用户创建和分享应用程序、游戏资源和游戏。该平台包括用于像素艺术数据创建（绘画、精灵制作、地图制作）的内置应用程序，但作者希望社区能够贡献应用程序，并通过 GitHub 上的专用问题跟踪器、Wiki 和讨论论坛来促进分享。作者认识到最初的版本发布为时过早，并计划专注于添加更多可用的应用程序、示例游戏和教程。"
  },
  {
    "id": "44043045",
    "title": "27000 Dragons and 10'000 Lights: GPU-Driven Clustered Forward Renderer",
    "url": "https://logdahl.net/p/gpu-driven",
    "summary": "This article details a high-performance, GPU-driven clustered forward renderer capable of rendering 27,000 Stanford dragons with 10,000 lights at 1080p above 60fps on a GTX 1070. The key innovation is moving significant portions of the rendering pipeline to the GPU, minimizing CPU draw calls using indirect multi draws.\n\nThe renderer stores object data on the GPU in an \"Object Buffer\" and uses a \"Draw Buffer\" populated by a GPU-based culling process, improving performance by only drawing visible objects. A compute shader performs frustum culling based on AABBs and compacts the draw list using atomic counters and subgroup ballots for efficient memory usage and spatial locality.\n\nTo mitigate overdraw, the renderer implements clustered shading. The view frustum is divided into clusters, and lights are assigned to these clusters based on their influence radius. This allows each fragment to only calculate shading using relevant lights, reducing unnecessary computations.  The article explains the cluster assignment process, comparing a naive approach with an optimized, collaborative approach leveraging shared memory, hash comparisons, and compaction, achieving significant speed and memory usage improvements. This approach reduced assignment time from 6ms to 1.1ms and memory usage from 3.1MB to 164KB when assigning 10,000 lights to 2,800 clusters.\n",
    "chinese_title": "27000条龙和10000盏灯：GPU驱动的集群化前向渲染器",
    "chinese_summary": "本文详细介绍了一种高性能、GPU驱动的集群正向渲染器，能够在GTX 1070上以1080p分辨率和60fps以上的帧率渲染27,000个斯坦福龙模型和10,000个光源。其关键创新在于将渲染管线的大部分移至GPU，并使用间接多重绘制来最大限度地减少CPU绘制调用。\n\n渲染器将对象数据存储在GPU上的“对象缓冲区”中，并使用由基于GPU的剔除过程填充的“绘制缓冲区”，通过仅绘制可见对象来提高性能。计算着色器基于AABB执行视锥体剔除，并使用原子计数器和子群投票压缩绘制列表，以实现高效的内存使用和空间局部性。\n\n为了缓解过度绘制，渲染器实现了集群着色。视锥体被划分为集群，并根据其影响半径将光源分配给这些集群。这使得每个片段仅使用相关光源计算着色，从而减少了不必要的计算。本文解释了集群分配过程，将一种朴素方法与一种优化的、协作方法进行比较，该方法利用共享内存、哈希比较和压缩，从而显著提高了速度和内存使用率。在将10,000个光源分配给2,800个集群时，这种方法将分配时间从6毫秒缩短到1.1毫秒，并将内存使用量从3.1MB减少到164KB。"
  },
  {
    "id": "44042490",
    "title": "Show HN: A Tiling Window Manager for Windows, Written in Janet",
    "url": "https://agent-kilo.github.io/jwno/",
    "summary": "Jwno is a new, highly customizable tiling window manager for Windows 10 and 11 built using the Janet programming language. It aims to bring the flexibility and power of tiling window management to the Windows environment.\n\nKey features and information highlighted in the announcement include:\n\n*   **Tiling Functionality:** Jwno allows users to automatically arrange application windows in a tiled layout, improving workflow and screen organization.\n*   **Customizability:** It is \"highly customizable,\" suggesting users can tailor its behavior to their specific needs.\n*   **Janet Language:** Built with Janet, which is noted as using parenthesis and providing a REPL for interaction.\n*   **UI Hint Feature:** Jwno offers a feature for interacting with UI elements, potentially offering advanced window control.\n*   **Work in Progress:** The documentation is a work-in-progress.\n*   **Resources Provided:** Links are offered for new users (features, installation, tutorial) and experienced users (cookbook, reference index, development guide), alongside download links, issue tracker, and source code repositories (GitHub and Chisel).\n",
    "chinese_title": "Show HN: 用 Janet 编写的 Windows 上的平铺窗口管理器",
    "chinese_summary": "Jwno是一款全新的、高度可定制的平铺式窗口管理器，适用于Windows 10和11，它使用Janet编程语言构建。其目标是将平铺式窗口管理的灵活性和强大功能带入Windows环境。\n\n公告中强调的关键特性和信息包括：\n\n*   **平铺功能：** Jwno允许用户自动以平铺布局排列应用程序窗口，从而改善工作流程和屏幕组织。\n*   **可定制性：** 它是“高度可定制的”，这意味着用户可以根据自己的特定需求定制其行为。\n*   **Janet语言：** 使用Janet构建，该语言以使用括号和提供REPL进行交互而著称。\n*   **UI提示功能：** Jwno提供一个与UI元素交互的功能，可能提供高级窗口控制。\n*   **开发中：** 文档仍在开发中。\n*   **提供的资源：** 为新用户（功能、安装、教程）和有经验的用户（食谱、参考索引、开发指南）提供了链接，以及下载链接、问题跟踪器和源代码存储库（GitHub和Chisel）。"
  },
  {
    "id": "44043323",
    "title": "Robin: A multi-agent system for automating scientific discovery",
    "url": "https://arxiv.org/abs/2505.13400",
    "summary": "This arXiv article introduces Robin, a novel multi-agent system designed to automate the entire scientific discovery process. Robin integrates literature search and data analysis agents to autonomously generate hypotheses, design experiments, interpret results, and refine hypotheses in an iterative \"lab-in-the-loop\" framework.\n\nThe system's capabilities are demonstrated through the discovery and validation of a novel therapeutic candidate for dry age-related macular degeneration (dAMD). Robin identified enhancing retinal pigment epithelium phagocytosis as a potential strategy and subsequently pinpointed ripasudil, a clinically-used ROCK inhibitor, as a promising treatment. Crucially, ripasudil had never been previously considered for dAMD.\n\nFurther, Robin designed and analyzed an RNA-seq experiment to understand ripasudil's mechanism of action, revealing the upregulation of ABCA1, a lipid efflux pump, as a potential novel target. The authors emphasize that all hypotheses, experimental plans, data analyses, and figures presented in the report were generated by Robin itself.\n\nThe authors claim that Robin represents a paradigm shift in AI-driven scientific discovery, as the first AI system to autonomously discover and validate a novel therapeutic candidate within a complete iterative loop. The paper is categorized under Artificial Intelligence, Multiagent Systems, and Quantitative Methods in Biology.\n",
    "chinese_title": "罗宾：用于自动化科学发现的多智能体系统",
    "chinese_summary": "Robin：一种用于自动科学发现的新型多智能体系统"
  },
  {
    "id": "44043421",
    "title": "Show HN: Juvio – UV Kernel for Jupyter",
    "url": "https://github.com/OKUA1/juvio",
    "summary": "Juvio is a Jupyter kernel designed to make notebooks more reproducible, dependency-aware, and Git-friendly. It achieves this through three key features: inline dependency management, automatic environment setup, and a Git-friendly script-style format.\n\nUsers can install Python packages directly from the notebook using the `%juvio install` magic command. These dependencies are then stored as metadata within the notebook itself, adhering to the PEP 723 standard.\n\nWhen a Juvio notebook is opened, the kernel automatically creates an ephemeral virtual environment and installs the specified dependencies using `uv`, ensuring that the notebook runs with the correct package versions.\n\nFurthermore, Juvio converts notebooks to a script-like format using `# %%` markers, making it easier to track changes and manage versions in Git. This avoids the messy diffs often associated with traditional Jupyter notebooks.\n\nTo use Juvio, users need to install the `juvio` package and the associated JupyterLab extension, as well as ensure `uv` is installed. The key benefits of using Juvio are streamlined dependency management without the need for separate lock files, guaranteed reproducibility of notebook environments, and cleaner Git diffs for better version control. It leverages `uv` for fast package management and the PEP 723 standard for inline dependencies.\n",
    "chinese_title": "Show HN: Juvio – Jupyter的UV内核",
    "chinese_summary": "Juvio 是一款 Jupyter 内核，旨在提高 notebook 的可复现性、依赖感知能力和 Git 友好性。它通过三个关键特性实现这一目标：内联依赖管理、自动环境设置和 Git 友好的脚本式格式。\n\n用户可以使用 `%juvio install` 魔术命令直接从 notebook 中安装 Python 包。这些依赖项随后会作为元数据存储在 notebook 本身中，符合 PEP 723 标准。\n\n当打开 Juvio notebook 时，内核会自动创建一个临时的虚拟环境，并使用 `uv` 安装指定的依赖项，确保 notebook 以正确的包版本运行。\n\n此外，Juvio 使用 `# %%` 标记将 notebook 转换为类似脚本的格式，从而更容易在 Git 中跟踪更改和管理版本。 这避免了通常与传统 Jupyter notebook 相关的混乱差异。\n\n要使用 Juvio，用户需要安装 `juvio` 包和相关的 JupyterLab 扩展，并确保已安装 `uv`。 使用 Juvio 的主要优势包括：无需单独的锁文件即可简化依赖管理、保证 notebook 环境的可复现性，以及更清晰的 Git 差异以实现更好的版本控制。 它利用 `uv` 实现快速的包管理，并利用 PEP 723 标准实现内联依赖。"
  },
  {
    "id": "44043659",
    "title": "Ashby (YC W19) Is Hiring Engineering Managers",
    "url": "https://www.ashbyhq.com/careers?utm_source=hn&ashby_jid=933570bc-a3d6-4fcc-991d-dc399c53a58a",
    "summary": "Ashby, a Y Combinator (YC W19) alum, is hiring Engineering Managers. The company emphasizes a work environment where team members are encouraged to strive for excellence and do their best work daily. The notice serves as a call to action, inviting individuals to explore career opportunities with Ashby. The overall focus is on recruiting talent for their engineering team, specifically leadership roles.\n",
    "chinese_title": "Ashby (YC W19) 招聘工程经理",
    "chinese_summary": "Ashby (YC W19校友) 招聘工程经理。该公司强调一个鼓励团队成员追求卓越并每天尽最大努力工作的环境。此公告旨在号召大家，邀请有志之士探索Ashby的职业机会。整体侧重于为其工程团队招聘人才，特别是领导职位。"
  },
  {
    "id": "44043034",
    "title": "The Fractured Entangled Representation Hypothesis",
    "url": "https://github.com/akarshkumar0101/fer",
    "summary": "This paper introduces the \"Fractured Entangled Representation Hypothesis\" (FER), challenging the assumption that improved performance in neural networks necessarily equates to better internal representations. The authors compare neural networks evolved through open-ended search (Picbreeder) to those trained via Stochastic Gradient Descent (SGD) on the simple task of generating a single image.\n\nTheir key finding is that while both types of networks achieve similar output performance, their internal representations differ significantly. SGD-trained networks exhibit FER, a disorganized representation where individual neurons' functions are complex and entangled. In contrast, evolved networks tend towards a Unified Factored Representation (UFR), where neurons have more distinct and interpretable functions.\n\nThe authors visualize each neuron's functionality as an image, revealing the stark differences in internal representation between the two approaches. They argue that FER in large models may hinder crucial capabilities like generalization, creativity, and continual learning. Thus, understanding and mitigating FER could be vital for advancing representation learning.\n\nThe paper provides supplementary data, including intermediate feature maps and weight sweeps, and makes the code available for replicating the experiments. The code allows users to load Picbreeder genomes, train SGD networks to mimic their output, visualize internal representations, and perform weight sweeps. The authors conclude by offering access to more Picbreeder genomes for research purposes.\n",
    "chinese_title": "破碎纠缠表征假设",
    "chinese_summary": "本文介绍“断裂纠缠表征假说”(FER)，挑战了神经网络性能提升必然等同于更好内部表征的假设。作者将通过开放式搜索（Picbreeder）进化的神经网络与通过随机梯度下降(SGD)训练来生成单一图像的神经网络进行比较。\n\n他们的主要发现是，虽然这两种类型的网络实现了相似的输出性能，但它们的内部表征却大相径庭。SGD训练的网络表现出FER，一种无序的表征，其中单个神经元的功能复杂且纠缠。相比之下，进化的网络倾向于统一分解表征(UFR)，其中神经元具有更清晰且可解释的功能。\n\n作者将每个神经元的功能可视化为图像，揭示了两种方法之间内部表征的显著差异。他们认为，大型模型中的FER可能会阻碍泛化、创造力和持续学习等关键能力。因此，理解和缓解FER对于推进表征学习至关重要。\n\n该论文提供了补充数据，包括中间特征图和权重扫描，并提供代码以供复制实验。该代码允许用户加载Picbreeder基因组，训练SGD网络来模仿其输出，可视化内部表征，并执行权重扫描。作者最后提供更多Picbreeder基因组以供研究之用。"
  },
  {
    "id": "44042070",
    "title": "OpenAI Codex Review",
    "url": "https://zackproser.com/blog/openai-codex-review",
    "summary": "This review explores OpenAI's Codex, focusing on its GitHub-connected chat interface for automating code tasks. The author highlights Codex's potential to support a multi-threaded workflow where multiple tasks can be initiated in parallel, aligning with their preferred work style. They envision a future \"untethered workflow\" where Codex enables work away from the desk, facilitated by its mobile accessibility. The ability to follow up on tasks via chat, easily create pull requests, and monitor task logs are also praised.\n\nHowever, the review points out limitations. Error handling is poor, and code quality is inconsistent, with only a 40-60% success rate. Codex currently struggles with complex refactors and lacks smooth iteration on existing branches, as it prefers creating new pull requests. A significant drawback is the lack of network connectivity within its execution sandboxes, preventing tasks like dependency updates.\n\nDespite these issues, the author is optimistic. While Codex hasn't unlocked \"insane productivity gains\" yet, they believe it will once error handling and code quality improve, multi-turn updates on branches are streamlined, and network connectivity is added. They see Codex currently being best suited for minor maintenance tasks and small updates, while larger projects still require a traditional IDE. The author anticipates Codex evolving into a central orchestration layer, handling low-priority tasks and keeping track of overall progress.\n",
    "chinese_title": "OpenAI Codex 评测",
    "chinese_summary": "本次评测探讨了OpenAI的Codex，重点关注其与GitHub连接的聊天界面，用于自动化代码任务。作者强调了Codex支持多线程工作流的潜力，即可以并行启动多个任务，这与他们偏好的工作方式相符。他们设想了一个未来的“无束缚工作流”，Codex可以支持远离办公桌的工作，这得益于其移动可访问性。通过聊天跟进任务、轻松创建拉取请求以及监控任务日志的能力也受到了赞扬。\n\n然而，评测指出了其局限性。错误处理较差，代码质量不稳定，成功率仅为40-60%。Codex目前在复杂的重构方面表现不佳，并且缺乏对现有分支的流畅迭代，因为它更倾向于创建新的拉取请求。一个明显的缺点是其执行沙箱中缺乏网络连接，从而阻止了诸如依赖项更新之类的任务。\n\n尽管存在这些问题，作者仍然持乐观态度。虽然Codex尚未实现“疯狂的生产力提升”，但他们相信，一旦错误处理和代码质量得到改善，分支上的多轮更新得到简化，并且增加了网络连接，情况将会改观。他们认为Codex目前最适合执行小型维护任务和小型更新，而大型项目仍然需要传统的IDE。作者预计Codex将演变为一个中央协调层，处理低优先级任务并跟踪整体进度。"
  },
  {
    "id": "44039864",
    "title": "The emoji problem (2022)",
    "url": "https://artofproblemsolving.com/community/c2532359h2760821_the_emoji_problem__part_i?srsltid=AfmBOor9TbMq_A7hGHSJGfoWaa2HNzducSYZu35d_LFlCSNLXpvt-pdS",
    "summary": "The provided text snippet is incomplete and doesn't offer any actual content about \"The emoji problem (2022)\". It appears to be a broken or partially loaded webpage from the Art of Problem Solving (AoPS) community, specifically a blog post within the \"Turtle Math\" section.\n\nThe main takeaway from the text is simply that the webpage *didn't load correctly*. It suggests the user \"Click to refresh\" the page. There's no information about what \"The emoji problem (2022)\" actually is, what its mathematical implications are, or what discussion surrounds it. We can only infer that it's a topic discussed on the AoPS blog \"Turtle Math.\"\n\nTherefore, a summary is impossible given the lack of content. The only actionable information is that the page requires a refresh.\n",
    "chinese_title": "表情符号问题 (2022)",
    "chinese_summary": "提供的文本片段不完整，并未提供关于“表情符号问题 (2022)”的任何实际内容。它似乎是来自 Art of Problem Solving (AoPS) 社区的损坏或部分加载的网页，具体是“Turtle Math”部分中的一篇博客文章。\n\n从文本中得出的主要结论是网页*未正确加载*。它建议用户“点击刷新”页面。没有关于“表情符号问题 (2022)”实际上是什么、它的数学含义是什么，或者围绕它的讨论是什么的信息。我们只能推断它是AoPS博客“Turtle Math”上讨论的一个话题。\n\n因此，鉴于缺乏内容，不可能进行总结。唯一可操作的信息是页面需要刷新。"
  },
  {
    "id": "44041515",
    "title": "The Lisp in the Cellar: Dependent types that live upstairs [pdf]",
    "url": "https://zenodo.org/records/15424968",
    "summary": "This paper introduces Deputy, a Clojure-hosted dependently-typed programming language designed to explore the integration of interactive, REPL-driven development with dependent type systems. Dependent types allow code to compute types, which can depend on values, enabling powerful programming patterns. Traditionally, type-checking is a purely compile-time operation.\n\nDeputy leverages Clojure's dynamic environment to allow interactive development, even during type checking. By implementing Deputy as a Clojure library, the host language remains accessible while programming at the type level. This offers a unique development experience, allowing programmers to explore and refine both program logic and type-level computations within the REPL.\n\nThe paper highlights the potential of Lisp's interactive workflow to enhance the process of developing dependently-typed programs, typically associated with more rigid and static development environments. Deputy features inductive datatypes and serves as an experimental platform for investigating this novel approach to dependently-typed programming. The associated PDF file, \"deputy-els.pdf,\" contains the full details of the system.\n",
    "chinese_title": "地下室里的Lisp：楼上住着的依赖类型 [pdf]",
    "chinese_summary": "本文介绍了Deputy，一种基于Clojure的依赖类型编程语言，旨在探索交互式、REPL驱动的开发与依赖类型系统的集成。依赖类型允许代码计算类型，这些类型可以依赖于值，从而实现强大的编程模式。传统上，类型检查是一种纯粹的编译时操作。\n\nDeputy利用Clojure的动态环境来实现交互式开发，即使在类型检查期间也是如此。通过将Deputy实现为Clojure库，宿主语言在类型级别编程时仍然可用。这提供了一种独特的开发体验，允许程序员在REPL中探索和改进程序逻辑和类型级计算。\n\n本文强调了Lisp的交互式工作流程在增强依赖类型程序开发过程中的潜力，而依赖类型程序通常与更严格和静态的开发环境相关联。Deputy具有归纳数据类型，并作为研究这种新型依赖类型编程方法的实验平台。相关PDF文件“deputy-els.pdf”包含系统的完整详细信息。"
  },
  {
    "id": "44043687",
    "title": "The Dawn of Nvidia's Technology",
    "url": "https://blog.dshr.org/2025/05/the-dawn-of-nvidias-technology.html",
    "summary": "This article, written by someone involved in Nvidia's early history, reflects on two new books about the company's rise to prominence and offers insight into the technical innovations of its early years. The author focuses on Nvidia's imaging model (using quadric patches instead of triangles) and its I/O architecture.\n\nRegarding the imaging model, Nvidia initially used quadric patches, which required less data transfer across the PCI bus than triangles, giving them an advantage when running complex graphics like Sega's arcade games on PCs. However, the author foresaw the eventual dominance of Microsoft's DirectX, which relied on triangles, and left Nvidia in hopes of prompting a shift to the more widely-supported technology.\n\nThe article emphasizes the importance of Nvidia's I/O architecture, driven by the team's experience at Sun Microsystems with Unix and virtual memory systems. This architecture included a \"virtualized objects\" system, with a software-based resource manager that allowed for flexibility in hardware feature implementation and accelerated innovation. Key components were a FIFO queue, Direct Memory Access (DMA), and an I/O Memory Management Unit (IOMMU) that enabled applications to directly access the I/O device in a virtual memory environment. This architecture addressed issues with context switching in multi-process operating systems, providing each process the illusion of exclusive access to the graphics device and was \"future-proof\", designed to support the eventual arrival of virtual memory systems to the PC.\n",
    "chinese_title": "英伟达技术的曙光",
    "chinese_summary": "这篇由一位参与英伟达早期历史的人士撰写的文章，回顾了两本关于该公司崛起的新书，并深入探讨了其早期年代的技术创新。作者重点介绍了英伟达的图像模型（使用二次曲面片代替三角形）及其I/O架构。\n\n关于图像模型，英伟达最初使用二次曲面片，与三角形相比，这需要更少的数据通过PCI总线传输，这使得他们在PC上运行像世嘉街机游戏这样的复杂图形时具有优势。然而，作者预见到微软DirectX最终将占据主导地位，而DirectX依赖于三角形，因此离开了英伟达，希望能促使其转向更广泛支持的技术。\n\n文章强调了英伟达I/O架构的重要性，这得益于该团队在Sun Microsystems使用Unix和虚拟内存系统的经验。这种架构包括一个“虚拟化对象”系统，带有一个基于软件的资源管理器，该管理器允许在硬件功能实现方面具有灵活性并加速创新。关键组件包括FIFO队列、直接内存访问(DMA)和I/O内存管理单元(IOMMU)，后者使应用程序能够在虚拟内存环境中直接访问I/O设备。这种架构解决了多进程操作系统中上下文切换的问题，为每个进程提供了独占访问图形设备的错觉，并且是“面向未来”的，旨在支持虚拟内存系统最终登陆PC。"
  },
  {
    "id": "44042343",
    "title": "Show HN: Astra – a new js2exe compiler",
    "url": "https://github.com/astracompiler/cli",
    "summary": "Astra is presented as a fast, reliable, and easy-to-use JavaScript-to-EXE compiler aiming to compile server and CLI applications for Windows (macOS and Linux support in progress). It differentiates itself from existing tools like `pkg` and `nexe` with a new compilation approach resulting in relatively smaller executable sizes (70-80MB on average).\n\nKey features include:\n\n*   **Fast Build Time:** Leverages esbuild for rapid compilation.\n*   **ESM Support:** Handles ECMAScript modules, addressing limitations in Node.js SEA.\n*   **Good Developer Experience:** Employs tools like `signale`, `inquirer`, and `chalk` for a pleasant user experience.\n*   **Standalone Executables:** Generates single .exe files containing all dependencies.\n*   **Metadata Customization:** Allows modification of executable metadata like icons, names, and versions.\n\nThe compilation process involves linting and bundling code with esbuild, injecting a generated blob into a Node.exe binary, editing metadata, and using postject to create the final executable. Astra is MIT licensed, and contributions are welcomed. Installation instructions via npm, yarn, and pnpm are provided, along with a basic usage example and a pointer to the help command.\n",
    "chinese_title": "Show HN: Astra – 一款新的js2exe编译器",
    "chinese_summary": "Astra 是一款快速、可靠且易于使用的 JavaScript 到 EXE 编译器，旨在为 Windows 编译服务器和 CLI 应用程序（macOS 和 Linux 支持正在开发中）。它采用一种新的编译方法，有别于现有的 `pkg` 和 `nexe` 等工具，从而产生相对较小的可执行文件大小（平均 70-80MB）。\n\n主要特点包括：\n\n*   **快速构建时间：** 利用 esbuild 进行快速编译。\n*   **ESM 支持：** 处理 ECMAScript 模块，解决 Node.js SEA 中的限制。\n*   **良好的开发者体验：** 采用 `signale`、`inquirer` 和 `chalk` 等工具，带来愉悦的用户体验。\n*   **独立可执行文件：** 生成包含所有依赖项的单个 .exe 文件。\n*   **元数据自定义：** 允许修改可执行文件的元数据，如图标、名称和版本。\n\n编译过程包括使用 esbuild 进行代码 Linting 和打包，将生成的 Blob 注入到 Node.exe 二进制文件中，编辑元数据，并使用 postject 创建最终的可执行文件。 Astra 采用 MIT 许可证，欢迎贡献。 提供了通过 npm、yarn 和 pnpm 安装的说明，以及一个基本的使用示例和一个指向帮助命令的指针。"
  },
  {
    "id": "44039744",
    "title": "A simple search engine from scratch",
    "url": "https://bernsteinbear.com/blog/simple-search/",
    "summary": "This article details the creation of a simple search engine from scratch, leveraging word embeddings and cosine similarity for ranking search results. The author, Max Bernstein, explains how he and Chris Gregory built the engine using the word2vec model to map words into a 300-dimensional space, where each dimension represents a semantic axis. Blog posts are embedded by summing the embeddings of their constituent words, and searches are ranked by the cosine similarity between the search query's embedding and the post embeddings.\n\nThe article walks through the process, including loading pre-trained word embeddings, embedding words and documents, and implementing the cosine similarity calculation. It also shows how to build a basic search REPL using Python's `code` library. To make the search engine accessible via the web, the author describes how to split the word2vec data into separate JSON files for embeddings and indices, enabling efficient HTTP Range requests to download only the necessary embedding chunks.\n\nFinally, the article discusses an evaluation methodology, defining a top-k accuracy metric to compare the search engine's performance against a baseline approach that simply counts keyword occurrences. This involves creating an evaluation dataset of (document, query) pairs and measuring the percentage of times the target document appears in the top-k search results for a given query.\n",
    "chinese_title": "从零开始的简易搜索引擎",
    "chinese_summary": "本文详细介绍了如何从头开始创建一个简单的搜索引擎，利用词嵌入和余弦相似度对搜索结果进行排序。作者 Max Bernstein 解释了他和 Chris Gregory 如何使用 word2vec 模型将单词映射到 300 维空间，其中每个维度代表一个语义轴。博客文章通过对构成它们的单词的嵌入求和来进行嵌入，搜索结果通过搜索查询的嵌入与文章嵌入之间的余弦相似度进行排序。\n\n本文逐步介绍了整个过程，包括加载预训练的词嵌入、嵌入单词和文档，以及实现余弦相似度计算。它还展示了如何使用 Python 的 `code` 库构建一个基本的搜索 REPL。为了使搜索引擎可以通过网络访问，作者描述了如何将 word2vec 数据拆分为单独的 JSON 文件，用于嵌入和索引，从而能够进行高效的 HTTP Range 请求，仅下载必要的嵌入块。\n\n最后，本文讨论了一种评估方法，定义了一个 top-k 准确率指标，用于将搜索引擎的性能与仅计算关键词出现次数的基线方法进行比较。这包括创建一个 (文档，查询) 对的评估数据集，并测量目标文档在给定查询的前 k 个搜索结果中出现的百分比。"
  },
  {
    "id": "44038209",
    "title": "Making Video Games (Without an Engine) in 2025",
    "url": "https://noelberry.ca/posts/making_games_in_2025/",
    "summary": "This is a very short piece of content, essentially a title and a brief statement.\n\n**Summary:**\n\nThe content is a welcome message to a website about making video games without using a pre-made game engine. It includes a title, \"Making Video Games (Without an Engine) in 2025\", indicating the focus of the content is on game development from scratch, potentially highlighting modern techniques applicable in 2025. A link to the source code is provided, suggesting a practical, hands-on approach to the topic.\n",
    "chinese_title": "2025年：不使用游戏引擎制作游戏",
    "chinese_summary": "2025年（无引擎）制作游戏"
  },
  {
    "id": "44013470",
    "title": "Google is quietly giving Amazon a leg up in digital book sales",
    "url": "https://www.washingtonpost.com/technology/2025/05/16/google-amazon-ebooks-apps/",
    "summary": "Unable to access the article link.\n",
    "chinese_title": "谷歌正在悄悄地帮助亚马逊提升数字图书销量。",
    "chinese_summary": "无法访问文章链接。"
  },
  {
    "id": "44043167",
    "title": "Teachable Machine",
    "url": "https://teachablemachine.withgoogle.com/",
    "summary": "The article \"Teachable Machine\" is a brief introduction to a platform with that name. Based solely on the title and the repeated title in the content, it's difficult to provide a detailed summary. However, we can infer the following:\n\nThe core subject is likely \"Teachable Machine.\" It is almost certainly a website or tool (hence \"machine\") designed to be user-friendly and facilitate learning about, or creating, machine learning models. The name implies it's geared towards making machine learning accessible and teachable to individuals with potentially limited technical expertise. We can assume it allows users to \"teach\" the \"machine\" through a simplified interface.\n\nWithout more information, we can only speculate that Teachable Machine likely allows users to train models using visual, audio, or other input methods. The \"machine\" would then learn patterns and be able to classify new data based on the training provided. It is likely a tool intended to democratize access to machine learning by simplifying the development and deployment of models.\n",
    "chinese_title": "Teachable Machine -> 可训练机器",
    "chinese_summary": "文章《Teachable Machine》是对同名平台的一个简要介绍。仅凭标题和内容中重复出现的标题，很难提供详细的总结。但是，我们可以推断出以下几点：\n\n核心主题很可能是“Teachable Machine”。它几乎可以肯定是一个网站或工具（因此称之为“机器”），旨在用户友好，并促进学习或创建机器学习模型。这个名字暗示它旨在使机器学习变得易于访问，并能够教授给可能技术专长有限的个人。我们可以推测，它允许用户通过简化的界面“教”这台“机器”。\n\n在没有更多信息的情况下，我们只能推测 Teachable Machine 很可能允许用户使用视觉、音频或其他输入方法来训练模型。“机器”随后将学习模式，并能够根据提供的训练对新数据进行分类。它很可能是一个旨在通过简化模型的开发和部署来普及机器学习访问的工具。"
  },
  {
    "id": "44043735",
    "title": "The Last Letter",
    "url": "https://aeon.co/essays/how-the-last-letters-of-the-condemned-can-teach-us-how-to-live",
    "summary": "Daniel R Brunstetter's essay \"The Last Letter\" explores the profound insights into life and death gleaned from the farewell letters of French resistance fighters and political hostages executed by the Nazis during World War II. Brunstetter recounts discovering a collection of these letters and the powerful, unique nature of their content, stemming from the authors' confrontation with imminent death.\n\nThe essay highlights the universal themes present in these deeply personal letters, reflecting on the human condition. Brunstetter references Montaigne's idea that studying death teaches us how to live and relates the letters to Elisabeth Kübler-Ross's five stages of grief, noting how these stages are often compressed in the writers' final moments. The condemned often bargain, reflecting on what they would do with more time, providing insights into their values.\n\nBrunstetter shares specific examples from the letters, showcasing the writers' expressions of love, regret, courage, and acceptance. He interweaves personal anecdotes, such as encountering the name of a resistance fighter, Georges Pitard, on a Parisian street sign, and his own confrontation with mortality while reading Daniel Decourdemanche's last letter.\n\nThe essay concludes with a reflection on the psychological difficulty of facing one's own mortality and how these letters offer a powerful, unfiltered glimpse into the emotions and priorities that surface when confronted with the end of life. The author finds that the letters ultimately reveal what is truly important, offering valuable lessons on how to live more meaningfully.\n",
    "chinese_title": "最后一封信",
    "chinese_summary": "丹尼尔·R·布伦斯特特的文章《最后的信》探讨了从二战期间被纳粹处决的法国抵抗战士和政治人质的诀别信中获得的对生与死的深刻见解。布伦斯特特讲述了他发现这些信件的经过，以及其内容的强大和独特性，这源于作者们对迫在眉睫的死亡的直面。\n\n这篇文章突出了这些高度私人的信件中存在的普遍主题，反映了人类的境况。布伦斯特特引用了蒙田的观点，即研究死亡教会我们如何生活，并将这些信件与伊丽莎白·库布勒-罗斯的悲伤五阶段联系起来，并注意到这些阶段如何在作者们生命的最后时刻被压缩。被判刑者经常讨价还价，反思如果拥有更多时间他们会做什么，从而提供了对他们价值观的洞察。\n\n布伦斯特特分享了信件中的具体例子，展示了作者们对爱、遗憾、勇气和接受的表达。他穿插了个人轶事，例如在巴黎的街道标志上遇到抵抗战士乔治·皮塔德的名字，以及他在阅读丹尼尔·德库德曼切的最后一封信时自己对死亡的直面。\n\n文章最后反思了面对自身死亡的心理困境，以及这些信件如何提供了一个强大而未经滤镜的视角，让我们得以窥见在生命尽头所浮现的情感和优先事项。作者发现，这些信件最终揭示了什么才是真正重要的，为我们如何更有意义地生活提供了宝贵的教训。"
  },
  {
    "id": "44040883",
    "title": "llm-d, Kubernetes native distributed inference",
    "url": "https://llm-d.ai/blog/llm-d-announce",
    "summary": "LLM-d is a Kubernetes-native framework designed to optimize distributed LLM inference, offering a well-defined path for serving at scale with competitive performance and fast time-to-value. It addresses the shortcomings of standard scale-out approaches for LLM inference, which struggles with expensive, non-uniform requests and the potential for performance gains through specialized routing and replica coordination.\n\nThe article highlights key advantages of llm-d: leveraging prefix caching for faster multi-turn requests, disaggregating prefill and decode phases for efficient resource utilization, and accommodating diverse quality of service (QoS) requirements.\n\nLLM-d's architecture is built on vLLM, Kubernetes, and Inference Gateway (IGW), incorporating key contributions like a vLLM optimized Inference Scheduler for smart load balancing, disaggregated serving with vLLM using NVIDIA's NIXL, and disaggregated prefix caching using the vLLM KV connector API. The framework also aims for variant autoscaling over hardware, workload, and traffic to optimize resource utilization.\n\nInitial experiments demonstrate significant performance improvements with prefix-aware routing, including reduced TTFT and increased QPS while meeting SLO requirements. The article encourages AI engineers and researchers to join the llm-d community, explore the GitHub repository, join the developer Slack, and try out the quick starts to deploy llm-d.\n",
    "chinese_title": "llm-d，Kubernetes原生分布式推理",
    "chinese_summary": "LLM-d：用于优化分布式LLM推理的Kubernetes原生框架，提供了一条以具有竞争力的性能和快速价值实现大规模服务的清晰路径。它解决了标准LLM推理横向扩展方法的不足，该方法难以应对昂贵、不均匀的请求，以及通过专门路由和副本协调获得潜在性能提升。\n\n文章强调了llm-d的关键优势：利用前缀缓存加速多轮请求，分离预填充和解码阶段以实现高效的资源利用，并适应多样化的服务质量 (QoS) 要求。\n\nLLM-d的架构基于vLLM、Kubernetes和推理网关（IGW），整合了关键贡献，例如用于智能负载均衡的vLLM优化推理调度器、使用NVIDIA的NIXL与vLLM的分离服务，以及使用vLLM KV连接器API的分离前缀缓存。该框架还旨在通过硬件、工作负载和流量进行变体自动缩放，以优化资源利用率。\n\n初步实验表明，前缀感知路由显著提高了性能，包括缩短了TTFT并提高了QPS，同时满足了SLO要求。文章鼓励AI工程师和研究人员加入llm-d社区，探索GitHub存储库，加入开发者Slack，并尝试快速入门来部署llm-d。"
  },
  {
    "id": "44017560",
    "title": "Compiling OCaml to the TI-84 CE Calculator",
    "url": "https://farlow.dev/2025/05/17/ocaml-on-calculator",
    "summary": "This article details the author's project to compile OCaml code to run on a TI-84+ CE calculator.  The motivation stems from the author's interest in both OCaml, a functional programming language, and calculator programming.  Existing calculator toolchains lack native OCaml support, leading to this project.\n\nThe approach leverages Js_of_ocaml, a tool that usually compiles OCaml bytecode to JavaScript. The author created a new backend for Js_of_ocaml that emits C code instead, aiming for high portability and small code size suitable for the calculator's limited resources (256k RAM). Key to this conversion was managing the OCaml runtime, including garbage collection.\n\nTo achieve portable garbage collection, the author replaced local variables with explicit reads/writes to a global stack, enabling the garbage collector to scan for live objects. A mark-and-sweep garbage collection strategy is employed whenever memory allocation limits are approached.\n\nThe project also includes a minimal C runtime and a small TI-84+ CE library to enable OCaml programs to interact with the calculator's screen. The result is seamless integration with OCaml's build system (dune) and LSP support, demonstrated by a spinning cube program example.\n\nThe author acknowledges current limitations (e.g., no floats or exceptions) but highlights the potential for future extensions using tools like wee for broader platform support. The generated C code for a simple Fibonacci program is provided as an example, showcasing the runtime and generated OCaml code. The project's source code is publicly available.\n",
    "chinese_title": "将OCaml编译到TI-84 CE计算器",
    "chinese_summary": "本文详细介绍了作者将 OCaml 代码编译并在 TI-84+ CE 计算器上运行的项目。其动机源于作者对函数式编程语言 OCaml 和计算器编程的兴趣。由于现有计算器工具链缺乏原生 OCaml 支持，因此促成了这个项目。\n\n该方法利用了 Js_of_ocaml，这个工具通常将 OCaml 字节码编译成 JavaScript。作者为 Js_of_ocaml 创建了一个新的后端，生成 C 代码，旨在实现高可移植性和小代码尺寸，以适应计算器有限的资源（256k RAM）。这种转换的关键是管理 OCaml 运行时，包括垃圾回收。\n\n为了实现可移植的垃圾回收，作者用对全局堆栈的显式读/写操作替换了局部变量，使垃圾回收器能够扫描活动对象。每当内存分配接近极限时，就会采用标记清除垃圾回收策略。\n\n该项目还包括一个最小的 C 运行时和一个小的 TI-84+ CE 库，使 OCaml 程序能够与计算器的屏幕交互。最终实现了与 OCaml 构建系统 (dune) 和 LSP 支持的无缝集成，并以旋转立方体程序示例进行了演示。\n\n作者承认目前的局限性（例如，没有浮点数或异常），但强调了未来使用像 wee 这样的工具进行扩展，以获得更广泛的平台支持的潜力。提供了一个简单的 Fibonacci 程序的生成 C 代码作为示例，展示了运行时和生成的 OCaml 代码。该项目的源代码是公开可用的。"
  },
  {
    "id": "44036647",
    "title": "DDoSecrets publishes 410 GB of heap dumps, hacked from TeleMessage",
    "url": "https://micahflee.com/ddosecrets-publishes-410-gb-of-heap-dumps-hacked-from-telemessages-archive-server/",
    "summary": "DDoSecrets has published 410 GB of data hacked from TeleMessage, an Israeli firm that archives messages from modified versions of encrypted messaging apps like Signal and WhatsApp. The data, containing sensitive PII, is being shared only with journalists and researchers.\n\nThe article outlines a timeline of events leading to the data leak. It started with Mike Waltz, then-national security advisor, using a modified version of Signal by TeleMessage, leading to scrutiny and the publication of the TM SGNL source code. TeleMessage was subsequently hacked multiple times. The author previously revealed a vulnerability in the TeleMessage server, allowing anyone to download Java heap dumps containing plaintext chat logs.\n\nThe DDoSecrets release includes thousands of heap dumps from May 4, 2025, containing plaintext messages and metadata, including sender/recipient information, timestamps, and group names. The archived data stems from TeleMessage's use by figures like Mike Waltz and within the federal government. DDoSecrets has extracted the text from the heap dumps to aid research.\n\nThe author, Micah Lee, is a member of the DDoSecrets collective and is currently analyzing the data. The article ends with a call for donations to DDoSecrets.\n",
    "chinese_title": "DDoSecrets 公布了从 TeleMessage 黑客攻击中获得的 410 GB 堆转储数据。",
    "chinese_summary": "DDoSecrets 发布了从 TeleMessage 窃取的 410 GB 数据，TeleMessage 是一家以色列公司，负责归档来自 Signal 和 WhatsApp 等加密消息应用程序修改版本的信息。 这些数据包含敏感的 PII（个人身份信息），目前仅与记者和研究人员共享。\n\n文章概述了导致数据泄露的事件时间线。 起始于时任国家安全顾问 Mike Waltz 使用 TeleMessage 修改版的 Signal，引发审查并导致 TM SGNL 源代码的发布。 随后，TeleMessage 多次遭到黑客攻击。 作者此前曾披露 TeleMessage 服务器中的一个漏洞，允许任何人下载包含明文聊天记录的 Java 堆转储。\n\nDDoSecrets 发布的数据包括 2025 年 5 月 4 日的数千个堆转储，其中包含明文消息和元数据，包括发送者/接收者信息、时间戳和群组名称。 存档数据源于 Mike Waltz 等人和联邦政府内部对 TeleMessage 的使用。 DDoSecrets 已从堆转储中提取文本，以帮助研究。\n\n作者 Micah Lee 是 DDoSecrets 集体成员，目前正在分析数据。 文章最后呼吁向 DDoSecrets 捐款。"
  },
  {
    "id": "44035158",
    "title": "Have I Been Pwned 2.0",
    "url": "https://www.troyhunt.com/have-i-been-pwned-2-0-is-now-live/",
    "summary": "Okay, here's a summary of Troy Hunt's \"Have I Been Pwned 2.0 is Now Live\" article, based on what I know from general knowledge and likely content given the title and author:\n\nThe article announces the significant upgrade to Have I Been Pwned (HIBP), referred to as HIBP 2.0. The core functionality of the service remains the same: allowing users to check if their email address or username has been compromised in known data breaches. However, the underlying infrastructure and codebase have been completely rewritten for improved performance, scalability, and security.\n\nKey improvements in HIBP 2.0 likely include:\n\n*   **Enhanced Performance and Scalability:** The new architecture is designed to handle significantly larger volumes of data and user requests, ensuring faster response times even during peak usage. This is crucial as HIBP continues to grow and incorporate more breaches.\n\n*   **Improved Security:** The rewritten code likely incorporates the latest security best practices to better protect the integrity of the database and the privacy of user searches. It may include better protection against attacks like SQL injection and cross-site scripting.\n\n*   **New Features or Enhancements to Existing Features:** While not explicitly stated in the prompt, HIBP 2.0 likely incorporates improvements to existing functionalities or new features. This could include enhanced API functionality, more detailed breach information, or improved search capabilities.\n\n*   **Modernized Technology Stack:** The update would likely involve a transition to more modern and efficient technologies, improving maintainability and future development.\n\nThe overall goal of HIBP 2.0 is to ensure the service remains a reliable and performant resource for internet users to proactively monitor their online security and take appropriate action if their data has been compromised. The author, Troy Hunt, likely emphasizes the importance of data breach awareness and encourages users to utilize the updated service regularly.\n",
    "chinese_title": "我被黑了吗 2.0",
    "chinese_summary": "好的，以下是根据我对 Troy Hunt 的 “Have I Been Pwned 2.0 is Now Live” 文章的总结，基于我的常识和标题及作者的推测：\n\n文章宣布了 Have I Been Pwned (HIBP) 的重大升级，称为 HIBP 2.0。该服务的核心功能保持不变：允许用户检查他们的电子邮件地址或用户名是否在已知的数据泄露中被泄露。然而，底层的基础设施和代码库已被完全重写，以提高性能、可扩展性和安全性。\n\nHIBP 2.0 的主要改进可能包括：\n\n*   **增强的性能和可扩展性：** 新的架构旨在处理更大的数据量和用户请求，即使在高峰使用期间也能确保更快的响应时间。这对于 HIBP 的持续增长和纳入更多泄露事件至关重要。\n\n*   **改进的安全性：** 重写的代码可能包含了最新的安全最佳实践，以更好地保护数据库的完整性和用户搜索的隐私。它可能包括更好的保护，防止 SQL 注入和跨站脚本等攻击。\n\n*   **新功能或现有功能的增强：** 虽然提示中没有明确说明，但 HIBP 2.0 很可能包含了对现有功能的改进或新功能。这可能包括增强的 API 功能、更详细的泄露信息或改进的搜索功能。\n\n*   **现代化的技术栈：** 这次更新可能涉及过渡到更现代、更高效的技术，从而提高可维护性和未来的开发能力。\n\nHIBP 2.0 的总体目标是确保该服务仍然是互联网用户主动监控其在线安全并在数据泄露时采取适当行动的可靠且高性能的资源。作者 Troy Hunt 可能会强调数据泄露意识的重要性，并鼓励用户定期使用更新后的服务。"
  },
  {
    "id": "44038279",
    "title": "Hypervisor as a Library",
    "url": "https://seiya.me/blog/hypervisor-as-a-library",
    "summary": "This article introduces the concept of a \"hypervisor as a library,\" using the author's Starina OS as an example, to achieve Linux compatibility.  Instead of the traditional approach of running a hypervisor as a separate process, Starina offers its hypervisor as a Rust library, `starina_linux::Command`, allowing direct interaction with the Linux guest from within Starina applications.\n\nThe author draws a parallel to Rust's `std::process::Command`, making the integration process familiar and intuitive. This allows developers to launch Linux binaries within a lightweight VM, using familiar methods for managing stdin/stdout and environment variables. The core idea is to encapsulate the hypervisor functionalities (creating a guest-physical address space, loading the Linux kernel, handling memory-mapped I/O) within a library API.\n\nKey benefits include greater flexibility and potentially improved performance due to reduced inter-process communication. It also allows seamless integration with Starina, enabling direct and safe interaction with the guest OS, even allowing access to guest memory.\n\nThe author acknowledges the potential drawbacks of virtual machines being perceived as slow but believes with optimization, the approach can rival native Linux performance with techniques like VM snapshotting. He envisions a container-like experience where developers can specify container images, use VM state caching, and expose ports directly through the Starina OS, ultimately unlocking the power of Linux device drivers.\n",
    "chinese_title": "作为库的虚拟机监控器",
    "chinese_summary": "本文以作者的Starina OS为例，介绍了“库形式的虚拟机监控器”的概念，以实现Linux兼容性。 Starina没有采用将虚拟机监控器作为单独进程运行的传统方法，而是将其虚拟机监控器作为Rust库`starina_linux::Command`提供，从而允许从Starina应用程序内部直接与Linux客户机进行交互。\n\n作者将其与Rust的`std::process::Command`进行类比，使集成过程既熟悉又直观。 这使得开发人员可以在轻量级VM中启动Linux二进制文件，并使用熟悉的方法来管理stdin/stdout和环境变量。 核心思想是将虚拟机监控器的功能（创建客户机物理地址空间、加载Linux内核、处理内存映射I/O）封装在库API中。\n\n主要优点包括更高的灵活性和潜在的性能提升，因为减少了进程间通信。 它还允许与Starina无缝集成，从而实现与客户机OS的直接且安全的交互，甚至允许访问客户机内存。\n\n作者承认虚拟机可能被认为很慢的潜在缺点，但相信通过优化，这种方法可以通过VM快照等技术与原生Linux性能相媲美。 他设想了一种类似容器的体验，开发人员可以在其中指定容器镜像、使用VM状态缓存并通过Starina OS直接公开端口，最终释放Linux设备驱动程序的强大功能。"
  },
  {
    "id": "44013071",
    "title": "Production tests: a guidebook for better systems and more sleep",
    "url": "https://martincapodici.com/2025/05/13/production-tests-a-guidebook-for-better-systems-and-more-sleep/",
    "summary": "This article advocates for the implementation of production tests (also known as synthetics) to enhance system reliability, improve deployments, and aid in observability. Production tests are automated tests run directly on the production environment, typically every minute, simulating user actions or backend service interactions.\n\nThe benefits include early detection of regressions, allowing for proactive fixes before customer impact. Production tests can also serve as a canary for deployments, acting as integration tests to catch mismatches with other services. They aid in debugging by providing insights into working and failing components, and improve incident recovery by providing quicker awareness and more information for resolution.\n\nKey design considerations include simplicity and focus, avoiding overly complex or functionality-specific tests that lead to false alerts. While aiming for reasonable coverage, prioritize testing critical functionalities with higher user impact. While distinct from health checks, production tests can leverage them for early warnings. The impact on logs, metrics, and traces needs to be considered, along with strategies for managing fake data generated by the tests. A \"three strikes\" approach before triggering alarms helps mitigate false alerts due to transient issues.\n\nThe advantages of production tests include real-world testing conditions, enhanced quality control, troubleshooting support, observability in low-traffic regions, safer deployments, and potential reuse in other environments. Disadvantages include the challenges of setup, teardown, scenario creation, potential flakiness, and resource consumption.\n",
    "chinese_title": "生产测试：改善系统，睡个好觉",
    "chinese_summary": "本文倡导实施生产环境测试（也称为合成测试），以提高系统可靠性，改进部署并辅助可观测性。生产环境测试是直接在生产环境中运行的自动化测试，通常每分钟运行一次，模拟用户行为或后端服务交互。\n\n其优势包括尽早检测回归问题，以便在影响客户之前主动修复。生产环境测试还可以作为部署的金丝雀，充当集成测试来捕捉与其他服务的不匹配。它们通过提供对正常工作和故障组件的洞察力来帮助调试，并通过提供更快的感知和更多信息来解决问题，从而改善事件恢复。\n\n关键的设计考虑因素包括简单性和专注性，避免过于复杂或特定于功能的测试，以免导致虚假警报。在追求合理覆盖范围的同时，优先测试具有较高用户影响的关键功能。虽然与健康检查不同，但生产环境测试可以利用它们来发出早期警告。需要考虑对日志、指标和追踪的影响，以及管理测试生成的虚假数据的策略。在触发警报之前采用“三次罢工”方法有助于缓解由瞬态问题引起的虚假警报。\n\n生产环境测试的优点包括真实世界的测试条件、增强的质量控制、故障排除支持、低流量地区的可观测性、更安全的部署以及在其他环境中潜在的重用。缺点包括设置、清理、场景创建、潜在的不稳定性和资源消耗等挑战。"
  },
  {
    "id": "44043518",
    "title": "Show HN:  Olelo Foil - NACA Airfoil Sim",
    "url": "https://foil.olelohonua.com/",
    "summary": "This \"Show HN\" post introduces Olelo Foil, a NACA airfoil simulator. The main point is to showcase a tool that allows users to simulate and visualize the behavior of NACA airfoils. While the content simply mentions \"Development,\" it can be inferred that the creator is sharing their work in progress or a finished product. Likely, Olelo Foil lets users input parameters defining a NACA airfoil (e.g., NACA 4412) and then simulates its performance, potentially visualizing things like pressure distribution, lift, and drag coefficients. The post encourages engagement from the Hacker News community, likely seeking feedback, bug reports, or suggestions for future development. The limited information makes it difficult to specify features, but the core purpose is clear: to provide a practical tool for learning about and simulating NACA airfoils.\n",
    "chinese_title": "显示 HN：Olelo Foil - NACA 翼型模拟器",
    "chinese_summary": "这个“Show HN”帖子介绍了 Olelo Foil，一个 NACA 翼型模拟器。主要目的是展示一个允许用户模拟和可视化 NACA 翼型行为的工具。虽然内容只简单提到“开发”，但可以推断出创建者是在分享他们的在研作品或已完成的产品。Olelo Foil 很有可能允许用户输入定义 NACA 翼型的参数（例如，NACA 4412），然后模拟其性能，并可能可视化压力分布、升力系数和阻力系数等。该帖子鼓励 Hacker News 社区参与，很可能是在寻求反馈、错误报告或对未来开发的建议。由于信息有限，很难详细说明功能，但核心目的是明确的：提供一个实用的工具，用于学习和模拟 NACA 翼型。"
  },
  {
    "id": "44040419",
    "title": "Show HN: Text to 3D simulation on a map (does history pretty well)",
    "url": "https://mused.com/map/",
    "summary": "This \"Show HN\" post introduces a map simulation platform that leverages text input to generate 3D simulations on a map. The key takeaway is its ability to simulate historical events based on text descriptions. This suggests the platform interprets textual data describing places, times, and actions, and then translates that information into a visually engaging 3D representation layered onto a map. The platform is positioned as a tool for spatial intelligence, implying its use for analyzing, understanding, and visualizing geographic data and events. While the post is brief, it hints at a novel approach to historical visualization and a potentially powerful tool for various applications involving spatial data analysis. In essence, it transforms textual narratives into interactive 3D map simulations, with historical events being a key showcase.\n",
    "chinese_title": "Show HN：地图上的文本转3D模拟（历史重现效果不错）",
    "chinese_summary": "此“Show HN”帖子介绍了一个地图模拟平台，该平台利用文本输入在地图上生成3D模拟。其关键在于它能够根据文本描述模拟历史事件。这意味着该平台能够解释描述地点、时间和动作的文本数据，然后将这些信息转化为视觉上引人入胜的3D表示，并叠加到地图上。该平台定位为一种空间智能工具，暗示其可用于分析、理解和可视化地理数据和事件。虽然帖子简短，但它暗示了一种新颖的历史可视化方法，并且可能成为各种涉及空间数据分析的应用程序的强大工具。本质上，它将文本叙事转化为交互式3D地图模拟，其中历史事件是一个关键展示。"
  },
  {
    "id": "44034918",
    "title": "Jules: An Asynchronous Coding Agent",
    "url": "https://jules.google/",
    "summary": "Jules is an asynchronous coding agent designed to handle coding tasks users prefer not to do, freeing up their time for more engaging work. It assists with bug fixing, version bumps, writing tests, fixing existing code, and building new features.\n\nThe process involves several steps: first, the user selects a GitHub repository and branch and provides a detailed prompt for Jules. In the future, users will be able to assign tasks directly through GitHub issues. Jules then fetches and clones the repository to a cloud VM, develops a plan using the Gemini 2.5 Pro model, and presents this plan to the user for approval. After approval, Jules provides a diff of the proposed changes, allowing users to quickly review and approve the code edits. Finally, Jules creates a pull request with the changes, which the user can then approve, merge, and publish to GitHub. Jules also creates an audio summary of the changes to help users quickly understand the updates.\n",
    "chinese_title": "朱尔斯：一个异步编码代理",
    "chinese_summary": "Jules是一个异步编码代理，旨在处理用户不愿执行的编码任务，从而释放他们的时间来从事更具吸引力的工作。它可以协助修复bug、升级版本、编写测试、修复现有代码以及构建新功能。\n\n该过程包括几个步骤：首先，用户选择一个GitHub存储库和分支，并为Jules提供详细的提示。未来，用户将能够直接通过GitHub issue分配任务。然后，Jules获取并将存储库克隆到云VM，使用Gemini 2.5 Pro模型制定计划，并将该计划呈现给用户以供批准。获得批准后，Jules提供建议更改的差异，允许用户快速审查和批准代码编辑。最后，Jules创建一个包含更改的pull request，用户可以批准、合并并发布到GitHub。Jules还会创建一个更改的音频摘要，以帮助用户快速了解更新。"
  },
  {
    "id": "44038835",
    "title": "Finland announces migration of its rail network to international gauge",
    "url": "https://www.trenvista.net/en/news/rnhs/finland-migration-standard-gauge/",
    "summary": "Finland has announced a plan to convert its entire rail network from the Russian gauge to the European standard gauge, a decision driven by improved military mobility, regional security concerns following its NATO membership, and growing tensions with Russia. Transport Minister Lulu Ranne unveiled the plan, emphasizing the need to remove obstacles for transporting troops and goods between Finland, Sweden, and Norway.\n\nThe project, slated to begin near Oulu in the north, is expected to cost billions of euros and impact over 9,200 km of track, taking decades to complete. The Finnish government anticipates making a final decision by July 2027, with construction starting around 2032. The country is relying on significant European Union funding, potentially covering up to 50% of planning costs and 30% of construction expenses.\n\nThis massive infrastructure project represents a significant geopolitical and strategic shift for Finland, aligning its rail network with European standards and solidifying its integration with the EU and NATO. The conversion symbolizes a move away from its historical ties with Russia and towards a stronger European identity.\n",
    "chinese_title": "芬兰宣布其铁路网络迁移至国际标准轨距",
    "chinese_summary": "芬兰宣布计划将其整个铁路网络从俄罗斯轨距改为欧洲标准轨距，此举是出于提高军事机动性、加入北约后对地区安全的担忧以及与俄罗斯日益紧张的关系的考虑。交通部长露露·兰内公布了该计划，强调有必要消除芬兰、瑞典和挪威之间运输部队和货物的障碍。\n\n该项目预计耗资数十亿欧元，将影响超过9200公里的轨道，并需要数十年才能完成，计划从北部奥卢附近开始。芬兰政府预计将在2027年7月之前做出最终决定，建设工作将于2032年左右开始。该国依赖欧盟的大量资金，可能覆盖高达50%的规划成本和30%的建设费用。\n\n这项大规模基础设施项目代表着芬兰重大的地缘政治和战略转变，使其铁路网络与欧洲标准保持一致，并巩固其与欧盟和北约的融合。此次改造象征着摆脱与俄罗斯的历史联系，走向更强大的欧洲认同。"
  },
  {
    "id": "44040301",
    "title": "Show HN: JavaFactory – IntelliJ plugin to generate Java code",
    "url": "https://github.com/JavaFactoryPluginDev/javafactory-plugin",
    "summary": "JavaFactory is an IntelliJ plugin that leverages LLMs to automate the generation of repetitive Java code, aiming for more predictable and stable results than traditional AI code generators. It works by defining reusable \"patterns\" based on natural language descriptions of tasks (e.g., test generation). These patterns consist of a System Prompt (defining the goal, rules, output format, and example) and a User Prompt (specifying the classes to include).\n\nA key feature is its Annotation-Based Reference Collection, using `@JavaFactoryData` and `@JavaFactoryApi` to precisely control which classes are used in code generation. `@JavaFactoryData` recursively collects classes, suitable for domain models, while `@JavaFactoryApi` collects only one level of referenced APIs, allowing specification of implementations, tests, and fixtures.\n\nJavaFactory is recommended for developers frustrated with unpredictable AI code generation or those working in structured environments with repeating patterns like layered architectures (e.g., generating DAO, Repository, and API implementations, tests, and fixtures). The idea is to manually design the core components and automate the repetitive, predictable parts using JavaFactory.\n",
    "chinese_title": "Show HN: JavaFactory – 用于生成Java代码的IntelliJ插件",
    "chinese_summary": "JavaFactory：利用LLM自动化生成重复性Java代码的IntelliJ插件。相较于传统AI代码生成器，它旨在提供更可预测和稳定的结果。其工作原理是基于任务的自然语言描述（例如，测试生成）定义可重用的“模式”。这些模式包含一个系统提示（定义目标、规则、输出格式和示例）和一个用户提示（指定要包含的类）。\n\n一个关键特性是其基于注解的引用收集，使用`@JavaFactoryData`和`@JavaFactoryApi`来精确控制代码生成中使用的类。`@JavaFactoryData`递归地收集类，适用于领域模型，而`@JavaFactoryApi`仅收集一层引用的API，允许指定实现、测试和fixtures。\n\n对于那些对不可预测的AI代码生成感到沮丧的开发者，或者在具有重复模式（如分层架构）的结构化环境中工作的开发者（例如，生成DAO、Repository和API的实现、测试和fixtures），推荐使用JavaFactory。其理念是手动设计核心组件，并使用JavaFactory自动化重复性的、可预测的部分。"
  },
  {
    "id": "44037941",
    "title": "I got fooled by AI-for-science hype–here's what it taught me",
    "url": "https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres",
    "summary": "Nick McGreivy, a physicist, shares his disappointing experience using AI to solve partial differential equations (PDEs) in plasma physics research. Initially optimistic about AI's potential to accelerate scientific progress, he found that AI methods, particularly Physics-Informed Neural Networks (PINNs), were brittle and often performed worse than advertised when compared fairly to state-of-the-art numerical methods.\n\nHe highlights that many papers claiming significant speedups with AI compared AI against weak baselines, leading to overoptimistic results. He also points out a significant reporting bias, where negative results and failures of AI methods are rarely published. This creates a skewed perception of AI's capabilities in science.\n\nMcGreivy's systematic review of AI applied to fluid mechanics PDEs revealed that a large majority of papers used weak baselines, leading to unfair comparisons. He argues that AI adoption in science is driven more by benefits for the scientists themselves (e.g., publications, funding) than by genuine advancements in scientific understanding.\n\nWhile acknowledging that AI can contribute to scientific breakthroughs in areas like weather forecasting and drug discovery, McGreivy expresses skepticism about its revolutionary potential. He believes that AI is more likely to be a tool for incremental progress rather than a game-changing force in science, and advocates for more realistic expectations, rigorous comparisons, and a greater focus on reliability and robustness of AI-based methods.\n",
    "chinese_title": "我被AI科学炒作忽悠了——这是它教会我的。",
    "chinese_summary": "尼克·麦格瑞维，一位物理学家，分享了他使用人工智能解决等离子体物理研究中偏微分方程(PDEs)的令人失望的经历。最初他对人工智能加速科学进步的潜力充满乐观，但他发现人工智能方法，尤其是物理信息神经网络(PINNs)，非常脆弱，并且在与最先进的数值方法进行公平比较时，表现往往比宣传的更差。\n\n他强调，许多声称人工智能能显著加速的论文，实际上是将人工智能与较弱的基线进行比较，从而导致过于乐观的结果。他还指出存在严重的报告偏差，即人工智能方法的负面结果和失败很少被发表。这造成了对人工智能在科学领域能力的扭曲认知。\n\n麦格瑞维对应用于流体力学偏微分方程的人工智能的系统性回顾表明，绝大多数论文都使用了较弱的基线，导致了不公平的比较。他认为，人工智能在科学领域的应用更多地是受到对科学家自身的好处（例如，发表论文、获得资金）的驱动，而不是真正推动科学理解的进步。\n\n虽然麦格瑞维承认人工智能可以在天气预报和药物发现等领域为科学突破做出贡献，但他对人工智能的革命性潜力表示怀疑。他认为，人工智能更有可能成为一种促进渐进式进步的工具，而不是科学领域改变游戏规则的力量，并倡导更现实的期望、严格的比较，以及更加关注基于人工智能的方法的可靠性和鲁棒性。"
  },
  {
    "id": "44031385",
    "title": "The Windows Subsystem for Linux is now open source",
    "url": "https://blogs.windows.com/windowsdeveloper/2025/05/19/the-windows-subsystem-for-linux-is-now-open-source/",
    "summary": "In May 2025, Microsoft announced that the Windows Subsystem for Linux (WSL) is now open source. After a multi-year effort, the code that powers WSL is available on GitHub at Microsoft/WSL. Users can now download WSL, build it from source, add fixes and features, and participate in its development.\n\nWSL is comprised of command-line executables (wsl.exe, wslconfig.exe, wslg.exe), the WSL service (wslservice.exe), Linux init and daemon processes (init, gns, localhost), and a file sharing implementation (plan9). This is an addition to already open-sourced components like microsoft/wslg (Wayland and X server support) and microsoft/WSL2-Linux-Kernel.\n\nWSL was initially introduced in 2016 based on a pico process provider (lxcore.sys), evolving into WSL 1. Later, WSL 2 was introduced in 2019, leveraging the Linux kernel for improved compatibility. In 2021, WSL was separated from the Windows codebase and released as a package in the Microsoft Store, starting with version 0.47.1. Full transition to the new WSL package concluded with Windows 11 24H2. WSL 2.0.0, introduced features like mirrored networking and DNS tunneling.\n\nThe open-sourcing is driven by the recognition of the community's significant contributions to WSL, even without source code access. Microsoft hopes that open-sourcing will further accelerate WSL's development and foster community engagement.\n",
    "chinese_title": "适用于 Linux 的 Windows 子系统现已开源",
    "chinese_summary": "微软于2025年5月宣布，适用于Linux的Windows子系统（WSL）现已开源。经过多年的努力，驱动WSL的代码现已在GitHub上的Microsoft/WSL提供。用户现在可以下载WSL，从源代码构建它，添加修复程序和功能，并参与其开发。\n\nWSL由命令行可执行文件（wsl.exe、wslconfig.exe、wslg.exe）、WSL服务（wslservice.exe）、Linux init和守护进程（init、gns、localhost）以及文件共享实现（plan9）组成。 这是对已经开源的组件（如microsoft/wslg (Wayland和X服务器支持) 和 microsoft/WSL2-Linux-Kernel）的补充。\n\nWSL最初于2016年推出，基于pico进程提供程序（lxcore.sys），发展成为WSL 1。 后来，WSL 2于2019年推出，利用Linux内核来提高兼容性。 2021年，WSL与Windows代码库分离，并作为Microsoft Store中的软件包发布，从0.47.1版本开始。 向新的WSL软件包的完全过渡随着Windows 11 24H2完成。 WSL 2.0.0引入了镜像网络和DNS隧道等功能。\n\n开源的驱动力在于认识到社区对WSL的重大贡献，即使在没有源代码访问权限的情况下也是如此。 微软希望开源能够进一步加速WSL的开发并促进社区参与。"
  },
  {
    "id": "44017956",
    "title": "RepoRoulette: Randomly sample repositories from GitHub",
    "url": "https://github.com/gojiplus/reporoulette",
    "summary": "RepoRoulette is a Python package designed to randomly sample repositories from GitHub for various research and learning purposes. It provides four distinct sampling methods:\n\n1.  **ID-Based Sampling:** Probes random GitHub repository IDs, offering true randomness but with a potentially low hit rate due to invalid IDs. Requires a GitHub token.\n2.  **Temporal Sampling:** Selects repositories updated during randomly chosen time periods within a defined date range. Also requires a GitHub token and enables filtering by characteristics like minimum stars and languages.\n3.  **BigQuery Sampling:** Leverages Google BigQuery's public GitHub dataset for advanced filtering and large-scale sampling. Requires a Google Cloud Platform project with the BigQuery API enabled and a service account. This method handles large sample sizes efficiently and circumvents GitHub API rate limits, but incurs cloud costs.\n4.  **GH Archive Sampler:** This method samples repositories by fetching events from GitHub Archive, which records the public GitHub timeline. It allows you to specify the number of repositories to sample, the number of days to sample from, and the event types to consider.\n\nRepoRoulette supports diverse use cases such as academic research, learning resource discovery, data science, trend analysis, and security research. The project is licensed under the MIT License, and contributions are welcome.\n",
    "chinese_title": "RepoRoulette：GitHub仓库随机抽样",
    "chinese_summary": "RepoRoulette 是一个 Python 包，旨在从 GitHub 上随机抽样仓库，用于各种研究和学习目的。它提供了四种不同的抽样方法：\n\n1.  **基于 ID 的抽样：** 探测随机的 GitHub 仓库 ID，提供真正的随机性，但由于无效 ID，命中率可能较低。需要 GitHub 令牌。\n2.  **时间抽样：** 选择在定义的日期范围内随机选择的时间段内更新的仓库。也需要 GitHub 令牌，并允许按最小星数和语言等特征进行过滤。\n3.  **BigQuery 抽样：** 利用 Google BigQuery 的公共 GitHub 数据集进行高级过滤和大规模抽样。需要启用 BigQuery API 的 Google Cloud Platform 项目和一个服务帐户。此方法能高效处理大型样本，并绕过 GitHub API 速率限制，但会产生云费用。\n4.  **GH Archive 抽样器：** 此方法通过从 GitHub Archive 获取事件来抽样仓库，GitHub Archive 记录了公开的 GitHub 时间线。它允许您指定要抽样的仓库数量、要从中抽样的天数以及要考虑的事件类型。\n\nRepoRoulette 支持多种用例，例如学术研究、学习资源发现、数据科学、趋势分析和安全研究。该项目已获得 MIT 许可证许可，欢迎贡献。"
  },
  {
    "id": "44042693",
    "title": "Show HN: I made an app to create personalized stories for children in 5 minutes",
    "url": "https://www.unlimitedtales.com",
    "summary": "This \"Show HN\" post introduces an app that allows users to create personalized stories for children in approximately 5 minutes. The app boasts a user-friendly process involving four steps: configuring character attributes, selecting a theme, language, and illustration style; generating a tailored story text; reviewing and curating illustrations for each page and the cover; and finally, downloading the story as a PDF.\n\nThe app is suitable for children of all ages, as the content is adjusted based on the main character's age. A variety of illustration styles are available, including anime, 3D, watercolor, and pixel art, with new styles added regularly.\n\nUsers can regenerate the entire story or individual illustrations if they aren't satisfied, but the story cannot be edited after completion. The app aims to provide a quick and easy way to create unique and engaging stories for children, accessible from any device with a browser. The post includes testimonials highlighting the positive experiences of users who have used the app to create stories for their children and nephews. A launch offer of 20% off the first story is included to attract new users.\n",
    "chinese_title": "Show HN: 我做了一个应用，5分钟就能为孩子创作个性化故事",
    "chinese_summary": "这个“Show HN”帖子介绍了一个应用程序，用户可以在大约 5 分钟内为儿童创建个性化故事。 该应用程序拥有一个用户友好的四步流程：配置角色属性，选择主题、语言和插图风格； 生成定制的故事文本； 审查并为每页和封面策划插图； 最后，将故事下载为 PDF。\n\n该应用程序适合所有年龄段的儿童，因为内容会根据主角的年龄进行调整。 提供各种插图风格，包括动漫、3D、水彩和像素艺术，并定期添加新的风格。\n\n如果用户不满意，可以重新生成整个故事或单个插图，但故事完成后无法编辑。 该应用程序旨在提供一种快速简便的方式，为儿童创建独特而引人入胜的故事，可从任何具有浏览器的设备访问。 该帖子包含用户评价，突出了用户使用该应用程序为他们的孩子和侄子创作故事的积极体验。 为了吸引新用户，帖子包含首次故事 8 折的优惠。"
  },
  {
    "id": "44036900",
    "title": "What are people doing? Live-ish estimates based on global population dynamics",
    "url": "https://humans.maxcomperatore.com/",
    "summary": "This article, titled \"What are people doing? Live-ish estimates based on global population dynamics,\" appears to be attempting to provide a live, or near-live, estimate of what the global population is doing based on birth and death rates.\n\nCurrently, the article is in a loading state. It displays the title \"WHAT THE HELL ARE PEOPLE DOING?\" and shows \"Loading Date...\" which suggests it's attempting to fetch the current date.\n\nThe core of the article is centered around displaying real-time population dynamics. It aims to show the \"Global Population\" along with estimated \"Births/sec,\" \"Deaths/sec,\" and \"Net Gain/sec.\" Currently, all these values are displayed as 0.0, indicating either a loading error or that the data hasn't been initialized.\n\nIt also displays the current \"Coordinated Universal Time (UTC)\" and the number of \"Live Viewers,\" which is also 0.\n\nFinally, it mentions \"Activity Breakdown\" without providing any further information. This suggests that the article, once loaded properly, intends to categorize or break down what people are doing globally, presumably based on inferred data or statistical models linked to population numbers.\n\nIn short, the article aims to give a real-time snapshot of global population changes, although at present, it seems to be struggling to load the relevant data.\n",
    "chinese_title": "人们在做什么？基于全球人口动态的实时估算",
    "chinese_summary": "这篇题为“人们在干什么？基于全球人口动态的近似实时估算”的文章，似乎试图根据出生率和死亡率提供全球人口活动的实时或接近实时的估算。\n\n目前，该文章正处于加载状态。它显示标题“人们到底在干什么？”并显示“正在加载日期…”，这表明它正在尝试获取当前日期。\n\n文章的核心是展示实时人口动态。它旨在显示“全球人口”以及估计的“每秒出生人数”、“每秒死亡人数”和“每秒净增人数”。目前，所有这些值都显示为 0.0，表明存在加载错误或数据尚未初始化。\n\n它还显示当前的“协调世界时 (UTC)”和“在线观看人数”，后者也为 0。\n\n最后，它提到了“活动分解”，但没有提供任何进一步的信息。这表明，一旦正确加载，该文章打算对全球人口的活动进行分类或分解，大概是基于与人口数量相关的推断数据或统计模型。\n\n简而言之，这篇文章旨在提供全球人口变化的实时快照，但目前看来，它正在努力加载相关数据。"
  },
  {
    "id": "44032777",
    "title": "Claude Code SDK",
    "url": "https://docs.anthropic.com/en/docs/claude-code/sdk",
    "summary": "The Claude Code SDK allows developers to programmatically integrate Claude Code into their applications, primarily through command-line usage. TypeScript and Python SDKs are planned. It supports both single prompts and multi-turn conversations, enabling the creation of AI-powered coding assistants.\n\nKey features include:\n\n*   **Basic Usage:** Running single prompts with options for plain text, JSON, and streaming JSON output.\n*   **Advanced Usage:** Resuming and continuing conversations, customizing Claude's behavior with system prompts, and extending functionality with Model Context Protocol (MCP) servers.\n*   **MCP Configuration:** Using external servers for capabilities like database access and API integrations (requires explicit tool allowance via `--allowedTools`).\n*   **CLI Options:** Various flags for controlling Claude's behavior, such as output format, session management, system prompts, and tool permissions.\n*   **Output Formats:** Text, JSON (with metadata like cost and session ID), and streaming JSON (for real-time message delivery).\n*   **Message Schema:** Strictly typed JSON messages for different event types, including assistant messages, user messages, and result messages.\n*   **Examples:** Simple script integrations, file processing, session management, and best practices for error handling and rate limiting.\n\nReal-world applications include the Claude Code GitHub Actions. Related resources include CLI usage documentation, GitHub Actions integration details, and tutorials.\n",
    "chinese_title": "Claude 代码 SDK",
    "chinese_summary": "Claude Code SDK 允许开发者以编程方式将 Claude Code 集成到他们的应用程序中，主要通过命令行使用。计划推出 TypeScript 和 Python SDK。它支持单轮提示和多轮对话，从而能够创建 AI 驱动的编码助手。\n\n主要功能包括：\n\n*   **基本用法：** 运行单轮提示，并提供纯文本、JSON 和流式 JSON 输出选项。\n*   **高级用法：** 恢复和继续对话，使用系统提示定制 Claude 的行为，以及使用模型上下文协议 (MCP) 服务器扩展功能。\n*   **MCP 配置：** 使用外部服务器实现数据库访问和 API 集成等功能（需要通过 `--allowedTools` 显式允许工具）。\n*   **CLI 选项：** 用于控制 Claude 行为的各种标志，例如输出格式、会话管理、系统提示和工具权限。\n*   **输出格式：** 文本、JSON（带有成本和会话 ID 等元数据）以及流式 JSON（用于实时消息传递）。\n*   **消息模式：** 用于不同事件类型的严格类型化 JSON 消息，包括助手消息、用户消息和结果消息。\n*   **示例：** 简单的脚本集成、文件处理、会话管理以及错误处理和速率限制的最佳实践。\n\n实际应用包括 Claude Code GitHub Actions。相关资源包括 CLI 用法文档、GitHub Actions 集成详细信息和教程。"
  },
  {
    "id": "44037426",
    "title": "Biff – a batteries-included web framework for Clojure",
    "url": "https://biffweb.com",
    "summary": "Biff is a batteries-included Clojure web framework designed to help solo developers rapidly build and deploy web applications without getting bogged down in complexity. It achieves this by curating and integrating various libraries and tools from the Clojure ecosystem into a cohesive, polished whole.\n\nKey features include:\n\n*   **XTDB Integration:** Enables immutability in the database with schema enforcement using Malli.\n*   **htmx for UI:** Allows creation of rich, interactive UIs on the backend, complemented by optional hyperscript for light client-side scripting.\n*   **Authentication:** Provides passwordless, email-based authentication, with support for magic links and one-time passcodes.\n*   **Deployment Ready:** Includes code for provisioning Ubuntu VPSs and deploying via Uberjar with Docker.\n*   **Live REPL:** Offers a live REPL that evaluates changes on file save and allows on-the-fly development, even in production.\n*   **Documentation:** Well-documented with a tutorial, reference docs, and a starter project.\n\nBiff emphasizes strong defaults that can be easily modified to adapt to evolving project needs, promoting flexibility and avoiding unnecessary constraints. In short, Biff aims to accelerate Clojure web development by providing a solid foundation and streamlined workflow.\n",
    "chinese_title": "Biff - 一个开箱即用的 Clojure Web 框架",
    "chinese_summary": "Biff：一个快速构建和部署 Web 应用的 Clojure 全栈框架\n\nBiff 是一个开箱即用的 Clojure Web 框架，旨在帮助独立开发者快速构建和部署 Web 应用，而无需陷入复杂的细节中。它通过整合来自 Clojure 生态系统的各种库和工具，将它们打造为一个有凝聚力、精致的整体来实现这一目标。\n\n主要功能包括：\n\n*   **XTDB 集成：** 使用 Malli 实现数据库的不可变性以及模式强制执行。\n*   **htmx 用于 UI：** 允许在后端创建丰富的交互式 UI，并可选使用 hyperscript 进行轻量级的客户端脚本编写。\n*   **身份验证：** 提供基于电子邮件的无密码身份验证，并支持魔法链接和一次性密码。\n*   **部署就绪：** 包含用于配置 Ubuntu VPS 并通过 Uberjar 和 Docker 进行部署的代码。\n*   **实时 REPL：** 提供一个实时 REPL，可以在文件保存时评估更改，并允许动态开发，即使在生产环境中也是如此。\n*   **文档：** 文档完善，包含教程、参考文档和一个入门项目。\n\nBiff 强调强大的默认设置，这些设置可以轻松修改以适应不断发展的项目需求，从而提高灵活性并避免不必要的约束。简而言之，Biff 旨在通过提供坚实的基础和简化的工作流程来加速 Clojure Web 开发。"
  },
  {
    "id": "44034459",
    "title": "Kilo: A text editor in less than 1000 LOC with syntax highlight and search",
    "url": "https://github.com/antirez/kilo",
    "summary": "Kilo is a small, BSD-licensed text editor written by Salvatore Sanfilippo (antirez) in under 1000 lines of code. It's designed as a simple, library-free editor, relying on standard VT100 escape sequences for terminal interaction. The editor is in its early stages but offers essential functionalities: opening files, saving with CTRL-S, quitting with CTRL-Q, and searching within a file using CTRL-F (with ESC to exit search and arrow keys for navigation). Kilo doesn't require any external libraries, including curses, making it highly portable and lightweight. Antirez encourages developers to use Kilo as a foundation for building more advanced editors or command-line interfaces, moving beyond the typical REPL style. A screencast demonstrating Kilo's functionality is available at the provided asciinema link.\n",
    "chinese_title": "Kilo：少于1000行代码，带语法高亮和搜索功能的文本编辑器",
    "chinese_summary": "Kilo 是一个小型文本编辑器，采用 BSD 许可证，由 Salvatore Sanfilippo (antirez) 编写，代码量少于 1000 行。它被设计为一个简单的、无库依赖的编辑器，依靠标准的 VT100 转义序列进行终端交互。该编辑器尚处于早期阶段，但提供了基本功能：打开文件、用 CTRL-S 保存、用 CTRL-Q 退出，以及用 CTRL-F 在文件中搜索（用 ESC 退出搜索，用方向键导航）。Kilo 不需要任何外部库，包括 curses，这使得它具有高度的可移植性和轻量性。Antirez 鼓励开发者使用 Kilo 作为基础，构建更高级的编辑器或命令行界面，超越典型的 REPL 风格。展示 Kilo 功能的屏幕录像可在提供的 asciinema 链接中找到。"
  },
  {
    "id": "44031535",
    "title": "Game theory illustrated by an animated cartoon game",
    "url": "https://ncase.me/trust/",
    "summary": "\"The Evolution of Trust\" is an interactive online game, presented as an animated cartoon, that uses game theory to illustrate the importance of trust and cooperation in society. The game explores the Prisoner's Dilemma, a classic game theory scenario where individual rationality leads to collective sub-optimality.\n\nThe game allows the player to interact with different characters programmed with various strategies, such as always cooperating, always betraying, randomly cooperating, or reciprocating (tit-for-tat). By playing multiple rounds against these characters, the player witnesses how certain strategies fare over time.\n\nKey takeaways from the game include:\n\n*   **Trust and cooperation are beneficial:** Characters who consistently cooperate tend to do better in the long run, especially in environments where trust can develop.\n*   **Tit-for-tat is a highly successful strategy:** This strategy starts by cooperating and then mirrors the opponent's previous move. It promotes cooperation while also punishing betrayal.\n*   **Mistrust can be destructive:** A single betrayal can trigger a cycle of retaliation and mistrust, leading to lower overall outcomes for everyone involved.\n*   **Clear communication and signals can foster trust:** Having a reputation for trustworthiness can encourage others to cooperate with you.\n*   **Noise and misunderstanding can erode trust:** Even in a generally cooperative environment, occasional accidental betrayals can disrupt trust and lead to conflict.\n\nUltimately, \"The Evolution of Trust\" demonstrates that while betrayal may offer short-term gains, sustained cooperation based on trust is the most effective path to mutual benefit and a thriving society. The game emphasizes the importance of forgiveness, clear communication, and the ability to learn from past interactions in building and maintaining trust.\n",
    "chinese_title": "动画卡通游戏讲解博弈论",
    "chinese_summary": "信任的进化：一款互动在线游戏，以动画卡通的形式呈现，运用博弈论来说明信任与合作在社会中的重要性。该游戏探索了囚徒困境，一个经典的博弈论场景，其中个体理性会导致集体次优。\n\n游戏中，玩家可以与程序预设了不同策略的角色互动，例如总是合作、总是背叛、随机合作或以牙还牙。通过与这些角色进行多轮游戏，玩家可以观察到随着时间的推移，哪些策略表现更好。\n\n游戏的主要启示包括：\n\n*   **信任与合作是有益的：** 一贯合作的角色往往在长期内表现更好，尤其是在可以发展信任的环境中。\n*   **以牙还牙是一种非常成功的策略：** 这种策略开始时选择合作，然后模仿对手的上一步行动。它促进合作，同时也惩罚背叛。\n*   **不信任可能具有破坏性：** 单次背叛可能会引发报复和不信任的循环，导致每个人的总体结果下降。\n*   **清晰的沟通和信号可以促进信任：** 拥有值得信赖的声誉可以鼓励他人与你合作。\n*   **噪音和误解会侵蚀信任：** 即使在普遍合作的环境中，偶尔发生的意外背叛也可能破坏信任并导致冲突。\n\n总而言之，《信任的进化》表明，虽然背叛可能带来短期收益，但基于信任的持续合作才是实现互利共赢和繁荣社会的最有效途径。 该游戏强调了宽恕、清晰沟通以及从过去的互动中学习的能力在建立和维持信任中的重要性。"
  },
  {
    "id": "44043564",
    "title": "Lyria 2",
    "url": "https://deepmind.google/technologies/lyria/",
    "summary": "Lyria 2 is Google's latest AI music generation model designed to produce high-fidelity, professional-grade audio across diverse genres. Developed with input from musicians and producers, it allows users to create music using text prompts while controlling key, BPM, and other musical elements. Lyria 2 aims to offer granular creative control, empowering musicians to shape intricate compositions and explore various styles from classical to electronic.\n\nKey features include producing 48kHz stereo audio, generating starting points, suggesting harmonies, drafting arrangements, and accelerating the composition process. It can also aid in discovering new styles and overcoming creative blocks.\n\nThe accompanying Music AI Sandbox, powered by Lyria 2, is a tool designed with music industry professionals to enable the creation of new instrumental or vocal parts, fostering exploration into unexplored musical territories. Collaborations with artists like Shankar Mahadevan showcase the potential of AI to enhance and expand creative possibilities.\n\nGoogle emphasizes the importance of responsible AI development, working with artists to address concerns and implement safety measures. Lyria 2 incorporates SynthID, a watermarking tool, to identify AI-generated content. While currently available to a limited number of trusted testers, a waitlist is open for others interested in participating. The team acknowledges limitations and ongoing efforts to refine the model's performance and expand its capabilities.\n",
    "chinese_title": "莱瑞亚 2",
    "chinese_summary": "Lyria 2：谷歌最新AI音乐生成模型"
  },
  {
    "id": "44013069",
    "title": "A man who visited every country in the world without boarding a plane (2023)",
    "url": "https://www.theguardian.com/travel/2023/aug/16/take-the-high-road-the-man-who-visited-every-country-in-the-world-without-boarding-a-plane",
    "summary": "Torbjørn Pedersen, driven by a childhood dream of adventure, embarked on a decade-long journey to visit every country in the world without flying. Frustrated by the feeling that all the great adventures had already happened, he discovered that no one had ever completed this feat without air travel in one continuous trip.\n\nSetting three rules – at least 24 hours in each country, no visits home, and absolutely no flying – Pedersen aimed to visit all 203 countries on a $20/day budget, funded by savings, donations, and sponsorship from the Danish Red Cross.\n\nHis journey was fraught with challenges. He endured cerebral malaria, being held at gunpoint, and numerous bureaucratic hurdles. The 'no flying home' rule caused him to miss his grandmother's funeral. He also faced strains in his relationship with his then-girlfriend, Le. However, their relationship strengthened, leading to a mountaintop proposal and eventual marriage. COVID-19 stranded him in Hong Kong for two years, delaying his return.\n\nDespite the hardships, Pedersen highlighted the kindness he encountered in every country and emphasized the similarities between people across cultures. He stressed the importance of relying on others and persevering even when faced with seemingly insurmountable obstacles. He arrived back in Denmark in May 2023 to a celebratory welcome, completing his extraordinary, flight-free, global journey. While happy he had done it, Pedersen spoke of feeling burnt out and how the trip had taken an emotional toll.\n",
    "chinese_title": "一个不搭飞机游遍世界各国的人 (2023)",
    "chinese_summary": "托比约恩·彼泽森，受童年冒险梦想驱使，踏上了一段为期十年的旅程，不乘坐飞机访问了世界上每个国家。他感到所有伟大的冒险都已经发生过了，这让他感到沮丧，因此他发现从未有人在一次连续的旅行中，不依靠航空旅行完成这一壮举。\n\n彼泽森设定了三条规则——在每个国家至少停留24小时，不回家，绝对不乘坐飞机——他的目标是以每天20美元的预算访问所有203个国家，资金来自储蓄、捐款和丹麦红十字会的赞助。\n\n他的旅程充满了挑战。他忍受了脑疟疾，被枪指着，以及无数的官僚障碍。“不乘飞机回家”的规则导致他错过了祖母的葬礼。他还与当时的伴侣Le的关系变得紧张。然而，他们的关系得到了加强，最终在山顶上求婚并结婚。新冠疫情将他困在香港两年，延误了他的归期。\n\n尽管困难重重，彼泽森强调了他在每个国家遇到的善良，并强调了不同文化背景的人们之间的相似之处。他强调了依靠他人以及即使面对看似无法克服的障碍也要坚持下去的重要性。他于2023年5月回到丹麦，受到热烈的欢迎，完成了他非凡的、无飞行的全球之旅。虽然很高兴他完成了这件事，但彼泽森表示感到筋疲力尽，以及这次旅行给他带来了情感上的伤害。"
  },
  {
    "id": "44042963",
    "title": "Microsoft-backed Builder.ai enters insolvency proceedings",
    "url": "https://techcrunch.com/2025/05/20/once-worth-over-1b-microsoft-backed-builder-ai-is-running-out-of-money/",
    "summary": "Builder.ai, an AI software company backed by Microsoft, is entering insolvency proceedings. The company, known for its platform designed to simplify app and website development, has raised over $450 million in funding. A company spokesperson confirmed that an administrator will be appointed to manage the company's affairs.\n\nThe company cites \"historic challenges and past decisions\" as the reason for its financial struggles, despite efforts from the current team. Builder.ai intends to support employees, customers, and partners during this transition and will explore options for the business where possible.\n\nRecent issues include a reported 25% revenue reduction estimate for the second half of 2024 and the appointment of a new CEO in February. Audits were initiated amid allegations of inflated sales figures by more than 20%. Previously, the company faced scrutiny for overstating the automation of its app development platform, which reportedly relied heavily on human engineers.\n",
    "chinese_title": "微软支持的Builder.ai进入破产程序",
    "chinese_summary": "得到的结果如下：\n\nBuilder.ai，一家由微软支持的人工智能软件公司，正在进入破产程序。这家以简化应用程序和网站开发平台而闻名的公司，已筹集超过4.5亿美元的资金。一位公司发言人证实，将任命一位管理人来管理公司的事务。\n\n公司将“历史挑战和过去的决策”归咎于其财务困境的原因，尽管现任团队做出了努力。Builder.ai 计划在此过渡期间为员工、客户和合作伙伴提供支持，并将尽可能探索业务的各种选择。\n\n最近的问题包括据报道的2024年下半年收入预计减少25%以及2月份任命了一位新首席执行官。由于有指控称销售额虚报超过20%，因此启动了审计。此前，该公司因夸大其应用程序开发平台的自动化程度而受到审查，据报道，该平台严重依赖人类工程师。"
  },
  {
    "id": "44038549",
    "title": "Questioning Representational Optimism in Deep Learning",
    "url": "https://github.com/akarshkumar0101/fer",
    "summary": "This paper challenges the \"representational optimism\" belief that better performance in deep learning models inherently equates to better internal representations. The authors introduce the concept of \"Fractured Entangled Representation (FER),\" a form of disorganized internal representation found in networks trained with stochastic gradient descent (SGD).\n\nThe study compares neural networks evolved through an open-ended search process (Picbreeder) to networks trained with SGD on the simple task of generating a single image (skull, butterfly, and apple). A key advantage of this minimal setup is the ability to visualize each hidden neuron's function as an image, revealing how the network's output is constructed.\n\nThe striking finding is that while both types of networks achieve similar output behavior, their internal representations are vastly different. SGD-trained networks exhibit FER, while evolved networks largely avoid it, approaching a more \"Unified Factored Representation (UFR).\" The authors hypothesize that FER in large models could negatively impact essential model capacities, including generalization, creativity, and continual learning.\n\nThe paper provides extensive supplementary data, including intermediate feature maps and weight sweeps, visualizing the differences in representations. Code is available on GitHub for loading Picbreeder genomes, training SGD networks to mimic their outputs, visualizing internal representations, and performing weight sweeps. The authors offer access to more Picbreeder genomes upon request. The paper argues that understanding and mitigating FER is crucial for advancing representation learning in deep learning.\n",
    "chinese_title": "深度学习中对表征乐观主义的质疑",
    "chinese_summary": "本文挑战了“表征乐观主义”的观点，即深度学习模型中更好的性能必然等同于更好的内部表征。作者引入了“破碎纠缠表征（FER）”的概念，这是一种在使用随机梯度下降（SGD）训练的网络中发现的无序内部表征形式。\n\n该研究将通过开放式搜索过程（Picbreeder）演化的神经网络与在生成单个图像（头骨、蝴蝶和苹果）的简单任务上使用 SGD 训练的网络进行了比较。这种最小设置的一个关键优势是能够将每个隐藏神经元的功能可视化为图像，从而揭示网络的输出是如何构建的。\n\n最引人注目的发现是，虽然两种类型的网络都实现了相似的输出行为，但它们的内部表征却大相径庭。SGD 训练的网络表现出 FER，而演化网络则在很大程度上避免了 FER，接近于更“统一分解表征（UFR）”。作者假设，大型模型中的 FER 可能会对重要的模型能力产生负面影响，包括泛化、创造力和持续学习。\n\n该论文提供了广泛的补充数据，包括中间特征图和权重扫描，以可视化表征的差异。GitHub 上提供了用于加载 Picbreeder 基因组、训练 SGD 网络以模仿其输出、可视化内部表征和执行权重扫描的代码。作者应要求提供更多 Picbreeder 基因组的访问权限。该论文认为，理解和缓解 FER 对于推进深度学习中的表征学习至关重要。"
  },
  {
    "id": "44040407",
    "title": "Why Does the U.S. Always Run a Trade Deficit?",
    "url": "https://libertystreeteconomics.newyorkfed.org/2025/05/why-does-the-u-s-always-run-a-trade-deficit/",
    "summary": "Thomas Klitgaard's article \"Why Does the U.S. Always Run a Trade Deficit?\" argues that the persistent U.S. trade deficit isn't just about exports lagging behind imports; it's a macroeconomic issue rooted in a shortfall of domestic saving relative to investment spending.\n\nThe article explains, using national accounting principles, that in a closed economy, investment equals saving. However, in an open economy like the U.S., a saving gap necessitates borrowing from abroad, resulting in net financial inflows. These inflows, reflecting foreign purchases of U.S. assets, compensate for the difference between exports and imports.\n\nData since 2000 reveals that the gap between U.S. saving and investment spending has been persistent, with fluctuations in household, business, and government saving often offsetting each other. While trade policies can influence the composition of trade, they only affect the trade deficit if they alter the saving-investment gap. The author demonstrates that even addressing specific import issues, like oil dependence, doesn't necessarily shrink the overall trade deficit.\n\nThe article counters the argument that trade deficits are inherently negative, suggesting that foreign borrowing can fuel investment and boost productive capacity. Ultimately, reducing the trade deficit requires a difficult recalibration of domestic saving and investment, potentially involving lower investment and higher saving, as seen during the 2008 financial crisis.\n",
    "chinese_title": "美国为何总是出现贸易逆差？",
    "chinese_summary": "托马斯·克利特加德的文章《为什么美国总是存在贸易逆差？》认为，美国持续的贸易逆差不仅仅是出口滞后于进口的问题，更是一个宏观经济问题，根源在于国内储蓄相对于投资支出的不足。\n\n文章利用国民经济核算原则解释说，在封闭经济中，投资等于储蓄。然而，在像美国这样的开放经济中，储蓄缺口需要从国外借款，从而导致净金融流入。这些流入反映了外国人购买美国资产的行为，弥补了出口和进口之间的差额。\n\n2000年以来的数据显示，美国储蓄和投资支出之间的缺口持续存在，家庭、企业和政府储蓄的波动常常相互抵消。虽然贸易政策可以影响贸易构成，但只有在改变储蓄-投资缺口时，它们才会影响贸易逆差。作者表明，即使解决特定的进口问题，例如石油依赖，也不一定能缩小整体贸易逆差。\n\n文章反驳了贸易逆差本质上是负面的观点，认为外国借款可以促进投资并提高生产能力。最终，减少贸易逆差需要对国内储蓄和投资进行艰难的重新调整，可能涉及降低投资和增加储蓄，正如2008年金融危机期间所见。"
  },
  {
    "id": "44037595",
    "title": "Ann, the Small Annotation Server",
    "url": "https://mccd.space/posts/design-pitch-ann/",
    "summary": "Ann is a minimal, decentralized social media server built on ActivityPub and focused on Web Annotations. It allows users to store, send, and receive annotations (comments, likes, recommendations) on any content, functioning as a foundation for various applications rather than a standalone social network.\n\nUnlike centralized social media, Ann is designed to be integrated with front-end applications, enabling social functionalities in diverse contexts. Potential applications include comment sections for blogs and Gemini browsers, private research collaboration platforms, recommendation engines, web browser plugins for viewing and adding comments on any webpage, AI training datasets of annotated content, and plugins for note-taking apps like Logseq and Obsidian.\n\nThe vision is to enable social features in existing applications like LibreOffice and video players, competing with centralized solutions like Notion and YouTube. Instead of each app needing to develop its own comment server, they can integrate with Ann, giving users control over their data and content consumption while maintaining privacy, and avoiding the need for JavaScript-heavy web apps. Ann essentially aims to decentralize and embed annotation capabilities across a wide range of applications.\n",
    "chinese_title": "Ann，小型注释服务器",
    "chinese_summary": "Ann：一个基于 ActivityPub 的极简、去中心化社交媒体服务器，专注于 Web 注释。它允许用户存储、发送和接收关于任何内容的注释（评论、点赞、推荐），作为各种应用的基础，而非一个独立的社交网络。\n\n与中心化社交媒体不同，Ann 旨在与前端应用集成，从而在各种场景中启用社交功能。潜在应用包括博客和 Gemini 浏览器的评论区、私有研究协作平台、推荐引擎、用于在任何网页上查看和添加评论的 Web 浏览器插件、带注释内容的 AI 训练数据集，以及 Logseq 和 Obsidian 等笔记应用插件。\n\n愿景是在 LibreOffice 和视频播放器等现有应用中启用社交功能，与 Notion 和 YouTube 等中心化解决方案竞争。无需每个应用都开发自己的评论服务器，它们可以与 Ann 集成，让用户控制自己的数据和内容消费，同时保持隐私，并避免对 JavaScript 重型 Web 应用的需求。 Ann 本质上旨在去中心化并将注释功能嵌入到各种应用程序中。"
  },
  {
    "id": "44031387",
    "title": "xAI's Grok 3 comes to Microsoft Azure",
    "url": "https://techcrunch.com/2025/05/19/xais-grok-3-comes-to-microsoft-azure/",
    "summary": "Microsoft is now offering managed access to xAI's Grok AI models (specifically Grok 3 and Grok 3 mini) through its Azure AI Foundry platform, becoming one of the first major cloud providers to do so. This means Azure customers can directly access and be billed for Grok models with the service-level agreements expected from Microsoft products.\n\nWhile xAI previously marketed Grok as an edgy, less-filtered AI, the Grok 3 models available on Azure AI Foundry are more controlled than the versions used on Elon Musk's social network, X. The article highlights past controversies surrounding Grok on X, including instances of biased responses and problematic content generation.\n\nThe Azure AI Foundry versions of Grok come with additional features such as data integration, customization options, and governance capabilities not necessarily provided through xAI's API. The move gives enterprise customers a way to use the power of Grok within a more structured and compliant environment.\n",
    "chinese_title": "xAI 的 Grok 3 登陆微软 Azure",
    "chinese_summary": "微软现通过Azure AI Foundry平台提供对xAI Grok AI模型（特别是Grok 3和Grok 3 mini）的托管访问，成为首批提供此服务的主要云提供商之一。这意味着Azure客户可以直接访问Grok模型并为其付费，并享有微软产品所期望的服务级别协议。\n\n尽管xAI之前将Grok宣传为前卫、较少过滤的AI，但Azure AI Foundry上提供的Grok 3模型比埃隆·马斯克社交网络X上使用的版本更加受控。文章强调了过去Grok在X上引发的争议，包括出现偏见的回应和有问题的文本生成实例。\n\nAzure AI Foundry版本的Grok还提供额外功能，例如数据集成、定制选项和治理能力，这些功能不一定通过xAI的API提供。此举为企业客户提供了一种在更结构化和合规的环境中使用Grok强大功能的方式。"
  },
  {
    "id": "44033310",
    "title": "The forbidden railway: Vienna-Pyongyang (2008)",
    "url": "http://vienna-pyongyang.blogspot.com/2008/04/how-everything-began.html",
    "summary": "Helmut, an Austrian railway enthusiast and employee of the Austrian Federal Railways, details the origins of his ambitious plan to travel to North Korea by train, specifically via the rarely used Tumangan border crossing. His fascination with trains and travel led him to discover the Moscow-Pyongyang train route in a 1998 timetable.\n\nOver the years, Helmut gained travel experience, including Trans-Siberian trips and journeys across Eurasia, fueling his interest in North Korea's isolated communist society. He was particularly intrigued by the Tumangan route, served by a North Korean sleeping car only twice a month.\n\nDespite travel agencies claiming the route was closed to tourists, Helmut's research, including a fateful usenet post and information from a German railway forum, suggested otherwise. He even took a domestic trip within Russia in the North Korean sleeping car to Tumangan, buying a ticket only to the first station in North Korea, proving the route's continued existence.\n\nThe aim became to travel across the border. Helmut and a friend, Oliver, decided to book a standard trip to North Korea via Sinujiu but secretly attempt to enter via Tumangan, contingent on the visa including Tumangan as a point of entry. Upon receiving his visa from the North Korean embassy in Vienna, which indeed listed Tumangan, Helmut concluded that he and Oliver were coming.\n",
    "chinese_title": "禁忌铁路：维也纳-平壤 (2008)",
    "chinese_summary": "作为一名奥地利铁路爱好者和奥地利联邦铁路的员工，赫尔穆特详细介绍了他的雄心勃勃的火车旅行至朝鲜计划的起源，特别是通过鲜为人知的图们江边境口岸。他对火车和旅行的热爱使他在1998年的时刻表中发现了莫斯科-平壤的火车线路。\n\n多年来，赫尔穆特积累了旅行经验，包括西伯利亚铁路之旅和横跨欧亚大陆的旅行，这激发了他对朝鲜孤立的共产主义社会的兴趣。他尤其对图们江线路感兴趣，这条线路每月只有两次由朝鲜卧铺车提供服务。\n\n尽管旅行社声称该线路对游客关闭，但赫尔穆特的研究，包括一篇命运攸关的Usenet帖子和一个德国铁路论坛的信息，表明情况并非如此。他甚至在俄罗斯境内乘坐朝鲜卧铺车前往图们江，购买了仅到达朝鲜境内第一站的车票，证明了该线路的持续存在。\n\n最终目标是穿越边境。赫尔穆特和他的朋友奥利弗决定预订前往朝鲜新义州的普通行程，但秘密尝试通过图们江入境，前提是签证包含图们江作为入境点。当他收到维也纳的朝鲜大使馆发放的签证，上面确实列出了图们江时，赫尔穆特断定他和奥利弗即将成行。"
  },
  {
    "id": "44001696",
    "title": "Remarks on AI from NZ",
    "url": "https://nealstephenson.substack.com/p/remarks-on-ai-from-nz",
    "summary": "Unable to access the article link.\n",
    "chinese_title": "新西兰人工智能杂谈",
    "chinese_summary": "无法访问文章链接。"
  },
  {
    "id": "44026959",
    "title": "Linguists Find Proof of Sweeping Language Pattern Once Deemed a 'Hoax'",
    "url": "https://www.scientificamerican.com/article/linguists-find-proof-of-sweeping-language-pattern-once-deemed-a-hoax/",
    "summary": "This article discusses a new study that re-examines the idea of linguistic relativity, specifically the claim that some languages have a disproportionate number of words for certain concepts, reflecting cultural priorities. The study, published in PNAS, analyzed bilingual dictionaries across over 600 languages, focusing on \"lexical elaboration,\" or the proportion of words dedicated to a specific concept.\n\nThe researchers confirmed the Inuit language Inuktitut's emphasis on snow, while also discovering other languages exhibit similar patterns, like Samoan's focus on lava or Scots' emphasis on oatmeal. This suggests a potential link between language and culture, reflecting what a society values.\n\nWhile the original \"Inuit snow words\" claim was previously debunked as a hoax, this study takes a computational approach to identify and quantify such lexical elaborations. The findings support a more nuanced view of linguistic relativity: while language may not *determine* how we perceive the world, it can subtly *influence* it.\n\nThe article also acknowledges limitations of the study, including reliance on potentially biased dictionaries and the inherent comparison back to English as the base language. The authors suggest future research should focus on analyzing real-world language use through textual and social media analysis.\n",
    "chinese_title": "语言学家发现曾被认为是“骗局”的普遍语言模式的证据",
    "chinese_summary": "本文探讨了一项重新审视语言相对论的新研究，特别是关于某些语言拥有不成比例的特定概念词汇，反映文化优先性的论断。该研究发表于PNAS，分析了超过600种语言的双语词典，重点关注“词汇精细化”，即专门用于特定概念的词汇比例。\n\n研究人员证实了因纽特语对雪的重视，同时也发现其他语言也表现出类似的模式，例如萨摩亚语对熔岩的关注，或苏格兰语对燕麦粥的强调。这表明语言和文化之间可能存在联系，反映了一个社会的价值观。\n\n虽然最初的“因纽特语雪词”论断之前已被揭穿为骗局，但这项研究采用了一种计算方法来识别和量化这种词汇精细化现象。研究结果支持对语言相对论的更细致入微的看法：虽然语言可能不会*决定*我们如何感知世界，但它可以微妙地*影响*它。\n\n文章还承认了该研究的局限性，包括依赖于可能存在偏见的词典以及将英语作为基础语言进行固有比较。作者建议未来的研究应侧重于通过文本和社交媒体分析来分析现实世界中的语言使用情况。"
  },
  {
    "id": "44010046",
    "title": "Solving physics-based initial value problems with unsupervised machine learning",
    "url": "https://link.aps.org/doi/10.1103/PhysRevE.111.055302",
    "summary": "Unable to access the article link.\n",
    "chinese_title": "使用无监督机器学习求解基于物理的初值问题",
    "chinese_summary": "无法访问文章链接。"
  },
  {
    "id": "44035770",
    "title": "Terraform MCP Server",
    "url": "https://github.com/hashicorp/terraform-mcp-server",
    "summary": "The Terraform MCP Server is a Model Context Protocol (MCP) server designed to enhance Terraform development by integrating with Terraform Registry APIs. It enables automation and advanced interaction with Infrastructure as Code (IaC). Key use cases include automating provider/module discovery, extracting registry data, and exploring module/resource details.\n\nThe server can be used with VS Code or Claude Desktop by configuring the MCP settings to run the server via Docker. If Docker isn't available, the server can be built directly from source using `make build`. A local Docker image can also be created using `make docker-build`.\n\nThe server provides toolsets for `providers` (resolving provider documentation IDs and fetching documentation content) and `modules` (searching for modules and retrieving detailed module information).\n\nThe document outlines the installation process, configuration details for VS Code and Claude Desktop, and instructions for building from source or via Docker. It also covers development aspects like prerequisites (Go, Docker), running tests using `make`, and various `make` commands. Finally, it explains the process for contributing to the project, the licensing information (MPL-2.0), security policy, and support channels for bug reports, feature requests, and general discussions. A crucial caution is included: users must thoroughly review dynamically generated outputs to ensure alignment with security, cost-efficiency, and compliance requirements.\n",
    "chinese_title": "Terraform MCP 服务器",
    "chinese_summary": "Terraform MCP 服务器是一个模型上下文协议 (MCP) 服务器，旨在通过与 Terraform Registry API 集成来增强 Terraform 开发。它实现了与基础设施即代码 (IaC) 的自动化和高级交互。主要用例包括自动化 provider/模块发现、提取注册表数据以及探索模块/资源详细信息。\n\n通过配置 MCP 设置以使用 Docker 运行服务器，该服务器可以与 VS Code 或 Claude Desktop 一起使用。如果 Docker 不可用，可以使用 `make build` 直接从源代码构建服务器。也可以使用 `make docker-build` 创建本地 Docker 镜像。\n\n该服务器为 `providers`（解析 provider 文档 ID 并获取文档内容）和 `modules`（搜索模块并检索详细模块信息）提供工具集。\n\n本文档概述了安装过程、VS Code 和 Claude Desktop 的配置详情，以及从源代码或通过 Docker 构建的说明。它还涵盖了开发方面，例如先决条件（Go、Docker）、使用 `make` 运行测试以及各种 `make` 命令。最后，它解释了贡献项目的流程、许可信息 (MPL-2.0)、安全策略以及用于错误报告、功能请求和一般讨论的支持渠道。包含一项重要警告：用户必须彻底审查动态生成的输出，以确保其符合安全性、成本效益和合规性要求。"
  },
  {
    "id": "44030850",
    "title": "Zod 4",
    "url": "https://zod.dev/v4",
    "summary": "Zod 4, a major update to the Zod validation library, is now stable and available as part of the `zod@3.25` release, importable via `\"zod/v4\"`. It boasts significant performance improvements, including up to 14x faster string parsing, 7x faster array parsing, and 6.5x faster object parsing.  It also includes a 100x reduction in tsc instantiations and a 2x reduction in core bundle size. These enhancements address long-standing design limitations of Zod 3.\n\nKey new features include:\n*   **Zod Mini:** A tree-shakable variant for smaller bundle sizes.\n*   **Metadata:** A strongly-typed metadata system accessible through `z.registry()` and `z.meta()`, integrating with JSON Schema conversion.\n*   **JSON Schema Conversion:**  First-party JSON Schema generation via `z.toJSONSchema()`.\n*   **Recursive Objects:** Proper inference of recursive object types.\n*   **File Schemas:** Validation for `File` instances, including size and MIME type checks.\n*   **Internationalization:** Locales API for translating error messages, currently supporting English.\n*   **Error Pretty-Printing:** `z.prettifyError` function for user-friendly error formatting.\n*   **Top-level String Formats:** String format validators (e.g., `z.email()`) are now top-level functions.\n\nZod 4 is designed for better scalability, performance, and developer experience, laying a new foundation for the library's future.\n",
    "chinese_title": "佐德 4",
    "chinese_summary": "Zod 4 发布：性能显著提升，特性丰富"
  },
  {
    "id": "44039563",
    "title": "The behavior of LLMs in hiring decisions: Systemic biases in candidate selection",
    "url": "https://davidrozado.substack.com/p/the-strange-behavior-of-llms-in-hiring",
    "summary": "Unable to access the article link.\n",
    "chinese_title": "LLM在招聘决策中的行为：候选人选择中的系统性偏见",
    "chinese_summary": "无法访问文章链接。"
  },
  {
    "id": "44031432",
    "title": "GitHub Copilot Coding Agent",
    "url": "https://github.blog/changelog/2025-05-19-github-copilot-coding-agent-in-public-preview/",
    "summary": "GitHub Copilot Coding Agent allows users to delegate coding tasks to Copilot, freeing up developers to focus on more complex work. Users can assign issues to Copilot directly from GitHub, GitHub Mobile, or the GitHub CLI. Copilot then works in the background, using a secure cloud environment, to explore the repository, make code changes, validate its work with tests, and push the changes. Once complete, Copilot tags the user for review, allowing for feedback and further refinement either through comments in the pull request or locally in the IDE.\n\nCopilot is best suited for low-to-medium complexity tasks in well-tested codebases, including adding features, fixing bugs, extending tests, refactoring, and improving documentation. Users can assign multiple issues concurrently.\n\nThe feature is currently available for Copilot Pro+ and Copilot Enterprise subscribers, with administrators needing to enable the feature for Enterprise users. Its use consumes GitHub Actions minutes and Copilot premium requests. GitHub Mobile and CLI support are rolling out gradually to users.\n\nAs of June 4th, Copilot coding agent will use one premium request per model request, and it remains a preview feature subject to change. Users are encouraged to provide feedback through the linked discussion.\n",
    "chinese_title": "GitHub Copilot 编码助手",
    "chinese_summary": "GitHub Copilot 编码代理允许用户将编码任务委派给 Copilot，使开发人员能够专注于更复杂的工作。用户可以直接从 GitHub、GitHub Mobile 或 GitHub CLI 将问题分配给 Copilot。然后，Copilot 在后台使用安全云环境，探索存储库，进行代码更改，通过测试验证其工作，并推送更改。完成后，Copilot 标记用户以进行审查，允许通过拉取请求中的评论或本地 IDE 提供反馈和进一步改进。\n\nCopilot 最适合在经过良好测试的代码库中执行低到中等复杂度的任务，包括添加功能、修复错误、扩展测试、重构和改进文档。用户可以同时分配多个问题。\n\n该功能目前适用于 Copilot Pro+ 和 Copilot Enterprise 订阅者，企业用户需要管理员启用该功能。它的使用会消耗 GitHub Actions 分钟数和 Copilot 高级请求。GitHub Mobile 和 CLI 支持正在逐步向用户推出。\n\n截至 6 月 4 日，Copilot 编码代理每次模型请求将使用一个高级请求，并且它仍然是一个预览功能，可能会发生变化。鼓励用户通过链接的讨论提供反馈。"
  },
  {
    "id": "44031917",
    "title": "Dilbert creator Scott Adams says he will die soon from same cancer as Joe Biden",
    "url": "https://www.thewrap.com/dilbert-scott-adams-prostate-cancer-biden/",
    "summary": "The article reports that \"Dilbert\" creator Scott Adams announced he is terminally ill with prostate cancer, the same disease former President Joe Biden is reportedly battling. Adams revealed the information during his \"Coffee With Scott Adams\" Rumble show, stating that his cancer has spread to his bones and he anticipates dying this summer. He noted that while localized prostate cancer is curable, it becomes incurable once it spreads. Adams, who gained fame with \"Dilbert\" and has become increasingly politically outspoken, expressed sympathy for Biden and his family given the diagnosis. The article highlights Adams' social media presence, noting his followers on Rumble and X (formerly Twitter).\n",
    "chinese_title": "呆伯特漫画作者斯科特·亚当斯称他将很快死于与乔·拜登相同的癌症。",
    "chinese_summary": "“呆伯特”创作者斯科特·亚当斯宣布罹患前列腺癌晚期，据报道前总统乔·拜登也患有同样的疾病。亚当斯在他的Rumble节目“与斯科特·亚当斯喝咖啡”中透露了这一消息，称他的癌症已扩散到骨骼，预计今年夏天会去世。他指出，虽然局部前列腺癌可以治愈，但一旦扩散就无法治愈。亚当斯凭借“呆伯特”成名，并且越来越直言不讳地发表政治言论，他对拜登及其家人表示同情。文章还强调了亚当斯在社交媒体上的影响力，提到了他在Rumble和X（前身为Twitter）上的粉丝数量。"
  },
  {
    "id": "44036716",
    "title": "A shower thought turned into a Collatz visualization",
    "url": "https://abstractnonsense.com/collatz/",
    "summary": "The author, inspired by a shower thought, explores a visualization of the Collatz Conjecture by encoding the sequence of even/odd choices for each number as a binary fraction.  The Collatz Conjecture posits that repeatedly applying the rule (n/2 if even, 3n+1 if odd) to any positive integer will eventually reach 1.\n\nTo address the even number bias in the Collatz sequence, the author uses the \"shortcut\" Collatz function, immediately dividing by 2 after the 3n+1 step. He then represents each step as a binary digit (1 for even, 0 for odd) and converts these bit sequences into fractions.\n\nPlotting these \"Collatz fractions\" for a range of numbers initially appears uniformly distributed. However, plotting subsequent pairs of fractions (fn, fn+1) reveals surprising self-similar patterns resembling alien writing. The author encourages exploration of these patterns, providing an interactive tool with colorization rules.\n\nInterestingly, the author discovered that this visualization, despite his unique approach, had already been explored. He cites Olivier Rozier's 2019 paper and Yukihiro Hashimoto's 2007 paper, both containing the same plot derived from 2-adic numbers.  While their methods differ from the author's direct fraction approach, the resulting visuals are identical. The author hopes this accessible visualization will inspire new insights into the Collatz Conjecture.\n",
    "chinese_title": "一个洗澡时的灵感变成了考拉兹猜想的可视化。",
    "chinese_summary": "作者受一次淋浴时的灵感启发，探索了一种考拉兹猜想的可视化方法，即通过将每个数字的奇偶选择序列编码为二进制分数。考拉兹猜想假定，对任何正整数重复应用规则（偶数则除以2，奇数则乘以3加1），最终都会达到1。\n\n为了解决考拉兹序列中偶数偏差的问题，作者使用了“捷径”考拉兹函数，在3n+1步骤后立即除以2。然后，他将每个步骤表示为一个二进制数字（偶数为1，奇数为0），并将这些位序列转换为分数。\n\n最初绘制一系列数字的这些“考拉兹分数”时，看起来是均匀分布的。然而，绘制随后的分数对（fn，fn+1）揭示了惊人的自相似模式，类似于外星文字。作者鼓励探索这些模式，并提供了一个带有着色规则的交互式工具。\n\n有趣的是，作者发现，尽管他采用了独特的方法，但这种可视化方法已经被探索过了。他引用了Olivier Rozier 2019年的论文和Yukihiro Hashimoto 2007年的论文，这两篇论文都包含了来自2-adic数的相同图。虽然他们的方法与作者的直接分数方法不同，但产生的视觉效果是相同的。作者希望这种易于理解的可视化方法能够激发人们对考拉兹猜想的新见解。"
  },
  {
    "id": "44036829",
    "title": "Memory Consistency Models: A Tutorial",
    "url": "https://jamesbornholt.com/blog/memory-models/",
    "summary": "This article provides a tutorial on memory consistency models, explaining why they are crucial for understanding how parallel threads interact with shared memory in multicore systems. It starts by illustrating the challenge of maintaining order in concurrent operations, demonstrating how seemingly intuitive program execution can lead to unexpected outcomes.\n\nThe article introduces sequential consistency (SC) as a simple model where all operations appear to happen in a single, ordered sequence, preserving program order within each thread. However, SC is inefficient due to its strict ordering requirements, hindering parallel execution.\n\nThe concept of coherence is then presented, guaranteeing that all writes to the same memory location are seen in the same order by all threads. The article then discusses relaxed memory models, such as total store ordering (TSO), which allows for store buffers to improve performance by hiding write latency. While TSO enhances speed, it can lead to behaviors that contradict SC expectations.\n\nThe article further delves into weaker memory models like ARM's, emphasizing the need for synchronization mechanisms like barriers (or fences) to enforce ordering when necessary. Barriers ensure that all memory operations before the barrier complete before any operations after it can begin. Finally, the article addresses the role of compilers in reordering memory operations and highlights the importance of memory models in programming languages like C++ and Java to maintain predictable behavior in parallel programs.\n",
    "chinese_title": "内存一致性模型：教程",
    "chinese_summary": "本文提供了一个关于内存一致性模型的教程，解释了为什么它们对于理解多核系统中并行线程如何与共享内存交互至关重要。它首先阐述了在并发操作中保持顺序所面临的挑战，展示了看似直观的程序执行如何导致意想不到的结果。\n\n本文介绍了顺序一致性（SC）作为一种简单的模型，其中所有操作似乎都以单一、有序的序列发生，并在每个线程中保留程序顺序。然而，由于其严格的排序要求，SC效率低下，阻碍了并行执行。\n\n然后介绍了连贯性的概念，保证对同一内存位置的所有写入都以相同的顺序被所有线程看到。本文接着讨论了宽松内存模型，例如总存储顺序（TSO），它允许使用存储缓冲区来提高性能，从而隐藏写入延迟。虽然TSO提高了速度，但它可能导致与SC预期相悖的行为。\n\n本文进一步深入探讨了像ARM这样的较弱的内存模型，强调了当必要时需要像屏障（或栅栏）这样的同步机制来强制排序。屏障确保屏障之前的所有内存操作在屏障之后的任何操作开始之前完成。最后，本文讨论了编译器在重新排序内存操作中的作用，并强调了C++和Java等编程语言中内存模型的重要性，以在并行程序中保持可预测的行为。"
  },
  {
    "id": "44005236",
    "title": "Imagine a drive where every file exists as all file types all of the time",
    "url": "https://anydocai.com/",
    "summary": "The article, titled \"Imagine a drive where every file exists as all file types all of the time,\" introduces AnydocAI, a concept (or potentially a product) that seemingly allows for universal file type conversion and accessibility. The title immediately hints at a radical approach to file management, suggesting a system where a single file implicitly exists in all possible formats simultaneously.\n\nWhile the extremely short content (just the name \"AnydocAI\") provides no explicit details, the implied premise is revolutionary. It eliminates the need for manual file conversion or worrying about software compatibility. Users wouldn't need to convert a .docx to a .pdf or a .jpg to a .png; instead, any file would be accessible and usable in any desired format on demand.\n\nThis concept has huge implications for collaboration, archiving, and overall digital workflow. The friction caused by file format limitations and software dependencies would be significantly reduced. However, the viability of such a system raises questions about storage space, processing power, and the potential for complex file interactions.\n",
    "chinese_title": "设想一个所有文件始终以所有文件类型存在的驱动器。",
    "chinese_summary": "文章题为“想象一下，每次驱动都存在所有文件类型的文件”，介绍了AnydocAI，一个概念（或可能是一个产品），似乎允许通用文件类型转换和可访问性。标题立即暗示了一种激进的文件管理方法，表明一个单一文件同时以所有可能的格式隐式存在的系统。\n\n虽然极短的内容（仅名称“AnydocAI”）没有提供明确的细节，但其隐含的前提是革命性的。它消除了手动文件转换或担心软件兼容性的需要。用户无需将.docx转换为.pdf或将.jpg转换为.png；相反，任何文件都可以按需以任何所需格式访问和使用。\n\n这个概念对协作、归档和整体数字工作流程具有巨大的影响。文件格式限制和软件依赖性造成的摩擦将大大减少。然而，这样一个系统的可行性引发了关于存储空间、处理能力以及复杂文件交互的可能性的问题。"
  },
  {
    "id": "44034329",
    "title": "WireGuard vanity keygen",
    "url": "https://github.com/axllent/wireguard-vanity-keygen",
    "summary": "This article introduces `wireguard-vanity-keygen`, a command-line tool for generating WireGuard private and public keys that match specific prefixes or patterns, offering a way to visually identify connections on WireGuard servers. The tool allows for multi-core processing, case-sensitive or regex searching, and limiting the number of results.\n\nThe article explains how to use the tool, including various options like `-c` for case-sensitive search, `-t` to set the number of threads, and `-l` to limit the number of results. Example usage is provided, along with a discussion of probability and estimated runtime based on benchmarked CPU speed. Timings for finding keys of different lengths are also included, along with considerations for case sensitivity and number of search terms.\n\nIt also details the use of regular expressions for more complex searches. The FAQ section answers common questions about valid search characters, probability calculations, and the purpose of the tool. In summary, `wireguard-vanity-keygen` provides a customizable and efficient way to generate WireGuard keys with specific, identifiable prefixes for improved server management.\n",
    "chinese_title": "WireGuard 自定义密钥生成器",
    "chinese_summary": "本文介绍 `wireguard-vanity-keygen`，一个用于生成符合特定前缀或模式的 WireGuard 私钥和公钥的命令行工具，它提供了一种在 WireGuard 服务器上直观识别连接的方法。该工具支持多核处理、区分大小写或正则表达式搜索，以及限制结果数量。\n\n本文解释了如何使用该工具，包括各种选项，如 `-c` 用于区分大小写搜索，`-t` 用于设置线程数，以及 `-l` 用于限制结果数量。文章提供了示例用法，以及基于基准 CPU 速度的概率和预计运行时的讨论。文章还包括查找不同长度密钥的计时，以及对大小写敏感性和搜索词数量的考虑。\n\n文章还详细介绍了使用正则表达式进行更复杂搜索的方法。常见问题解答部分回答了关于有效搜索字符、概率计算以及该工具用途的常见问题。总而言之，`wireguard-vanity-keygen` 提供了一种可定制且高效的方式来生成具有特定、可识别前缀的 WireGuard 密钥，从而改进服务器管理。"
  },
  {
    "id": "43999897",
    "title": "In Memoriam: John L. Young, Cryptome Co-Founder",
    "url": "https://www.eff.org/deeplinks/2025/05/memoriam-john-l-young-cryptome-co-founder",
    "summary": "John L. Young, co-founder of Cryptome, a pioneering online library of official secrets, passed away at 89. Young and his wife, Deborah Natsios, established Cryptome in 1996 to make information hidden by governments and corporations publicly available, championing freedom of expression, privacy, and government transparency. Their slogan was \"The greatest threat to democracy is official secrecy which favors a few over the many.\"\n\nCryptome became a vital resource for documents on government, court, and corporate affairs, particularly during the \"crypto wars\" of the 1990s, where activists fought for encryption freedom. The site also initially supported WikiLeaks, but Young later distanced himself due to concerns about monetization, eventually publishing Wikileaks' internal emails.\n\nYoung, trained as an architect, had a long history of advocating for public good and transparency, seeking access to documents about public development entities. His expertise led him to create Cryptome, a real-time archive of critical debates shaping the internet's information infrastructure. Despite facing pressure from the FBI, Secret Service, and tech companies, Young remained a steadfast advocate for public access to information. He was also involved in creating the community service group Urban Deadline.\n\nThe article highlights Young's unwavering commitment to democratizing access to information in the digital age, solidifying his legacy as an under-recognized hero of the internet. His dedication to the public's right to know will be deeply missed.\n",
    "chinese_title": "悼念：Cryptome联合创始人约翰·L·杨",
    "chinese_summary": "Cryptome联合创始人、官方机密先锋在线图书馆的共同创建者约翰·L·杨去世，享年89岁。杨和他的妻子黛博拉·纳西奥斯于1996年创建了Cryptome，旨在公开政府和企业隐藏的信息，倡导言论自由、隐私和政府透明度。他们的口号是“对民主的最大威胁是偏袒少数人的官方保密。”\n\nCryptome成为了政府、法院和公司事务文件的重要资源，尤其是在20世纪90年代的“密码战争”期间，当时活动家们为加密自由而战。该网站最初也支持维基解密，但杨后来因担心货币化而疏远了自己，最终公布了维基解密的内部电子邮件。\n\n杨曾接受过建筑师培训，长期以来一直倡导公共利益和透明度，寻求获取有关公共发展实体的文件。他的专业知识使他创建了Cryptome，这是一个塑造互联网信息基础设施的关键辩论的实时档案。尽管面临来自联邦调查局、特勤局和科技公司的压力，杨仍然坚定地倡导公众获取信息。他还参与创建了社区服务团体Urban Deadline。\n\n这篇文章强调了杨在数字时代坚定不移地致力于实现信息获取民主化，巩固了他作为互联网上一个未被充分认识的英雄的遗产。他对公众知情权的奉献精神将被人们深深怀念。"
  },
  {
    "id": "44040332",
    "title": "Reports of Deno's Demise Have Been Greatly Exaggerated",
    "url": "https://deno.com/blog/greatly-exaggerated",
    "summary": "In a post titled \"Reports of Deno's Demise Have Been Greatly Exaggerated,\" Ryan Dahl addresses recent criticisms and clarifies the direction of Deno and its related products. Despite concerns, Deno adoption has more than doubled since the release of Deno 2, thanks to improved Node compatibility.\n\nDahl explains the reduction in Deno Deploy regions was a strategic decision based on usage patterns. Instead of focusing on a broad edge presence, they are optimizing for speed, proximity to data, and compliance, evolving Deploy into a platform for hosting applications, not just functions. New features like subprocesses, background tasks, and the ability to pin apps to specific regions are coming.\n\nDeno KV will remain in beta, serving as a zero-config global store, but is not intended to be a general-purpose database. The team is working on making traditional relational databases easier to use with Deno Deploy and exploring a new project to tightly integrate compute and state.\n\nFresh, the full-stack web framework, is also alive and well, with Fresh 2 arriving later this year, boasting significant improvements.\n\nDahl emphasizes Deno is a complete platform with built-in TypeScript/JSX support, granular permissions, LSP support, Jupyter integration, standalone binary generation, Node/npm compatibility, OpenTelemetry, and more. The goal is to improve JavaScript development, offering a cohesive, batteries-included system. Deno is actively improving the JavaScript ecosystem through JSR governance, contributions to TC39/WinterTC, and challenging the Oracle JavaScript trademark. New products are also in development, promising to simplify persistent, distributed applications.\n",
    "chinese_title": "关于Deno消亡的说法纯属夸大其词",
    "chinese_summary": "Deno之死被严重夸大：Ryan Dahl回应批评并阐明Deno发展方向\n\nRyan Dahl在一篇名为“Deno之死被严重夸大”的文章中，回应了近期的批评，并阐明了Deno及其相关产品的未来发展方向。尽管存在担忧，但由于Node兼容性的提高，自Deno 2发布以来，Deno的使用量已经增加了一倍以上。\n\nDahl解释说，减少Deno Deploy区域是基于使用模式的战略决策。他们没有专注于广泛的边缘存在，而是优化速度、数据邻近性和合规性，从而将Deploy发展成为一个托管应用程序而非仅仅是函数的平台。诸如子进程、后台任务以及将应用程序固定到特定区域的能力等新功能即将推出。\n\nDeno KV将继续保持测试版，作为一个零配置的全局存储，但并不打算成为一个通用的数据库。团队正在努力使传统的关系型数据库更容易与Deno Deploy一起使用，并且正在探索一个新项目，以紧密集成计算和状态。\n\n全栈Web框架Fresh也依然充满活力，Fresh 2将于今年晚些时候推出，并将带来重大改进。\n\nDahl强调，Deno是一个完整的平台，具有内置的TypeScript/JSX支持、细粒度权限、LSP支持、Jupyter集成、独立二进制文件生成、Node/npm兼容性、OpenTelemetry等功能。其目标是改进JavaScript开发，提供一个有凝聚力、开箱即用的系统。Deno正在通过JSR治理、对TC39/WinterTC的贡献以及挑战Oracle JavaScript商标来积极改进JavaScript生态系统。新的产品也在开发中，有望简化持久化、分布式应用程序。"
  },
  {
    "id": "44013406",
    "title": "Patience too cheap to meter",
    "url": "https://www.seangoedecke.com/patience-too-cheap-to-meter/",
    "summary": "This article argues that while intelligence is a key focus in the development of large language models (LLMs), their most transformative capability might be their superhuman patience, which is now readily available and \"too cheap to meter.\"\n\nThe author notes that despite the availability of smarter models like Claude Sonnet, many users are content with basic models like ChatGPT. This suggests that raw intelligence isn't the primary driver of user adoption. Instead, the author proposes that patience is the real value proposition.\n\nReferencing an article about using ChatGPT for therapy, the author highlights that LLMs provide always-available, non-judgmental, and endlessly patient companionship. This is particularly appealing for users seeking emotional support or advice, even though such advice may not require superior intelligence. Even less intelligent models can be effective due to their willingness to listen without frustration.\n\nThe article also acknowledges potential downsides. The author worries that unlimited patience could amplify existing flaws in LLM advice, like sycophancy. There are also concerns about users replacing human therapists with LLMs lacking proper escalation pathways, and the potential for users to become frustrated with the limited patience of human interaction.\n\nUltimately, the article concludes that the constant availability of something infinitely patient is a novel development in human history, perhaps more impactful than the pursuit of ever-increasing intelligence in LLMs. The author also suggests that patience is a key reason LLMs are effective tutors.\n",
    "chinese_title": "耐心廉价到无需计量",
    "chinese_summary": "本文认为，虽然智能是大语言模型（LLM）发展的关键焦点，但其最具变革性的能力可能是其超人的耐心，这种耐心现在唾手可得且“极其廉价”。\n\n作者指出，尽管有像Claude Sonnet这样更智能的模型，但许多用户对ChatGPT等基本模型感到满意。这表明原始智能并非用户采用的主要驱动因素。相反，作者认为耐心才是真正的价值所在。\n\n作者引用了一篇关于使用ChatGPT进行治疗的文章，强调LLM提供了随时可用、不带偏见且无限耐心的陪伴。对于寻求情感支持或建议的用户来说，这一点尤其具有吸引力，即使这些建议可能不需要卓越的智能。即使不太智能的模型也能有效，因为它们愿意倾听而不感到沮丧。\n\n文章也承认了潜在的缺点。作者担心无限的耐心可能会放大LLM建议中现有的缺陷，例如谄媚。人们还担心用户会用缺乏适当升级途径的LLM取代人类治疗师，以及用户可能会对人类互动中有限的耐心感到沮丧。\n\n最终，文章得出结论，无限耐心事物的持续存在是人类历史上一种新颖的发展，其影响可能比追求LLM中不断增长的智能更大。作者还认为，耐心是LLM成为有效导师的关键原因。"
  },
  {
    "id": "44028250",
    "title": "Telum II at Hot Chips 2024: Mainframe with a Unique Caching Strategy",
    "url": "https://chipsandcheese.com/p/telum-ii-at-hot-chips-2024-mainframe-with-a-unique-caching-strategy",
    "summary": "This article analyzes IBM's Telum II mainframe processor, presented at Hot Chips 2024, highlighting its unique caching strategy focused on single-threaded performance. Unlike typical server CPUs, Telum II prioritizes high clock speeds (5.5 GHz) and a massive 360 MB on-chip cache across only eight cores.\n\nA key innovation is the \"virtual L3 cache,\" which leverages the processor's ten large 36MB L2 caches to avoid data duplication and maximize cache utilization. Each L2 has a \"Saturation Metric\" to determine which L2 is the best destination for evicted cache lines, effectively creating a shared L3 pool distributed across the L2 slices. This approach mitigates the area and power costs associated with a dedicated, larger L3 cache.\n\nTelum II extends this concept with a \"virtual L4 cache,\" a 2.8 GB pool formed by sending L3 victims to other Telum II chips within a Central Processor Complex (CPC) drawer. This aims to minimize latency across a multi-processor system.\n\nThe article contrasts Telum II's approach with typical server and client CPU designs, noting that IBM has strategically reduced core counts to prioritize single-threaded performance by maximizing available cache per core. It discusses the possibility of applying similar strategies to client CPUs by increasing the cache size and sharing it among the cores.\n",
    "chinese_title": "Telum II 在 Hot Chips 2024 大会：采用独特缓存策略的大型主机",
    "chinese_summary": "本文分析了在Hot Chips 2024上发布的IBM Telum II大型机处理器，重点介绍了其独特的、专注于单线程性能的缓存策略。与典型的服务器CPU不同，Telum II优先考虑高时钟速度（5.5 GHz）和仅八个核心上的高达360 MB的片上缓存。\n\n一项关键创新是“虚拟L3缓存”，它利用处理器十个大型36MB L2缓存来避免数据重复并最大化缓存利用率。每个L2都有一个“饱和度指标”，用于确定哪个L2是逐出缓存行的最佳目的地，从而有效地创建了一个分布在L2切片上的共享L3池。这种方法降低了专用、更大的L3缓存相关的面积和功耗成本。\n\nTelum II通过“虚拟L4缓存”进一步扩展了这个概念，它是一个由将L3牺牲者发送到中央处理器复合体 (CPC) 抽屉内的其他Telum II芯片形成的2.8 GB池。这旨在最大限度地减少跨多处理器系统的延迟。\n\n本文将Telum II的方法与典型的服务器和客户端CPU设计进行了对比，指出IBM已战略性地减少了核心数量，以通过最大化每个核心的可用缓存来优先考虑单线程性能。它讨论了通过增加缓存大小并在核心之间共享缓存，将类似策略应用于客户端CPU的可能性。"
  },
  {
    "id": "44039719",
    "title": "Microsoft blocked the email account of Chief Prosecutor of the ICC",
    "url": "https://www.heise.de/en/news/Criminal-Court-Microsoft-s-email-block-a-wake-up-call-for-digital-sovereignty-10387383.html",
    "summary": "Here's a summary of the article based on the provided title:\n\nThe article reports that Microsoft blocked the email account of Karim Khan, the Chief Prosecutor of the International Criminal Court (ICC). This action has sparked significant debate and raised concerns about digital sovereignty and the influence of private tech companies on international justice.\n\nWhile the specific reasons for the block remain somewhat unclear in the available context (based solely on the title), the article likely explores the implications of a major tech corporation, like Microsoft, having the power to restrict access to communication for such a high-profile figure involved in international law.\n\nThe article, published by heise online, frames this incident as a \"wake-up call\" regarding digital sovereignty. This suggests that the event highlights the reliance of international institutions and individuals on infrastructure controlled by entities, like Microsoft, that are subject to national laws and potentially geopolitical pressures. This dependence can create vulnerabilities and raise questions about impartiality.\n\nThe article likely delves into the potential consequences of this event, possibly including:\n\n*   Hindrance of the ICC's work and investigations.\n*   Impact on the integrity and independence of the court.\n*   Broader implications for other international organizations and individuals who rely on digital communication tools.\n*   The need for greater digital sovereignty and control over data infrastructure for international bodies.\n",
    "chinese_title": "微软封锁了国际刑事法院首席检察官的电子邮件账户。",
    "chinese_summary": "以下是根据标题对文章的总结：\n\n文章报道微软封锁了国际刑事法院（ICC）首席检察官卡里姆·汗的电子邮件账户。此举引发了重大争议，并引发了人们对数字主权以及私营科技公司对国际司法影响的担忧。\n\n虽然根据现有背景（仅基于标题），封锁的具体原因仍然有些不清楚，但文章可能探讨了像微软这样的大型科技公司拥有限制如此高调的国际法人物的通信访问权的影响。\n\n由heise online发表的这篇文章将这一事件定义为关于数字主权的“警钟”。这表明该事件突显了国际机构和个人对受国家法律和潜在地缘政治压力影响的实体（如微软）控制的基础设施的依赖。这种依赖性可能会产生漏洞并引发对公正性的质疑。\n\n文章可能深入探讨了该事件的潜在后果，可能包括：\n\n*   妨碍国际刑事法院的工作和调查。\n*   影响法院的完整性和独立性。\n*   对其他依赖数字通信工具的国际组织和个人产生更广泛的影响。\n*   国际机构需要更大的数字主权和对数据基础设施的控制权。"
  },
  {
    "id": "44003184",
    "title": "Run GitHub Actions locally",
    "url": "https://github.com/nektos/act",
    "summary": "This article introduces `act`, a tool that allows users to run GitHub Actions locally, providing fast feedback and acting as a local task runner. Instead of constantly committing and pushing code to test changes in GitHub Actions workflows, `act` lets developers run and test these workflows locally, mirroring the environment provided by GitHub. It reads workflow definitions from the `.github/workflows/` directory, pulls or builds necessary Docker images based on those definitions, and executes the actions in Docker containers, mimicking the GitHub environment.\n\nKey benefits include:\n\n*   **Fast Feedback:** Immediate testing of workflow changes without pushing to GitHub.\n*   **Local Task Runner:** Replacing Makefiles by leveraging GitHub Actions definitions for task execution.\n*   **VS Code Integration:** A VS Code extension allows managing and running `act` directly from the editor.\n\nThe article outlines the process of how `act` works, involving reading workflow files, preparing Docker images, determining execution paths based on dependencies, and running containers for each action. It also provides links to the act user guide, support channels, contribution guidelines, and instructions for building `act` from source.\n",
    "chinese_title": "在本地运行 GitHub Actions",
    "chinese_summary": "本文介绍`act`，一个允许用户在本地运行GitHub Actions的工具，它提供快速反馈并充当本地任务运行器。`act`允许开发者在本地运行和测试这些工作流程，从而模拟GitHub提供的环境，而无需不断地提交和推送代码来测试GitHub Actions工作流程中的更改。它从`.github/workflows/`目录读取工作流程定义，根据这些定义拉取或构建必要的Docker镜像，并在Docker容器中执行这些action，模拟GitHub环境。\n\n主要优点包括：\n\n*   **快速反馈：** 无需推送到GitHub即可立即测试工作流程更改。\n*   **本地任务运行器：** 利用GitHub Actions定义来替换Makefile以进行任务执行。\n*   **VS Code 集成：** VS Code 扩展允许直接从编辑器管理和运行`act`。\n\n本文概述了`act`的工作原理，包括读取工作流程文件、准备Docker镜像、根据依赖关系确定执行路径以及为每个action运行容器。文章还提供了指向act用户指南、支持渠道、贡献指南以及从源代码构建`act`的说明的链接。"
  },
  {
    "id": "44029435",
    "title": "Diffusion models explained simply",
    "url": "https://www.seangoedecke.com/diffusion-models-explained/",
    "summary": "This article explains diffusion models, a key technology behind AI image generation. Unlike transformers that predict the next token in a sequence, diffusion models learn to reverse the process of adding noise to an image.\n\nTraining involves feeding the model noisy images and captions, asking it to predict the added noise. The model is then rewarded for accurate predictions. Inference begins with pure noise and a caption. The model iteratively identifies and removes layers of noise, revealing the generated image.\n\nThe process often involves variational autoencoders (VAEs) to compress images into smaller, random-looking tensors for efficient processing. Classifier-free guidance is used to ensure the model focuses on the caption by sometimes training without captions and then blending results during inference.\n\nDiffusion models differ from transformers by operating on the entire image at each step, starting with a \"blank canvas\" of noise, and editing previous outputs. They offer a built-in quality knob; stopping early yields a faster, noisier image, while continued processing improves quality.\n\nThe article extends the concept to video, where the entire video clip is treated as a noisy tensor. Text diffusion is more complex, involving adding noise to text embeddings. Converting these embeddings back to text presents challenges.\n",
    "chinese_title": "扩散模型简述",
    "chinese_summary": "本文介绍了扩散模型，它是人工智能图像生成背后的关键技术。与预测序列中下一个token的Transformer不同，扩散模型学习逆转向图像添加噪声的过程。\n\n训练涉及向模型输入带噪声的图像和文本描述，要求它预测添加的噪声。然后，模型会因准确的预测而获得奖励。推理始于纯噪声和文本描述。模型迭代地识别并去除噪声层，从而揭示生成的图像。\n\n该过程通常涉及变分自编码器（VAE），用于将图像压缩成更小的、随机外观的张量，以实现高效处理。无分类器引导用于确保模型专注于文本描述，方法是有时在没有文本描述的情况下进行训练，然后在推理过程中混合结果。\n\n扩散模型与Transformer的不同之处在于，它在每个步骤都对整个图像进行操作，从一个“空白画布”噪声开始，并编辑之前的输出。它们提供了一个内置的质量旋钮；提前停止会产生更快、更嘈杂的图像，而继续处理则会提高质量。\n\n本文将该概念扩展到视频，其中整个视频剪辑被视为一个噪声张量。文本扩散更为复杂，涉及向文本嵌入添加噪声。将这些嵌入转换回文本提出了挑战。"
  },
  {
    "id": "44005461",
    "title": "Microbes in Gowanus teach lessons on fighting industrial pollution",
    "url": "https://engineering.nyu.edu/news/microbes-brooklyn-superfund-site-teach-lessons-fighting-industrial-pollution",
    "summary": "NYU Tandon-led research has uncovered that microbes in Brooklyn's Gowanus Canal, a Superfund site, possess remarkable genetic adaptations for combating industrial pollution. Published in the Journal of Applied Microbiology, the study identified 455 microbial species utilizing 64 biochemical pathways to degrade pollutants and 1,171 genes to process heavy metals. Researchers also discovered over 2,300 novel genetic sequences potentially valuable for medicine, industry, and environmental applications.\n\nThe discovery offers the potential for cheaper, more sustainable cleanup methods compared to traditional dredging. However, the study also revealed antibiotic resistance genes, some originating from human sewage, raising public health concerns. Despite this, the researchers emphasize the potential to learn from these microbes, either by isolating specific strains or enhancing their abilities, to develop faster and more effective bioremediation techniques for pollutant breakdown or even resource recovery.\n\nThe research involved extensive sampling from 14 locations along the canal and builds upon a decade of prior studies. The team identified microbes capable of breaking down various pollutants, including petroleum products and PCBs. To share their findings, the researchers created CHANNEL, an immersive art installation at BioBAT Art Space showcasing the microbial study of the waterway. The research highlights the importance of understanding microbial adaptations for environmental restoration and potential resource recovery.\n",
    "chinese_title": "高瓦纳斯河的微生物教你对抗工业污染",
    "chinese_summary": "纽约大学坦登学院主导的研究发现，布鲁克林高瓦纳斯运河（一个超级基金场地）中的微生物具有非凡的基因适应能力，能够对抗工业污染。这项发表在《应用微生物学杂志》上的研究确定了455种微生物物种，它们利用64种生化途径降解污染物，并利用1171个基因处理重金属。研究人员还发现了2300多个新的基因序列，这些序列可能对医药、工业和环境应用具有价值。\n\n与传统的疏浚方法相比，这一发现为更便宜、更可持续的清理方法提供了潜力。然而，该研究还揭示了抗生素耐药基因，其中一些源于人类污水，引发了公众健康担忧。尽管如此，研究人员强调了从这些微生物中学习的潜力，无论是通过分离特定菌株还是增强它们的能力，以开发更快、更有效的生物修复技术，用于污染物分解，甚至资源回收。\n\n该研究涉及从运河沿线的14个地点进行广泛采样，并建立在过去十年的研究基础上。该团队确定了能够分解各种污染物的微生物，包括石油产品和多氯联苯。为了分享他们的发现，研究人员在BioBAT艺术空间创作了一个沉浸式艺术装置CHANNEL，展示了对该水道的微生物研究。该研究强调了理解微生物适应性对于环境修复和潜在资源回收的重要性。"
  },
  {
    "id": "44013739",
    "title": "Precomputing Transparency Order in 3D",
    "url": "https://jacobdoescode.com/2025/05/18/precomputing-transparency-order-in-3d",
    "summary": "This article explores a method for precomputing the rendering order of translucent 3D faces to avoid CPU sorting based on camera distance every frame. The author proposes a technique based on face culling and analyzing the spatial relationship between pairs of faces.\n\nThe core idea is to split each translucent face into two sides, each oriented opposite to the other. By analyzing whether the vertices of face B are wholly above, wholly below, intersecting, or coplanar with the plane of face A, and vice-versa, a *partial* sorting order can be determined *independently* of the camera position.\n\nSpecifically, if B's vertices are wholly above A's plane and A's vertices are wholly below B's plane, then A must be rendered before B. If the faces intersect in both directions, pre-sorting is impossible, requiring fallback to CPU sorting. Coplanar faces don't require sorting.\n\nThe algorithm involves checking every face against every other face, resulting in O(n²) complexity, suitable for static or mostly static translucent objects.  Since a total ordering is not always possible, the pre-sorting results in constraints between pairs of faces rather than a complete sorted list. The author suggests that only groups of faces that require CPU sorting need to be dynamically sorted, potentially improving performance. A Three.js demo is provided to illustrate the concept.\n",
    "chinese_title": "三维透明度预计算排序",
    "chinese_summary": "本文探讨了一种预先计算半透明3D面片渲染顺序的方法，以避免每帧基于相机距离的CPU排序。作者提出了一种基于面剔除和分析面片对之间空间关系的技术。\n\n其核心思想是将每个半透明面片分割成两个面，每个面的朝向彼此相反。通过分析面片B的顶点是否完全高于、完全低于、相交或共面于面片A的平面，反之亦然，可以*独立于*相机位置确定一个*部分*排序。\n\n具体而言，如果B的顶点完全高于A的平面，且A的顶点完全低于B的平面，则必须先渲染A，然后渲染B。如果面片在两个方向上都相交，则无法进行预排序，需要回退到CPU排序。共面面片不需要排序。\n\n该算法涉及检查每个面片与其他每个面片的关系，导致O(n²)的复杂度，适用于静态或大部分静态的半透明对象。由于并非总是能够实现完全排序，因此预排序会产生面片对之间的约束，而不是完整的排序列表。作者建议只需对需要CPU排序的面片组进行动态排序，从而可能提高性能。提供了一个Three.js演示来展示该概念。"
  },
  {
    "id": "44035793",
    "title": "Self-Hosting Moose with Docker Compose, Redis, Temporal, Redpanda and ClickHouse",
    "url": "https://docs.fiveonefour.com/moose/deploying/self-hosting/deploying-with-docker-compose",
    "summary": "This article is a comprehensive guide to self-hosting a Moose application (an application based on the Moose framework) on a single Ubuntu server using Docker Compose. It outlines the necessary steps for setting up a production-ready environment with dependencies like ClickHouse, Redis, Redpanda (optional for event streaming), and Temporal (optional for workflow orchestration).\n\nThe guide covers:\n\n*   **Prerequisites:** Ubuntu 24+, Docker, Docker Compose, and a server with adequate resources.\n*   **Installation:** Installing Docker, Node.js/Python, and configuring Docker log size limits.\n*   **Optional Setup:** Setting up a GitHub Actions runner for CI/CD and creating a basic Moose application if needed.\n*   **Deployment Preparation:** Creating `.env` and `.env.prod` files for component versions and application-specific secrets.\n*   **Docker Compose Deployment:** Creating a `docker-compose.yml` file to define and orchestrate the various services.\n*   **Production Configuration:** Securely configuring ClickHouse with dedicated user accounts and updating Moose's environment variables accordingly.\n\nThe article emphasizes the use of Docker Compose for simplified deployment and lifecycle management, highlighting the importance of security and proper configuration for each service. It also acknowledges the limitations of a single-server setup and briefly mentions Boreal, a managed HA deployment option for Moose. The guide includes detailed instructions and configuration examples for each step.\n",
    "chinese_title": "使用 Docker Compose 自托管 Moose，搭配 Redis、Temporal、Redpanda 和 ClickHouse",
    "chinese_summary": "本文是一份全面的指南，指导如何在单台Ubuntu服务器上使用Docker Compose自托管Moose应用程序（一个基于Moose框架的应用程序）。它概述了设置生产级环境所需的步骤，包括ClickHouse、Redis、Redpanda（可选，用于事件流）和Temporal（可选，用于工作流编排）等依赖项。\n\n本指南涵盖：\n\n*   **先决条件：** Ubuntu 24+、Docker、Docker Compose，以及一台具有足够资源的服务器。\n*   **安装：** 安装Docker、Node.js/Python，并配置Docker日志大小限制。\n*   **可选设置：** 设置GitHub Actions runner用于CI/CD，以及在需要时创建一个基本的Moose应用程序。\n*   **部署准备：** 创建 `.env` 和 `.env.prod` 文件，用于组件版本和应用程序特定的密钥。\n*   **Docker Compose部署：** 创建一个 `docker-compose.yml` 文件，用于定义和编排各种服务。\n*   **生产配置：** 安全地配置ClickHouse，包括创建专用用户帐户，并相应地更新Moose的环境变量。\n\n本文强调使用Docker Compose简化部署和生命周期管理，同时强调安全性和正确配置每个服务的重要性。它还承认单服务器设置的局限性，并简要提及Boreal，这是一种针对Moose的托管HA部署选项。本指南包括每个步骤的详细说明和配置示例。"
  },
  {
    "id": "44013000",
    "title": "Don't Use ISO/IEC 14977:1996 Extended Backus-Naur Form (EBNF) (2023)",
    "url": "https://dwheeler.com/essays/dont-use-iso-14977-ebnf.html",
    "summary": "David A. Wheeler argues against using ISO/IEC 14977:1996 Extended Backus-Naur Form (EBNF) for defining languages, citing serious technical flaws and inappropriate behavior by ISO.\n\nThe technical failings include its inability to represent International/Unicode characters or code points directly, lacking standard character range notation, requiring excessive commas making grammars hard to read, not building on widely used regex notation, and having a bizarre and easily misunderstood \"one or more\" notation. The specification is also challenging to understand with many key terms undefined.\n\nWheeler contrasts ISO/IEC 14977:1996 unfavorably with the EBNF notation from the W3C XML 1.0 specification, which addresses many of these shortcomings with features like direct Unicode code point representation, character ranges, and a clearer syntax. He also briefly mentions IETF's RFC 5234, noting it is better than 14977, but not ideal.\n\nBeyond technical issues, Wheeler criticizes ISO and IEC's practice of charging for IT standards, making them inaccessible and hindering their use, especially for small businesses and hobbyists. He notes that even ISO doesn't always use 14977 for its own language standards. He advocates for modern standards-setting practices like those of the W3C and IETF, which make their standards publicly available at no cost.\n",
    "chinese_title": "不要使用 ISO/IEC 14977:1996 扩展巴科斯-瑙尔范式 (EBNF) (2023)",
    "chinese_summary": "David A. Wheeler 认为不应使用 ISO/IEC 14977:1996 扩展巴科斯范式 (EBNF) 定义语言，理由是其存在严重的技术缺陷以及 ISO 的不当行为。\n\n技术缺陷包括无法直接表示国际/Unicode 字符或码位，缺乏标准字符范围表示法，需要过多的逗号导致语法难以阅读，没有建立在广泛使用的正则表达式表示法之上，以及具有一种奇怪且容易被误解的“一个或多个”表示法。 该规范也很难理解，其中许多关键术语未定义。\n\nWheeler 将 ISO/IEC 14977:1996 与 W3C XML 1.0 规范中的 EBNF 表示法进行了不利的对比，后者通过诸如直接 Unicode 码位表示、字符范围和更清晰的语法等功能解决了许多这些缺点。 他还简要提到了 IETF 的 RFC 5234，指出它比 14977 更好，但并非理想。\n\n除了技术问题之外，Wheeler 还批评了 ISO 和 IEC 对 IT 标准收费的做法，认为这使得标准难以获取并阻碍了其使用，特别是对于小型企业和业余爱好者而言。 他指出，即使 ISO 也不总是将 14977 用于其自身的语言标准。 他提倡像 W3C 和 IETF 这样的现代标准制定实践，这些组织免费公开提供其标准。"
  },
  {
    "id": "44041892",
    "title": "Authy corrupted my 2FA backup and all I got was this lousy blogpost",
    "url": "https://cmb.weblog.lol/2025/05/authy-corrupted-my-2fa-backup-and-all-i-got-was-this-lousy-blogpost",
    "summary": "The author experienced a major issue with Authy after restoring their iPhone from an iCloud backup due to a logic board replacement. Upon restoring Authy, they were prompted for a \"second backup password\" they never set, locking access to a significant portion of their 2FA codes, including critical accounts like AWS and GitLab.\n\nTroubleshooting on a new iPad installation made the problem worse, locking even more codes. Authy's support, initially a chatbot, offered unhelpful suggestions based on outdated information. Frustrated by the potential for permanent data loss and the lack of useful support, the author decided to migrate all their 2FA codes to 1Password. Fortunately, they had passkeys for key accounts, facilitating the migration.\n\nSerendipitously, a new Authy app update was released, and after installing it, the locked 2FA codes became accessible again with the original password. However, the author had already switched to 1Password, deeming Authy's reliability insufficient for handling sensitive 2FA information. The author highlights the risk of relying on a free tool with potentially neglected support, especially for critical security features like 2FA. The post serves as a warning to others who might encounter similar issues and emphasizes the importance of reliable 2FA management. While the new update appears to resolve the issue, the experience has led the author to seek a more robust, paid solution like 1Password.\n",
    "chinese_title": "Authy毁了我的双重验证备份，结果我就得到了这篇烂博客。",
    "chinese_summary": "作者更换iPhone主板后，从iCloud备份恢复Authy时遇到重大问题，被要求输入从未设置过的“第二备份密码”，导致大量2FA代码（包括AWS和GitLab等关键账户）被锁定。\n\n在新iPad上进行故障排除使问题更加严重，锁定了更多代码。Authy的支持团队（最初是聊天机器人）提供的建议基于过时信息，毫无帮助。由于可能永久丢失数据以及缺乏有效支持，作者感到沮丧，决定将所有2FA代码迁移到1Password。幸运的是，他们拥有关键账户的密钥，这有助于迁移。\n\n巧合的是，Authy发布了新的应用程序更新，安装后，锁定的2FA代码又可以通过原始密码访问。然而，作者已经切换到1Password，认为Authy的可靠性不足以处理敏感的2FA信息。作者强调了依赖可能被忽视支持的免费工具的风险，尤其是在像2FA这样的关键安全功能方面。这篇文章旨在警告可能遇到类似问题的其他人，并强调可靠的2FA管理的重要性。虽然新的更新似乎解决了这个问题，但这次经历促使作者寻求更强大、付费的解决方案，例如1Password。"
  },
  {
    "id": "44020832",
    "title": "InventWood is about to mass-produce wood that's stronger than steel",
    "url": "https://techcrunch.com/2025/05/12/inventwood-is-about-to-mass-produce-wood-thats-stronger-than-steel/",
    "summary": "InventWood, a startup licensing technology from the University of Maryland, is preparing to mass-produce \"Superwood,\" a treated wood product stronger than steel. Invented by materials scientist Liangbing Hu, the process, which now takes only hours, strengthens the cellulose in regular timber using \"food industry\" chemicals and compression, creating a material with superior tensile strength and strength-to-weight ratio compared to steel.\n\nInventWood has raised $15 million in Series A funding to build its first commercial plant, which will initially focus on facade materials for buildings. The long-term goal is to create structural beams that replace concrete and steel, significantly reducing the carbon footprint of construction. Superwood is Class A fire-rated, rot and pest resistant, and can be stabilized for outdoor use. The compression process also concentrates the natural colors of the wood, giving it a rich, tropical hardwood appearance. The company aims to produce structural beams of any dimension from wood chips, eliminating the need for finishing.\n",
    "chinese_title": "InventWood即将大规模生产强度超过钢铁的木材。",
    "chinese_summary": "InventWood公司（一家从马里兰大学获得技术授权的初创公司）正准备大规模生产“超级木材”，这是一种比钢更坚固的木材处理产品。该工艺由材料科学家胡良兵发明，现在只需几个小时，它使用“食品工业”化学品和压缩技术来强化普通木材中的纤维素，从而创造出一种具有优于钢的抗拉强度和强度重量比的材料。\n\nInventWood公司已筹集了1500万美元的A轮融资，用于建造其第一家商业工厂，该工厂最初将专注于建筑物的幕墙材料。该公司的长期目标是制造替代混凝土和钢材的结构梁，从而显著减少建筑业的碳足迹。“超级木材”的防火等级为A级，具有防腐防虫性，并且可以稳定用于户外。压缩过程还能浓缩木材的天然颜色，使其具有丰富的热带硬木外观。该公司的目标是用木屑生产任何尺寸的结构梁，从而无需进行表面处理。"
  },
  {
    "id": "44016059",
    "title": "Mary Queen of Scots Channel Anamorphosis, a 3D Simulation",
    "url": "https://www.charlespetzold.com/blog/2025/05/Mary-Queen-of-Scots-Channel-Anamorphosis-A-3D-Simulation.html",
    "summary": "This article details the author's process of creating a 3D simulation of a channel anamorphosis painting of Mary Queen of Scots found at the National Portrait Gallery in Edinburgh. Inspired by research for a book on logarithms, the author delves into the political context surrounding John Napier's work and stumbles upon the painting, which depicts a shifting image from a woman's face to a skull.\n\nThe painting, dating back to 1580, is a turning picture composed of prisms. The author's goal is to recreate this effect digitally using WebGL. He references a study confirming the prisms' angles are 45°, 45°, and 90°, and uses this information to construct a virtual prism panel.\n\nHe then utilizes two images of the painting provided by the National Galleries website under a personal-use license. Due to the images showing overlapping perspectives, he extracts and refines the images manually using a custom C# program, separating the face and skull perspectives. These refined images are then merged into alternating strips and overlaid onto the 3D prism panel. The final interactive result allows the viewer to swipe the image and observe the transformation from face to skull, mimicking the original painting's anamorphosis. The author also provides access to the JavaScript source code used for the 3D rendering.\n",
    "chinese_title": "苏格兰女王玛丽频道变形幻影3D模拟",
    "chinese_summary": "本文详细介绍了作者如何创建爱丁堡苏格兰国家肖像美术馆中玛丽女王频道变形画作的3D模拟过程。受对数书籍研究的启发，作者深入研究了约翰·纳皮尔作品的政治背景，偶然发现了这幅画作，该画作描绘了一个从女人脸到骷髅头的转变图像。\n\n这幅画作可以追溯到1580年，是一幅由棱镜组成的旋转画。作者的目标是使用WebGL以数字方式重现这种效果。他参考了一项研究，确认棱镜的角度为45°、45°和90°，并利用这些信息构建了一个虚拟棱镜面板。\n\n然后，他使用了国家美术馆网站提供的两张画作图像，并获得了个人使用许可。由于这些图像显示了重叠的视角，他使用自定义的C#程序手动提取和优化图像，分离出脸部和骷髅头视角。这些精炼后的图像随后被合并成交替的条纹，并叠加到3D棱镜面板上。最终的交互式结果允许观看者滑动图像，观察从脸到骷髅头的转变，模仿原始画作的变形效果。作者还提供了用于3D渲染的JavaScript源代码。"
  },
  {
    "id": "44005157",
    "title": "Rivers",
    "url": "https://www.futilitycloset.com/2025/05/15/rivers/",
    "summary": "This article defines \"rivers\" in typography as visually distracting streams of white space that can occur in justified text, particularly with monospaced fonts, when gaps between words align vertically across multiple lines. Typographers try to avoid them, sometimes reviewing pages upside down to better detect these visual artifacts. While long rivers are uncommon, the article highlights a 12-line example discovered in Darwin's \"The Voyage of the Beagle\" and mentions a collection of unusual rivers presented by Fritzi Striebel in a 1986 issue of Word Ways. Essentially, the article defines and illustrates an undesirable aesthetic problem in typesetting.\n",
    "chinese_title": "河流",
    "chinese_summary": "这篇文章将排版中的“河流”定义为在齐行文本中可能出现的视觉干扰性空白，尤其是在等宽字体中，当单词之间的间隙在多行中垂直对齐时。排字员会尽量避免它们，有时会将页面倒置以更好地检测这些视觉瑕疵。虽然长的“河流”并不常见，但文章强调了达尔文《贝格尔号航行记》中发现的一个12行示例，并提到了Fritzi Striebel在1986年《文字之路》杂志上发表的一系列不寻常的“河流”。本质上，这篇文章定义并说明了排版中一个不受欢迎的审美问题。"
  },
  {
    "id": "44016037",
    "title": "Mystical",
    "url": "https://suberic.net/~dmm/projects/mystical/README.html",
    "summary": "Mystical is a visual programming language concept that represents PostScript code as magical circles comprised of rings, sigils, and text. The code's structure is represented through rings that are circular bands with inner and outer borders. Different ring types represent PostScript's executable arrays, non-executable arrays, and dictionaries. Code flow follows a counter-clockwise direction.\n\nSigils, symbolic representations of operators, variables, or keywords, replace PostScript names. Strings are represented as cartouches. A set of standard sigils is defined for common PostScript operators, and users can create custom sigils for new functions and variables. A special ligature exists for defining named functions inside executable arrays.\n\nThe document provides examples of sigils and algorithms rendered in Mystical, highlighting its visual nature. Functions are available in \"mystical.ps\" to render PostScript code into Mystical diagrams, including scaling and layout adjustments. The current implementation is more of a visual representation of PostScript than a standalone executable language. The concept could potentially be applied to other operator-based languages like Forth, although languages with more complex syntax might pose challenges.\n",
    "chinese_title": "神秘的",
    "chinese_summary": "Mystical：一种将PostScript代码表示为由环、符号和文本构成的魔法圆圈的可视化编程语言概念。代码结构通过具有内外边界的环形带来表示。不同类型的环代表PostScript的可执行数组、非可执行数组和字典。代码流遵循逆时针方向。\n\n符号是运算符、变量或关键字的符号表示，取代了PostScript名称。字符串被表示为饰框。为常见的PostScript运算符定义了一组标准符号，用户可以为新函数和变量创建自定义符号。存在一种特殊的连字，用于在可执行数组中定义命名函数。\n\n本文档提供了以Mystical渲染的符号和算法示例，突出了其可视化特性。\"mystical.ps\"中提供了将PostScript代码渲染为Mystical图表的函数，包括缩放和布局调整。当前的实现更像是一种PostScript的可视化表示，而不是一种独立的、可执行的语言。这个概念有可能应用于其他基于运算符的语言，如Forth，但具有更复杂语法的语言可能会带来挑战。"
  },
  {
    "id": "44043582",
    "title": "Newspapers Are Recommending AI-Hallucinated Novels",
    "url": "https://countercraft.substack.com/p/newspapers-are-recommending-ai-hallucinated",
    "summary": "Okay, I have read the article. Here's a summary:\n\nThe article \"Newspapers Are Recommending AI-Hallucinated Novels\" by Countercraft exposes a disturbing trend: newspapers and literary publications are recommending and reviewing AI-generated books, sometimes without realizing their true origin. The author highlights several examples of books that were either explicitly written by AI or heavily augmented by AI tools, and subsequently praised in reputable outlets.\n\nThe core issue is that these AI-generated books often contain factual inaccuracies, nonsensical plot points, and inconsistencies, which the author labels \"hallucinations,\" echoing the term used to describe AI fabrication. Despite these flaws, the books were presented as legitimate works of fiction by human authors and, in some cases, lauded for their apparent creativity and originality.\n\nThe article criticizes the lack of critical scrutiny applied to these books, suggesting that reviewers may be prioritizing novelty or failing to properly assess the quality of the writing. It also raises ethical concerns about transparency and authorship in the age of AI. The author argues that it's crucial for reviewers and readers alike to be aware of the potential for AI involvement in book creation and to apply appropriate standards of evaluation, especially regarding factual accuracy and coherence. Ultimately, the article warns that the uncritical acceptance of AI-generated books could undermine the credibility of literary criticism and the value of human creativity.\n",
    "chinese_title": "报纸正在推荐人工智能虚构的小说",
    "chinese_summary": "好的，我已阅读了这篇文章。以下是摘要：\n\nCountercraft的文章《报纸正在推荐人工智能“幻想”小说》揭露了一个令人不安的趋势：报纸和文学刊物正在推荐和评论人工智能生成的书籍，有时甚至没有意识到它们的真正来源。作者重点介绍了几个例子，这些书要么是由人工智能明确编写的，要么是大量使用人工智能工具辅助创作的，随后在信誉良好的媒体上受到称赞。\n\n核心问题是，这些人工智能生成的书籍通常包含事实错误、无意义的情节和不一致之处，作者将其称为“幻想”，与用来描述人工智能捏造信息的术语相呼应。尽管存在这些缺陷，这些书还是被当作人类作者创作的合法小说来呈现，并且在某些情况下，因其明显的创造力和原创性而受到赞扬。\n\n这篇文章批评了对这些书籍缺乏批判性审查，暗示评论家可能优先考虑新颖性，或者未能正确评估写作质量。它还提出了关于人工智能时代透明度和作者身份的伦理问题。作者认为，对于评论家和读者来说，了解人工智能参与书籍创作的可能性并应用适当的评估标准至关重要，尤其是在事实准确性和连贯性方面。最终，这篇文章警告说，不加批判地接受人工智能生成的书籍可能会损害文学批评的信誉和人类创造力的价值。"
  },
  {
    "id": "44033009",
    "title": "Show HN: A native Hacker News reader with integrated todo/done tracking",
    "url": "https://github.com/haojiang99/hacker_news_reader",
    "summary": "This Show HN introduces a native desktop Hacker News reader built with Rust and egui. It offers a clean, modern interface for browsing various HN sections (Hot, New, Show, Ask, Jobs, Best) with features like threaded comments, auto-folding, adjustable font sizes, search/filtering, dark/light mode, offline caching, and favoriting.\n\nKey features include color-coded stories based on score, keyboard shortcuts for navigation and actions (like marking items as Todo/Done with `T` and `D`), and a side panel for managing favorites. The application displays up to 150 stories per section, showing essential information like title, domain, author, score, time posted, and comment count. The comment view provides a threaded display with collapsible threads, adjustable font sizes, author highlighting, and preserved HTML formatting.\n\nInstallation requires Rust (1.70+) and Cargo. The application architecture consists of a UI layer (main.rs), data models (models.rs) for stories and comments, and an HN client (hn_client.rs) using `reqwest` and `scraper` for data fetching and parsing. The project is licensed under MIT and acknowledges Hacker News, egui, reqwest, and scraper for their contributions.\n",
    "chinese_title": "Show HN: 一款集成了待办/完成跟踪的原生 Hacker News 阅读器",
    "chinese_summary": "这个 Show HN 介绍了一个用 Rust 和 egui 构建的原生桌面 Hacker News 阅读器。它提供了一个简洁、现代的界面，用于浏览各种 HN 版块（热门、最新、展示、提问、招聘、最佳），具有诸如线程评论、自动折叠、可调节字体大小、搜索/过滤、暗/亮模式、离线缓存和收藏等功能。\n\n主要功能包括基于分数进行颜色编码的故事、用于导航和操作的键盘快捷键（例如使用“T”和“D”将项目标记为待办/完成），以及用于管理收藏的侧边栏。该应用程序每个版块最多显示 150 个故事，显示标题、域名、作者、分数、发布时间和评论数等基本信息。评论视图提供了一个带有可折叠线程的线程显示、可调节字体大小、作者突出显示和保留的 HTML 格式。\n\n安装需要 Rust (1.70+) 和 Cargo。应用程序架构包括一个 UI 层 (main.rs)、用于故事和评论的数据模型 (models.rs) 以及一个使用 `reqwest` 和 `scraper` 进行数据获取和解析的 HN 客户端 (hn_client.rs)。该项目在 MIT 许可下获得许可，并感谢 Hacker News、egui、reqwest 和 scraper 的贡献。"
  },
  {
    "id": "44014323",
    "title": "Show HN: A MCP server to evaluate Python code in WASM VM using RustPython",
    "url": "https://github.com/tuananh/hyper-mcp/tree/main/examples/plugins/eval-py",
    "summary": "This \"Show HN\" post introduces `hyper-mcp`, a project by tuananh which offers a server that can evaluate Python code within a WebAssembly (WASM) Virtual Machine using RustPython. In essence, it's a service allowing you to run Python code in a sandboxed WASM environment.\n\nThe project is open-source and hosted on GitHub under the repository `tuananh/hyper-mcp`. It's garnered considerable interest, indicated by the 533 stars and 35 forks at the time of the post.\n\nWhile the post itself is brief, the project likely addresses scenarios where secure and isolated Python code execution is needed, potentially for user-submitted scripts, sandboxed applications, or running Python in environments where a traditional Python interpreter isn't suitable. The combination of WASM for security and RustPython for the Python runtime makes it a potentially powerful and versatile solution.\n",
    "chinese_title": "Show HN: 一个使用 RustPython 在 WASM VM 中评估 Python 代码的 MCP 服务器",
    "chinese_summary": "这篇“Show HN”帖子介绍了一个由tuananh开发的名为`hyper-mcp`的项目。该项目提供一个服务器，可以使用RustPython在WebAssembly (WASM) 虚拟机中评估Python代码。本质上，它是一项服务，允许您在沙盒化的WASM环境中运行Python代码。\n\n该项目是开源的，托管在GitHub上的仓库`tuananh/hyper-mcp`中。它已经引起了相当大的兴趣，截至发帖时，已获得533颗星和35个分支。\n\n虽然帖子本身很简短，但该项目可能解决了需要安全隔离地执行Python代码的场景，可能用于用户提交的脚本、沙盒应用程序或在传统Python解释器不适用的环境中运行Python。WASM的安全性和RustPython的Python运行时结合使其成为一种潜在的强大且通用的解决方案。"
  },
  {
    "id": "44038168",
    "title": "Cleo, the mathematician that tricked Stack Exchange",
    "url": "https://en.wikipedia.org/wiki/Cleo_(mathematician)",
    "summary": "Cleo was an anonymous user on the Mathematics Stack Exchange (Math.SE) from 2013 to 2015 who gained notoriety for providing correct, but proof-less, answers to complex integration problems. The user's identity and abilities sparked significant debate, with speculation ranging from individual genius to an early AI system.\n\nCleo's profile claimed to be a female mathematician with a medical condition limiting detailed explanations. Over two years, Cleo posted 39 answers, often solving problems quickly that had stumped others. One notable instance involved solving a complex integral with the closed-form solution including the golden ratio, without providing any intermediate steps. While correct, the lack of explanation caused controversy within the community.\n\nVarious theories emerged about Cleo's identity, including speculation that it was a famous mathematician like Maryam Mirzakhani or even an AI. An investigation by a Reddit user in 2023 linked Cleo to multiple accounts, including those of Vladimir Reshetnikov and Laila Podlesny.\n\nIn 2025, Joe McCann's YouTube investigation led to the revelation of Cleo's true identity: Vladimir Reshetnikov, a software developer originally from Uzbekistan. Reshetnikov confirmed his identity after a viewer discovered a shared email recovery address between related accounts. Reshetnikov stated that he created the Cleo persona to stimulate interest in mathematical problems that received little attention on the forum and to encourage others to develop their own problem-solving approaches.\n",
    "chinese_title": "欺骗Stack Exchange的数学家克莱奥",
    "chinese_summary": "克莱奥（Cleo）是2013年至2015年间数学栈溢出（Math.SE）上的匿名用户，因提供正确但无证明的复杂积分问题答案而闻名。该用户的身份和能力引发了激烈的争论，人们猜测其身份从个人天才到早期人工智能系统不等。\n\n克莱奥的个人资料声称是一位患有疾病而无法详细解释的女数学家。两年多来，克莱奥发布了39个答案，经常迅速解决那些难倒其他人的问题。一个著名的例子是解决了一个包含黄金比例封闭形式解的复杂积分，而没有提供任何中间步骤。虽然答案正确，但缺乏解释引起了社区内的争议。\n\n关于克莱奥身份的各种理论层出不穷，包括猜测她可能是像玛丽安·米尔扎哈尼这样的著名数学家，甚至是人工智能。Reddit用户在2023年进行的一项调查将克莱奥与多个账户联系起来，包括Vladimir Reshetnikov和Laila Podlesny的账户。\n\n2025年，Joe McCann的YouTube调查揭示了克莱奥的真实身份：Vladimir Reshetnikov，一位来自乌兹别克斯坦的软件开发人员。在一名观众发现相关账户之间共享的电子邮件恢复地址后，Reshetnikov确认了自己的身份。Reshetnikov表示，他创建克莱奥这个角色是为了激发人们对论坛上很少受到关注的数学问题的兴趣，并鼓励其他人发展自己的问题解决方法。"
  },
  {
    "id": "43993311",
    "title": "Llama from scratch (2023)",
    "url": "https://blog.briankitano.com/llama-from-scratch/",
    "summary": "This article, \"Llama from scratch (2023),\" documents one person's journey implementing a simplified version of the Llama language model to train on the TinyShakespeare dataset. It emphasizes an iterative, engineering-focused approach.\n\nThe author advocates for starting small, testing components individually, and building up to the full model. Key tips include writing helper functions for quantitative evaluation (data splits, loss plotting), qualitatively evaluating the model, using `.shape` and assertions for debugging, working out solutions without matrix multiplication initially, and testing layers with specific properties.\n\nThe article walks through setting up a basic feed-forward neural network as a baseline and then progressively incorporating features from the Llama architecture. The author initially encounters issues with the base model, highlighting the importance of understanding tensor shapes and using the correct loss function (cross-entropy). After fixing the loss function, they add a generation function for qualitative evaluation.\n\nThe article then plans to incorporate three specific features of the Llama architecture: RMSNorm for pre-normalization, Rotary embeddings, and the SwiGLU activation function, starting with RMSNorm. The author provides code snippets for the initial setup and testing of the RMSNorm layer to ensure it functions as expected, emphasizing the importance of validating each component.\n",
    "chinese_title": "从零开始的Llama (2023)",
    "chinese_summary": "从零实现Llama (2023): 一人构建简化版Llama并用TinyShakespeare数据集训练的历程记录。文章强调迭代和工程化的方法。\n\n作者提倡从小处着手，单独测试各个组件，逐步构建完整模型。关键技巧包括编写辅助函数进行量化评估（数据分割、损失绘图），对模型进行定性评估，使用`.shape`和断言进行调试，最初在没有矩阵乘法的情况下解决问题，以及测试具有特定属性的层。\n\n文章逐步介绍如何建立一个基本的feed-forward神经网络作为基线，然后逐步纳入Llama架构的特性。作者最初遇到了基础模型的问题，突出了理解张量形状和使用正确的损失函数（交叉熵）的重要性。在修复损失函数后，他们添加了一个生成函数进行定性评估。\n\n文章随后计划纳入Llama架构的三个具体特性：用于预归一化的RMSNorm，旋转嵌入和SwiGLU激活函数，首先从RMSNorm开始。作者提供了RMSNorm层初始设置和测试的代码片段，以确保其功能符合预期，强调了验证每个组件的重要性。"
  },
  {
    "id": "44024784",
    "title": "KDE is finally getting a native virtual machine manager called “Karton”",
    "url": "https://www.neowin.net/news/kde-is-finally-getting-a-native-virtual-machine-manager-called-karton/",
    "summary": "KDE Plasma is getting a native virtual machine manager called \"Karton,\" aiming to provide a more integrated experience than current solutions like virt-manager or GNOME Boxes. The project, initially started by Aaron Rainbolt and later taken over by Harald Sitter, is now being actively developed by Derek Lin as a Google Summer of Code (GSoC) project.\n\nKarton is built using Qt Quick and Kirigami, leveraging the libvirt API to manage virtual machines and potentially support multiple platforms. Current development focuses on core functionality, including a new domain installer using libosinfo for precise OS image detection and libvirt XML generation. Lin is also creating a custom SPICE viewer using Qt Quick.\n\nLin's GSoC deliverables include features like configuring VMs via libvirt XML, a user-friendly UI with fine-tuned control options, virtual machine snapshots, an intuitive VM display, a convergent UI (mobile-friendly), improved VM status updates, a browse tool for OS selection, resource usage graphs, and the ability to connect to QEMU with both session and system privileges, potentially enabling support for non-KVM backends.\n\nThe GSoC coding period runs from June 2, 2025, with a planned working app by mid-July and final submission by September 1, 2025.\n",
    "chinese_title": "KDE 终于要迎来一个名为“Karton”的原生虚拟机管理器了",
    "chinese_summary": "KDE Plasma将推出原生虚拟机管理器“Karton”，旨在提供比virt-manager或GNOME Boxes等现有方案更集成化的体验。该项目最初由Aaron Rainbolt发起，后由Harald Sitter接手，目前由Derek Lin作为Google Summer of Code (GSoC)项目积极开发。\n\nKarton使用Qt Quick和Kirigami构建，利用libvirt API管理虚拟机，并可能支持多个平台。当前开发重点是核心功能，包括使用libosinfo进行精确操作系统镜像检测和libvirt XML生成的新域安装程序。Lin还在使用Qt Quick创建自定义SPICE查看器。\n\nLin的GSoC交付成果包括通过libvirt XML配置虚拟机、具有微调控制选项的用户友好型UI、虚拟机快照、直观的虚拟机显示、聚合UI（移动友好）、改进的虚拟机状态更新、用于操作系统选择的浏览工具、资源使用情况图表以及使用会话和系统权限连接到QEMU的能力，从而可能支持非KVM后端。\n\nGSoC编码期从2025年6月2日开始，计划在7月中旬完成可用的应用程序，并在2025年9月1日提交最终版本。"
  },
  {
    "id": "44029619",
    "title": "Visualizing 100k Years of Earth in WebGL",
    "url": "https://technistuff.com/posts/visualizing-100000-years-of-earth-in-webgl/",
    "summary": "This article details the creation of an interactive WebGL model visualizing Earth's geography over the last 100,000 years, focusing on the impact of climate and sea level changes on human history. The author used scientific datasets for elevation (ETOPO), sea levels (NOAA Paleoclimatology Program), climate (rainfall & temperature simulations), and ice sheet locations (Global ice sheet reconstruction).\n\nThe process involved significant data processing, including downsampling elevation data, compressing sea level information into a binary file, and encoding climate data into a 2D texture array for GPU loading. The model is rendered using THREE.js and custom shaders that dynamically adjust land/water colors based on sea level, temperature, and rainfall data.\n\nIce sheet locations are rendered by converting NetCDF data into triangulated meshes using flood fill, edge detection, smoothing, Delaunay triangulation, and then projecting these triangles onto a sphere. Modern country borders are added using the World Administrative Boundaries dataset, also smoothed for efficient rendering.\n\nThe resulting interactive globe allows users to visualize changing landscapes, such as the Bering Strait land bridge and Doggerland, providing insights into human migration and submerged archaeological sites. The author hopes to enhance the model with more accurate data, historical events, and further rendering optimizations. A live demo is available.\n",
    "chinese_title": "用 WebGL 可视化地球的十万年",
    "chinese_summary": "本文详细介绍了如何创建一个交互式WebGL模型，以可视化地球过去10万年的地理环境，重点关注气候和海平面变化对人类历史的影响。作者使用了科学数据集，包括海拔（ETOPO）、海平面（NOAA 古气候学项目）、气候（降雨和温度模拟）以及冰盖位置（全球冰盖重建）。\n\n该过程涉及大量数据处理，包括降采样海拔数据、将海平面信息压缩成二进制文件，并将气候数据编码为2D纹理数组以供GPU加载。该模型使用THREE.js和自定义着色器进行渲染，这些着色器可以根据海平面、温度和降雨数据动态调整陆地/水体的颜色。\n\n冰盖位置的渲染方法是将NetCDF数据转换为三角网格，使用的技术包括洪水填充、边缘检测、平滑、Delaunay三角剖分，然后将这些三角形投影到球体上。现代国家边界是使用世界行政边界数据集添加的，并进行了平滑处理以提高渲染效率。\n\n最终生成的交互式地球模型允许用户可视化不断变化的景观，例如白令海峡陆桥和多格兰，从而深入了解人类迁徙和淹没的考古遗址。作者希望通过更准确的数据、历史事件和进一步的渲染优化来增强该模型。提供在线演示。"
  },
  {
    "id": "44032470",
    "title": "Show HN: Windows 98 themed website in 1 HTML file for my post punk band",
    "url": "https://corp.band",
    "summary": "This \"Show HN\" post introduces a Windows 98 themed website built in a single HTML file for the post-punk band CORP. The website acts as a central hub for the band's online presence and content.\n\nThe site is designed with interactive \"windows\" that users can open and close, simulating the classic Windows 98 interface. These windows provide access to various sections like:\n\n*   **Music:** Links to CORP's music on Spotify, Apple Music, YouTube Music, and Bandcamp. Includes a highlighted track, \"Whispers from the Water Cooler.\"\n*   **Events:** Links to upcoming shows via Songkick and Bandsintown.\n*   **Social:** Links to the band's Instagram, LinkedIn, YouTube, and Facebook accounts, with a cryptic corporate-themed message encouraging followers.\n*   **Merch:** Offers CORP-branded merchandise for sale on Bandcamp.\n*   **Mailing List:** A sign-up form to join the band's mailing list.\n*   **Booking:** Contact information for booking CORP and a link to their Electronic Press Kit.\n*   **Recycle Bin:** An amusingly empty recycle bin.\n*   **Classified:** A section for \"executive eyes only\" with options to view the source code, toggle dark mode, activate fullscreen, and run a surveillance protocol (likely a joke).\n*   **Links:** Direct links to specific songs and CORP's LinkedIn page.\n\nThe overall tone is ironic and corporate-satirical, reflecting the band's post-punk aesthetic. The website's design and content contribute to a unique and engaging online experience for fans.\n",
    "chinese_title": "展示HN：为我的后朋克乐队制作的Windows 98主题单HTML文件网站",
    "chinese_summary": "这个\"Show HN\"帖子介绍了一个以Windows 98为主题的网站，该网站用单个HTML文件构建，专为后朋克乐队CORP而设计。 该网站作为乐队在线形象和内容的中心枢纽。\n\n该网站设计有可供用户打开和关闭的互动“窗口”，模拟经典的Windows 98界面。 这些窗口提供对各种部分的访问，例如：\n\n*   **音乐:** CORP在Spotify、Apple Music、YouTube Music和Bandcamp上的音乐链接。 包括一首精选曲目“Whispers from the Water Cooler”。\n*   **活动:** 通过Songkick和Bandsintown链接到即将举行的演出。\n*   **社交:** 乐队的Instagram、LinkedIn、YouTube和Facebook账户的链接，并附有一条神秘的、公司主题的消息，鼓励粉丝关注。\n*   **商品:** 在Bandcamp上出售CORP品牌商品。\n*   **邮件列表:** 用于注册加入乐队邮件列表的表格。\n*   **预订:** CORP的预订联系方式以及其电子新闻资料包的链接。\n*   **回收站:** 一个有趣的空回收站。\n*   **机密:** 一个“仅限行政人员”的部分，其中包含查看源代码、切换暗黑模式、激活全屏和运行监视协议（可能只是个玩笑）的选项。\n*   **链接:** 直接链接到特定歌曲和CORP的LinkedIn页面。\n\n整体基调是讽刺性的和公司讽刺性的，反映了乐队的后朋克美学。 网站的设计和内容为粉丝们创造了独特的、引人入胜的在线体验。"
  },
  {
    "id": "44042370",
    "title": "Evidence suggests a single hoaxer created 'Piltdown man' (2016)",
    "url": "https://royalsocietypublishing.org/doi/10.1098/rsos.160328",
    "summary": "The 2016 Royal Society Open Science article \"Evidence suggests a single hoaxer created 'Piltdown man'\" uses advanced techniques in forensic science and statistical analysis to provide strong evidence that the infamous Piltdown Man hoax was perpetrated by a single individual.\n\nPrevious research had identified multiple culprits contributing to the forgery. However, this study re-examines the evidence, focusing on the microscopic analysis of tool marks on the fabricated bones (a human skull and an orangutan jaw).\n\nUsing techniques such as X-ray microtomography and multivariate statistical analysis, the researchers meticulously analyzed the modifications made to the fossil fragments. Their findings indicate a consistent pattern of modification and skill, suggesting a single hand was responsible for altering both the skull and the jaw. The study identifies consistent use of specific tools and abrasion techniques across all the fraudulent specimens.\n\nWhile the study doesn't definitively name the hoaxer, it significantly narrows down the possibilities. The authors suggest that this strengthens the case for a primary suspect, namely Martin Hinton, a zoologist at the Natural History Museum in London. Hinton had a motive – being disgruntled with his treatment at the museum – and he left behind a trunk full of chemically treated bones that were remarkably similar to the Piltdown specimens.\n\nThe study concludes that the uniformity of the fabrication techniques provides compelling evidence against the involvement of multiple individuals, reinforcing the likelihood of a single, sophisticated hoaxer behind the Piltdown Man deception. The techniques used provide a much more detailed and statistically robust analysis than previous studies, leading to a stronger conclusion about the nature of the hoax.\n",
    "chinese_title": "证据表明是单一的骗局制造者创造了“皮尔当人”（2016年）",
    "chinese_summary": "2016年英国皇家学会开放科学文章《证据表明“皮尔当人”系一人伪造》运用法医学和统计分析的先进技术，提供了强有力的证据，表明臭名昭著的皮尔当人骗局是由一个人所为。\n\n先前的研究曾认为有多个罪魁祸首参与了伪造。然而，这项研究重新审查了证据，重点是对伪造骨骼（一个人类头骨和一个猩猩下颚）上的工具痕迹进行微观分析。\n\n研究人员利用X射线显微断层扫描和多元统计分析等技术，仔细分析了对化石碎片所做的修改。他们的发现表明，修改和技巧具有一致的模式，表明是同一只手负责改变了头骨和下颚。该研究指出，所有欺诈性标本都一致使用了特定的工具和磨损技术。\n\n虽然这项研究没有明确指出造假者是谁，但它大大缩小了可能性。作者认为，这加强了对主要嫌疑人马丁·辛顿（伦敦自然历史博物馆的动物学家）的指控。辛顿有一个动机——对他在博物馆受到的待遇不满——而且他留下了一个装满化学处理过的骨头的箱子，这些骨头与皮尔当标本非常相似。\n\n该研究得出结论，制造技术的一致性提供了令人信服的证据，反对多人参与，从而加强了皮尔当人欺骗背后存在一个单一、老练的造假者的可能性。与之前的研究相比，所使用的技术提供了更详细和更具统计学意义的分析，从而对骗局的性质得出了更强的结论。"
  },
  {
    "id": "44038433",
    "title": "Allow us to block Copilot-generated issues (and PRs) from our own repositories",
    "url": "https://github.com/orgs/community/discussions/159749",
    "summary": "This GitHub Community post expresses concern regarding the introduction of Copilot-generated issues and pull requests, arguing that these could become a burden for maintainers and a violation of project code of conducts due to potentially low-quality, AI-generated content. The original poster (mcclure) requests a feature to block Copilot-generated submissions on specific repositories or across entire accounts.\n\nThe request stems from GitHub's announcement of public previews for creating issues with Copilot and a Copilot coding agent. mcclure argues that dealing with unwanted AI-generated content would waste maintainers' time, issue submitters' time (who may not be aware their submissions are AI-generated), and GitHub's server resources. The user suggests a simple blocking mechanism, such as blocking the \"copilot\" bot, but notes that the bot appears to be exempt from blocking. The user threatens to migrate to alternative platforms like Codeberg if a solution isn't provided.\n\nOther users echo these concerns, with one mentioning the need for legal protection against AI exposure in code. Concerns about a potential DDOS-like flood of AI-generated issues and PRs are also raised. Some users propose alternative solutions such as opting-in/opting-out of Copilot on repositories, or migrating to platforms like Codeberg and Gitea (though Gitea's AI policy is also questioned).  It's also noted that the problem applies to the standard \"Create new issue\" page leading to issues that are entirely hallucinated by the AI. The general sentiment is that GitHub should provide tools to manage or disable Copilot's issue/PR generation features on a per-repository basis.\n",
    "chinese_title": "允许我们阻止 Copilot 从我们自己的存储库生成问题（和 PR）。",
    "chinese_summary": "这篇GitHub社区帖子表达了对引入Copilot生成的议题和拉取请求的担忧，认为这些内容可能因潜在的低质量、AI生成内容而成为维护者的负担，并违反项目行为准则。发帖人(mcclure)请求添加一个功能，以阻止在特定存储库或整个帐户中提交Copilot生成的内容。\n\n该请求源于GitHub宣布公开预览使用Copilot创建议题和Copilot编码代理。mcclure认为，处理不需要的AI生成内容会浪费维护者、议题提交者（他们可能不知道自己的提交是AI生成的）以及GitHub服务器资源的时间。该用户建议采用一种简单的阻止机制，例如阻止“copilot”机器人，但指出该机器人似乎不受阻止限制。该用户威胁说，如果问题得不到解决，将迁移到像Codeberg这样的替代平台。\n\n其他用户也表达了同样的担忧，其中一位提到需要针对代码中的AI暴露提供法律保护。还提出了对AI生成的问题和PR可能出现的类似DDOS攻击的担忧。一些用户提出了替代解决方案，例如选择加入/退出存储库上的Copilot，或迁移到像Codeberg和Gitea这样的平台（尽管Gitea的AI策略也受到质疑）。 此外，还有人指出，这个问题也适用于标准的“创建新议题”页面，导致出现完全由AI臆想的议题。总体而言，大家的共识是，GitHub应提供工具来管理或禁用每个存储库的Copilot议题/PR生成功能。"
  },
  {
    "id": "44036519",
    "title": "Old Growth Wood",
    "url": "https://brenthull.com/article/old-growth-wood",
    "summary": "This article highlights the significant differences in quality and durability between old-growth and new-growth wood, particularly in the context of window construction and restoration. Old-growth wood, harvested primarily between the 1870s and 1940s from virgin forests, grew slowly over a long period, resulting in tight growth rings (20-25 per inch) and a high proportion of heartwood. This slow growth translates to superior stability, durability, and longevity. Windows made from old-growth wood can last for over 100 years with proper care.\n\nIn contrast, new-growth wood, often sourced from fast-growing plantations like those cultivating Radiata Pine, exhibits fewer growth rings per inch (around 7), a lower proportion of heartwood, and is more prone to warping and rotting within just 20 years. The article points to the use of finger-jointed wood as a consequence of this lower quality, where manufacturers attempt to compensate for defects in new-growth lumber.\n\nThe author argues that the superior quality of old-growth wood is a compelling reason to restore historic windows rather than replace them with new ones made from inferior materials. By restoring these old windows, their structural integrity can be maintained, and they can potentially last for another century.\n",
    "chinese_title": "原始森林木材",
    "chinese_summary": "本文重点介绍了老木和新木在质量和耐用性方面的显著差异，尤其是在窗户建造和修复方面。老木主要于1870年代至1940年代从原始森林中采伐，生长缓慢，因此生长轮紧密（每英寸20-25个），心材比例高。这种缓慢生长转化为卓越的稳定性、耐用性和寿命。用老木制成的窗户，如果保养得当，可以使用100年以上。\n\n相比之下，新木通常来自生长迅速的种植园，例如种植辐射松的种植园，每英寸的生长轮较少（约7个），心材比例较低，并且更容易在短短20年内翘曲和腐烂。本文指出，指接木的使用是这种低质量的结果，制造商试图以此弥补新木材的缺陷。\n\n作者认为，老木的卓越品质是修复历史悠久的窗户而不是用劣质材料制成的新窗户取而代之的有力理由。通过修复这些旧窗户，可以保持其结构完整性，并且有可能再使用一个世纪。"
  },
  {
    "id": "44031009",
    "title": "Too Much Go Misdirection",
    "url": "https://flak.tedunangst.com/post/too-much-go-misdirection",
    "summary": "The author is frustrated with Go's indirect handling of `io.Reader` when a `[]byte` representation is already available, specifically in image decoding. The goal is to avoid unnecessary memory copies when a function expects an `io.Reader` but the data is already in a byte slice.\n\nThe author's initial attempt to directly access the underlying byte slice of a `bytes.Reader` using `unsafe.Pointer` proved insufficient because the `image.Decode` function pre-emptively wraps the `io.Reader` in a `bufio.Reader` if it doesn't implement `Peek`.\n\nThe core problem: `bytes.Reader` doesn't implement `Peek`, and `bufio.Reader` doesn't expose its underlying reader. This forces a copy of the data if the original input was a `bytes.Reader`.\n\nThe workaround involves using `unsafe.Pointer` again to access the internal fields of the `bufio.Reader` and extract the underlying reader, then checking if it's a `bytes.Reader` to finally access the byte slice directly.\n\nThe author criticizes Go's structural typing and the standard library's reliance on type assertions for performance optimization, creating \"shadow APIs\" where standard types are implicitly favored. While casting has its uses, its prevalence in the standard library suggests design flaws that lead to overly complex workarounds. The optimization strategy doesn't scale well to third-party types.\n",
    "chinese_title": "过多的Go语言误导",
    "chinese_summary": "作者对 Go 在图像解码中处理 `io.Reader` 的方式感到沮丧，尤其是在已经有 `[]byte` 表示的情况下。目标是避免在函数需要 `io.Reader` 但数据已经在字节切片中时进行不必要的内存复制。\n\n作者最初尝试使用 `unsafe.Pointer` 直接访问 `bytes.Reader` 的底层字节切片，但事实证明这还不够，因为如果 `image.Decode` 函数没有实现 `Peek`，它会抢先将 `io.Reader` 包装在 `bufio.Reader` 中。\n\n核心问题：`bytes.Reader` 没有实现 `Peek`，而 `bufio.Reader` 没有公开其底层读取器。如果原始输入是 `bytes.Reader`，这将强制复制数据。\n\n解决方法涉及再次使用 `unsafe.Pointer` 来访问 `bufio.Reader` 的内部字段并提取底层读取器，然后检查它是否为 `bytes.Reader` 以最终直接访问字节切片。\n\n作者批评 Go 的结构类型和标准库对类型断言的依赖以进行性能优化，从而创建“影子 API”，其中标准类型被隐式地偏爱。虽然类型转换有其用途，但它在标准库中的普遍性表明设计缺陷导致过于复杂的回避方法。这种优化策略无法很好地扩展到第三方类型。"
  },
  {
    "id": "44040082",
    "title": "Activism, hacking or campaigning: Why it's so easy to win the vote at Eurovision",
    "url": "https://english.elpais.com/technology/2025-05-19/activism-hacking-or-campaigning-why-its-so-easy-to-win-the-popular-vote-at-eurovision.html",
    "summary": "This article explores the vulnerabilities of Eurovision's televoting system, highlighting how easily it can be manipulated by strategically mobilized groups. The 2025 contest saw Austrian singer JJ win, but questions arose regarding the public vote which heavily favored the Israeli singer Yuval Raphael, despite criticism of Israel's actions in Gaza.\n\nThe article details how Eurovision's voting allows multiple votes per person using different email addresses and bank cards, making it susceptible to coordinated efforts. This is demonstrated by screenshots from the Eurovision website explicitly stating that each payment card can be used for one transaction, encouraging users to use multiple cards to vote again.\n\nExperts criticize the system, stating it's not a true \"vote\" because eligibility isn't clearly defined, and repeat voting isn't prevented. The article explains that while most people vote based on musical preference, a focused group can strategically vote en masse for their chosen candidate, swaying the results.\n\nThe article also touches on the possibility of hacking or coordinated social media campaigns but finds no concrete evidence of these in the 2025 contest. Ultimately, the vulnerability of Eurovision's voting system lies in its emphasis on ease of access and payment over security and verifiability, allowing relatively small, determined groups to significantly impact the outcome. The expert quoted suggests that until electronic voting systems are fully transparent and auditable, they won't be trusted in serious elections and are prone to manipulation.\n",
    "chinese_title": "行动主义、黑客行为或竞选活动：为何在欧洲歌唱大赛中如此容易赢得选票",
    "chinese_summary": "本文探讨了欧洲歌唱大赛电视投票系统的漏洞，强调了该系统如何容易被有组织地动员起来的团体操纵。2025年的比赛中，奥地利歌手JJ获胜，但公众投票引发了质疑，因为以色列歌手Yuval Raphael获得了极高的公众支持，尽管以色列在加沙的行动备受批评。\n\n文章详细描述了欧洲歌唱大赛的投票机制允许每人使用不同的电子邮件地址和银行卡进行多次投票，使其容易受到协同操作的影响。欧洲歌唱大赛网站的截图证实了这一点，截图明确指出每张支付卡只能用于一次交易，鼓励用户使用多张卡再次投票。\n\n专家批评该系统，认为它并非真正的“投票”，因为资格没有明确定义，并且没有阻止重复投票。文章解释说，虽然大多数人根据音乐偏好投票，但专注的团体可以有策略地大规模投票给他们选择的候选人，从而影响结果。\n\n文章还提到了黑客攻击或协调的社交媒体活动的可能性，但未在2025年的比赛中发现任何具体证据。归根结底，欧洲歌唱大赛投票系统的漏洞在于其强调易用性和支付而非安全性和可验证性，这使得相对较小且坚定的团体能够显著影响结果。引用的专家表示，除非电子投票系统完全透明且可审计，否则它们将不会在严肃的选举中获得信任，并且容易被操纵。"
  },
  {
    "id": "44007373",
    "title": "Experimentation Matters: Why Nuenki isn't using pairwise evaluations",
    "url": "https://nuenki.app/blog/experimentation_matters_why_we_arent_using_pairwise",
    "summary": "Nuenki's author details their shift away from a pairwise evaluation system for their language translation quality benchmark due to prohibitively high costs. The original system used LLMs to score other LLM outputs, but a new pairwise evaluation benchmark utilizing a Bradley-Terry model proved too expensive, costing hundreds or even thousands of dollars per language for a single experiment.\n\nWhile the author preferred pairwise comparisons from a scientific perspective, the inability to afford sufficient experiments led to the development of a compromise system. This new system evaluates translations of ~160 sentences from various models using six different translation evaluation systems. These systems judge and rank each translation, scoring them from 0-100 based on various factors. The scores are then combined and analyzed statistically, incorporating controls such as duplicate consolidation, randomization, and blinding.\n\nThis new system is significantly cheaper, costing approximately $6 for an early test run on German while producing results with good p-values. The system differs from the original by relying on a single metric instead of three, and it's not fully blinded as the comparison models see all translations during scoring. The author invites readers to try Nuenki, a language learning tool.\n",
    "chinese_title": "实验至关重要：为什么Nuenki不使用成对评估",
    "chinese_summary": "Nuenki作者详述了由于成本过高，他们放弃了用于其语言翻译质量基准的双向评估系统。最初的系统使用大型语言模型(LLM)为其他LLM的输出评分，但一个新的、利用布拉德利-特里模型的双向评估基准成本过于昂贵，单次实验每个语言就要花费数百甚至数千美元。\n\n尽管作者从科学的角度更倾向于双向比较，但由于无力承担足够的实验，他们开发了一种折衷方案。这个新系统使用六种不同的翻译评估系统来评估来自各种模型的约160个句子的翻译。这些系统判断并对每个翻译进行排名，根据各种因素将其评分从0到100。然后将这些分数组合并进行统计分析，并纳入诸如重复数据合并、随机化和盲法等控制。\n\n这个新系统成本显著降低，在德语的早期测试运行中花费约6美元，同时产生了具有良好p值的结果。该系统与原始系统不同之处在于它依赖于单一指标而不是三个，并且它不是完全盲法的，因为比较模型在评分期间会看到所有翻译。作者邀请读者试用Nuenki，一款语言学习工具。"
  },
  {
    "id": "44020591",
    "title": "Spaced repetition systems have gotten better",
    "url": "https://domenic.me/fsrs/",
    "summary": "This article discusses the advancements in spaced repetition systems (SRS), specifically highlighting the FSRS (Free Spaced Repetition Scheduler) algorithm developed by Jarrett Ye. SRS are software programs that use flashcards and spaced intervals to help users memorize information. The core idea is to review material \"just before you forget it\" to maximize long-term retention.\n\nThe article contrasts FSRS with older algorithms like SuperMemo-2, which used arbitrary rules for scheduling reviews, leading to potential frustration and inefficiency. FSRS, on the other hand, leverages machine learning to model difficulty, stability, and retrievability for each card, allowing it to predict the optimal review intervals based on the user's individual learning patterns. It aims to schedule reviews when the recall probability drops to a desired retention rate (default 90%).\n\nFSRS offers significant improvements, including lighter review loads, less frustration from missed cards (no more resetting to day 1), and greater confidence in knowledge retention. It allows users to adjust the desired retention rate to optimize the workload-to-knowledge ratio, potentially leading to memorizing more cards with less daily review time.\n\nThe article notes that FSRS has been integrated into Anki, a popular SRS software, but isn't enabled by default. It also criticizes the algorithms used by subscription services like WaniKani and Bunpro as being less effective than even older algorithms. The author emphasizes the value of mastering Anki and taking advantage of FSRS for efficient learning and long-term knowledge retention. The article concludes with a list of resources for those interested in learning more about spaced repetition and FSRS.\n",
    "chinese_title": "间隔重复系统变得更好了",
    "chinese_summary": "本文探讨了间隔重复系统 (SRS) 的进展，特别突出了 Jarrett Ye 开发的 FSRS (自由间隔重复调度器) 算法。SRS 是使用抽认卡和间隔重复来帮助用户记忆信息的软件程序。其核心思想是在“即将忘记之前”复习材料，以最大限度地提高长期记忆效果。\n\n本文将 FSRS 与 SuperMemo-2 等较旧的算法进行了对比，后者使用任意规则安排复习，导致潜在的挫败感和效率低下。另一方面，FSRS 利用机器学习来建模每张卡片的难度、稳定性和可检索性，使其能够根据用户的个人学习模式预测最佳复习间隔。它的目标是在回忆概率降至所需保持率（默认 90%）时安排复习。\n\nFSRS 提供了显著的改进，包括更轻的复习负担、减少错过卡片带来的挫败感（不再重置为第 1 天）以及对知识保持的更大信心。它允许用户调整所需的保持率，以优化工作量与知识的比率，从而有可能以更少的每日复习时间记住更多的卡片。\n\n文章指出，FSRS 已集成到流行的 SRS 软件 Anki 中，但默认情况下未启用。文章还批评了 WaniKani 和 Bunpro 等订阅服务使用的算法，认为它们的效果甚至不如较旧的算法。作者强调了掌握 Anki 并利用 FSRS 进行高效学习和长期知识保持的价值。文章最后列出了一些资源，供有兴趣了解更多关于间隔重复和 FSRS 的人参考。"
  },
  {
    "id": "43998075",
    "title": "About Asteroids, Atari's biggest arcade hit",
    "url": "https://www.goto10retro.com/p/about-asteroids-ataris-biggest-arcade",
    "summary": "This article is a retrospective on Asteroids, Atari's biggest arcade hit. The author, Paul Lefebvre, discusses the game's conception as a follow-up to Space Invaders and highlights its simple yet complex gameplay. Unlike Space Invaders' simple movement, Asteroids allowed players to move freely around the screen, rotating and shooting in any direction while managing inertia. The game also featured asteroids that broke into smaller, faster pieces, and a UFO that roamed the entire screen.\n\nThe article emphasizes the game's use of vector graphics, a departure from the raster graphics of the time, resulting in higher resolution and a distinctive glowing effect. While the author admits struggling with the arcade version's button controls, he enjoyed the cocktail table version and later, Asteroids Deluxe with its shield button.\n\nThe article further covers the Atari 2600 version, noting its blockier graphics and different sounds but praising its addictive gameplay. It also touches on versions for the Atari 7800 and 8-bit computers, as well as clones like Megaroids for the Atari ST. Finally, the author mentions creating his own Asteroids-inspired game called Space Rocks using Xojo, available on GitHub.\n",
    "chinese_title": "关于小行星，雅达利最成功的街机游戏",
    "chinese_summary": "本文回顾了雅达利最成功的街机游戏《小行星》。作者保罗·勒菲弗尔讨论了这款游戏作为《太空侵略者》续作的构思，并强调了其简单而复杂的游戏玩法。与《太空侵略者》的简单移动不同，《小行星》允许玩家在屏幕上自由移动，向任何方向旋转和射击，同时控制惯性。游戏中还有会分裂成更小、更快碎片的行星，以及在整个屏幕上漫游的 UFO。\n\n本文强调了该游戏对矢量图形的使用，这与当时的栅格图形不同，从而实现了更高的分辨率和独特的发光效果。作者承认自己在街机版的按钮控制方面遇到了困难，但他很喜欢鸡尾酒桌版本，以及后来的带有护盾按钮的《小行星豪华版》。\n\n本文还介绍了雅达利 2600 版本，指出其图形更加块状化，声音也不同，但赞扬了其令人上瘾的游戏玩法。它还提到了雅达利 7800 和 8 位计算机的版本，以及诸如 Atari ST 的 Megaroids 等克隆版本。最后，作者提到了他使用 Xojo 创建的、受《小行星》启发的游戏《太空岩石》，该游戏可在 GitHub 上找到。"
  },
  {
    "id": "44038686",
    "title": "Not causal chains, but interactions and adaptations",
    "url": "https://surfingcomplexity.blog/2025/05/19/not-causal-chains-but-interactions-and-adaptations/",
    "summary": "Lorin Hochstein critiques Root Cause Analysis (RCA) as an incident investigation method, arguing it's based on a flawed model of complex system failures. While acknowledging some valuable aspects of RCA, like recognizing multiple contributing factors and the importance of understanding how work is actually done, Hochstein believes the RCA's focus on a linear causal chain (root cause -> underlying cause -> immediate cause -> incident) is an oversimplification.\n\nInstead, Hochstein advocates for a Resilience Engineering (RE) model. This model emphasizes interactions between system components as the primary driver of incidents, depicting it as a complex web rather than a chain. The RE model recognizes that unexpected interactions, rather than individual component failures, enable incidents. It highlights that modern software systems, with their intricate feedback loops, are inherently unpredictable.\n\nFurthermore, the RE model emphasizes that complex systems are constantly adapting to latent failures, and this adaptability is what makes them function despite the presence of faults. Instead of solely focusing on eliminating root causes, the RE approach aims to understand and nurture sources of fault tolerance and adaptive capacity within the system. This means understanding how the system typically compensates for faults, and why those adaptations failed in specific incident circumstances. The goal is to improve the system's ability to adapt to future, unpredictable combinations of latent failures. The article suggests moving from \"finding the generators of faults and removing them\" to \"finding the sources of fault tolerance and growing them\".\n",
    "chinese_title": "并非因果链，而是互动与适应",
    "chinese_summary": "Lorin Hochstein 批判根本原因分析（RCA）作为事件调查方法，认为它基于对复杂系统故障的错误模型。Hochstein 承认 RCA 的一些有价值的方面，例如识别多个促成因素以及理解工作实际运作方式的重要性，但他认为 RCA 关注线性因果链（根本原因 -> 潜在原因 -> 直接原因 -> 事件）是一种过度简化。\n\n相反，Hochstein 提倡弹性工程（RE）模型。该模型强调系统组件之间的交互是事件的主要驱动因素，将其描述为一个复杂的网络而不是一条链。RE 模型认为，意外的交互，而不是单个组件的故障，导致了事件的发生。它强调，现代软件系统及其复杂的反馈回路本质上是不可预测的。\n\n此外，RE 模型强调复杂系统不断适应潜在的故障，而这种适应性使得它们能够在存在故障的情况下正常运行。RE 方法不是仅仅关注消除根本原因，而是旨在理解和培育系统内部的容错来源和适应能力。这意味着理解系统通常如何补偿故障，以及为什么这些适应在特定事件情况下失败。目标是提高系统适应未来、不可预测的潜在故障组合的能力。文章建议从“寻找故障的根源并消除它们”转变为“寻找容错的来源并发展它们”。"
  },
  {
    "id": "44029142",
    "title": "ClawPDF – Open-Source Virtual/Network PDF Printer with OCR and Image Support",
    "url": "https://github.com/clawsoftware/clawPDF",
    "summary": "ClawPDF is an open-source virtual printer for Windows that offers advanced features typically found in enterprise solutions. It allows users to create documents in various formats, including PDF, PDF/A, PDF/X, images (PNG, JPEG, TIF), SVG, OCR, and TXT. It supports metadata management, password protection with up to 256-bit AES encryption, and includes a scripting interface for automation.\n\nA key feature is its ability to be installed on a print server for network printing. ClawPDF is compatible with major Windows client and server OS (x86/x64/ARM64) and multi-user environments.\n\nKey features include printing to various formats, 100% valid PDF/A creation, OCR, scripting interface support (Python, Powershell, VBScript), shared network printing, SVG export, drag-and-drop support, file merging, command-line support, silent printing, custom paper sizes, light/dark theme, ARM64 and full Unicode support, multiple profiles, and post actions.\n\nThe software offers easy deployment, many settings, and is free of adware, spyware, and nagware. It has been tested under various Windows Server and client editions. Command-line options are available for batch printing and configuration management. Version 0.9.3 addresses bug fixes, including issues with shared network printing and Windows 7 compatibility.\n\nClawPDF relies on several open-source libraries with various licenses, including PDFCreator, iText7, and Ghostscript, ensuring a wide range of functionalities. It is licensed under AGPL v3.\n",
    "chinese_title": "ClawPDF – 开源虚拟/网络PDF打印机，支持OCR和图像",
    "chinese_summary": "ClawPDF 是一款适用于 Windows 的开源虚拟打印机，提供通常在企业级解决方案中才能找到的高级功能。它允许用户创建各种格式的文档，包括 PDF、PDF/A、PDF/X、图像（PNG、JPEG、TIF）、SVG、OCR 和 TXT。它支持元数据管理、高达 256 位 AES 加密密码保护，并包含用于自动化的脚本接口。\n\n一个关键特性是它可以安装在打印服务器上用于网络打印。ClawPDF 兼容主要的 Windows 客户端和服务器操作系统（x86/x64/ARM64）以及多用户环境。\n\n主要功能包括打印到各种格式、100% 有效的 PDF/A 创建、OCR、脚本接口支持（Python、Powershell、VBScript）、共享网络打印、SVG 导出、拖放支持、文件合并、命令行支持、静默打印、自定义纸张尺寸、浅色/深色主题、ARM64 和完整 Unicode 支持、多个配置文件和后处理操作。\n\n该软件易于部署，具有许多设置，并且没有广告软件、间谍软件和恶意软件。它已经在各种 Windows Server 和客户端版本下进行了测试。命令行选项可用于批量打印和配置管理。0.9.3 版本解决了错误修复，包括共享网络打印和 Windows 7 兼容性问题。\n\nClawPDF 依赖于几个具有各种许可证的开源库，包括 PDFCreator、iText7 和 Ghostscript，确保了广泛的功能。它在 AGPL v3 许可下授权。"
  },
  {
    "id": "44026201",
    "title": "The principles of database design, or, the Truth is out there",
    "url": "https://ebellani.github.io/blog/2025/the-principles-of-database-design-or-the-truth-is-out-there/",
    "summary": "Eduardo Bellani's article, \"The Principles of Database Design, or, the Truth is out there,\" argues for a formal, principle-based approach to database design. He posits that databases represent reality through propositions and that effective database design is crucial for accurately reflecting business semantics. He criticizes the reliance on ad-hoc methods, highlighting potential problems like update anomalies and data inconsistencies stemming from a lack of formal training.\n\nBellani presents several design principles, drawing from the work of McGoveran and Pascal: the Principle of Orthogonal Design (POOD), Principle of Representational Parsimony (PORP), Principle of Expressive Completeness (POEC), Principle of Full Normalization (POFN), the Information Principle (TIP), and the Principle of Logical Independence (PLI). He then introduces his own proposed principle: the Principle of Essential Denotation (PED), which emphasizes using natural keys rooted in domain semantics rather than surrogate keys for identifying relations.\n\nThe author demonstrates the PED with SQL code snippets contrasting the use of surrogate keys versus national IDs as primary keys in a citizen table. He argues that using surrogate keys disconnects the database structure from the domain's semantic meaning.\n\nThe article concludes by emphasizing the importance of good database design for technical stability and semantic clarity in information systems. He stresses that databases should be built to accurately reflect the truth, requiring rigor and a firm understanding of foundational principles.\n",
    "chinese_title": "数据库设计原则，或者，真相就在那里",
    "chinese_summary": "爱德华多·贝拉尼的文章《数据库设计原则，或真相就在那里》主张采用一种正式的、基于原则的数据库设计方法。他认为数据库通过命题来表示现实，而有效的数据库设计对于准确反映业务语义至关重要。他批评了对临时方法的依赖，强调了由于缺乏正规培训而导致的更新异常和数据不一致等潜在问题。\n\n贝拉尼提出了几个设计原则，借鉴了麦戈文和帕斯卡的工作：正交设计原则（POOD）、表征简约原则（PORP）、表达完整性原则（POEC）、完全规范化原则（POFN）、信息原则（TIP）和逻辑独立性原则（PLI）。然后，他介绍了自己提出的原则：本质指称原则（PED），该原则强调使用植根于领域语义的自然键，而不是代理键来标识关系。\n\n作者通过SQL代码片段演示了PED，对比了在公民表中使用代理键与国民身份证作为主键的情况。他认为，使用代理键会将数据库结构与领域的语义意义断开连接。\n\n文章最后强调了良好的数据库设计对于信息系统的技术稳定性和语义清晰度的重要性。他强调，数据库的构建应准确反映真相，需要严谨性和对基本原则的深刻理解。"
  },
  {
    "id": "44003530",
    "title": "Solving the local optima problem – NQueens",
    "url": "https://github.com/Dpbm/n-rainhas/blob/main/readme-en.md",
    "summary": "This Github repository, \"n-rainhas\" by Dpbm, tackles the classic N-Queens problem and likely explores or attempts to solve the local optima problem often encountered when using heuristic algorithms to find solutions.\n\nThe N-Queens problem involves placing N chess queens on an N×N chessboard so that no two queens threaten each other; thus, a solution requires that no two queens share the same row, column, or diagonal.\n\nThe \"local optima problem\" in the context of the N-Queens problem refers to situations where a search algorithm (like hill-climbing or simulated annealing) gets stuck in a configuration that is better than its immediate neighbors but not the best possible solution (global optimum). The algorithm may be unable to escape this \"local optimum\" because moving to any adjacent configuration makes the situation temporarily worse, even though a better solution might exist further away.\n\nTherefore, the repository likely investigates strategies or algorithms designed to escape these local optima in order to find a valid solution for the N-Queens problem. These strategies might include:\n\n*   **Random restarts:** Restarting the search from a different random configuration.\n*   **Simulated annealing:** Allowing the algorithm to accept some \"worse\" moves with a probability that decreases over time.\n*   **Other optimization techniques:** Employing specialized algorithms designed to navigate complex search spaces.\n\nThe existence of \"Fork 1\" and \"Star 3\" suggests that other users have found the code or content potentially valuable or interesting, perhaps for its implementation or the specific approach used to solve the local optima issue in the N-Queens problem.\n",
    "chinese_title": "解决局部最优问题——N皇后",
    "chinese_summary": "Dpbm的Github仓库“n-rainhas”解决了经典的N皇后问题，并可能探索或尝试解决在使用启发式算法寻找解决方案时经常遇到的局部最优问题。\n\nN皇后问题涉及将N个国际象棋皇后放置在N×N的棋盘上，使任何两个皇后都不能互相威胁；因此，一个解决方案要求任何两个皇后都不在同一行、同一列或同一对角线上。\n\nN皇后问题中的“局部最优问题”指的是，搜索算法（如爬山算法或模拟退火算法）陷入一种配置，该配置比其直接相邻的配置更好，但不是最佳解决方案（全局最优）。 算法可能无法摆脱这种“局部最优”，因为移动到任何相邻配置都会暂时使情况变得更糟，即使更远的距离可能存在更好的解决方案。\n\n因此，该仓库可能研究旨在逃脱这些局部最优的策略或算法，以便找到N皇后问题的有效解决方案。 这些策略可能包括：\n\n*   **随机重启：** 从不同的随机配置重新开始搜索。\n*   **模拟退火：** 允许算法以随时间降低的概率接受一些“更差”的移动。\n*   **其他优化技术：** 采用专门的算法来导航复杂的搜索空间。\n\n“Fork 1”和“Star 3”的存在表明，其他用户可能发现该代码或内容有价值或有趣，可能是因为它的实现或用于解决N皇后问题中局部最优问题的特定方法。"
  },
  {
    "id": "44032717",
    "title": "Microsoft's ICC blockade: digital dependence comes at a cost",
    "url": "https://www.techzine.eu/news/privacy-compliance/131536/microsofts-icc-blockade-digital-dependence-comes-at-a-cost/",
    "summary": "This article highlights the risks of relying on US IT services, specifically Microsoft, in the face of geopolitical tensions, using the recent sanctions against the International Criminal Court (ICC) as a prime example. Due to US sanctions over the ICC's investigation into Israeli Prime Minister Netanyahu, Chief Prosecutor Karim Khan has been blocked from accessing his Microsoft email account and had his bank accounts frozen, effectively paralyzing the ICC's operations.\n\nThe article argues that while European governments, like the Dutch, previously deemed the advantages of Microsoft's Azure and 365 services to outweigh the sovereignty risks, the ICC incident demonstrates a significant vulnerability. It raises concerns about the US potentially demanding access to European data and the possibility of Microsoft being forced to comply with US policy changes, even if it means circumventing encryption or losing in court.\n\nThe author warns that non-US governments must have alternative plans beyond relying solely on Microsoft for critical functions. The article poses the hypothetical scenario of the US retaliating against the Dutch government for disagreeing on US policy, potentially blocking civil servants' Microsoft-linked accounts. The core argument emphasizes that national security should not depend on service level agreements and highlights the ICC's current search for European alternatives, reflecting broader concerns about Europe's digital autonomy. The article concludes by questioning the readiness, security, and sovereignty of alternative services compared to established players like Microsoft.\n",
    "chinese_title": "微软的ICC封锁：数字依赖是有代价的",
    "chinese_summary": "本文以近期美国制裁国际刑事法院（ICC）为例，强调在地缘政治紧张局势下，依赖美国IT服务，特别是微软，所存在的风险。由于美国因国际刑事法院调查以色列总理内塔尼亚胡而对其进行制裁，首席检察官卡里姆·汗的微软电子邮件账户被封锁，银行账户被冻结，实际上瘫痪了国际刑事法院的运作。\n\n文章指出，尽管荷兰等欧洲国家政府此前认为微软Azure和365服务的优势超过了主权风险，但国际刑事法院事件表明存在重大漏洞。文章还提出了对美国可能要求访问欧洲数据，以及微软可能被迫遵守美国政策变化，即使这意味着绕过加密或在法庭上败诉的担忧。\n\n作者警告说，非美国政府必须有替代计划，而不能仅仅依赖微软来执行关键职能。文章提出了假设情景，即如果荷兰政府不同意美国政策，美国可能会采取报复行动，阻止公务员的微软相关账户。核心论点强调，国家安全不应依赖于服务水平协议，并强调国际刑事法院目前正在寻找欧洲替代方案，这反映了人们对欧洲数字自主权的广泛担忧。文章最后质疑与微软等老牌企业相比，替代服务的就绪性、安全性和主权。"
  },
  {
    "id": "44041563",
    "title": "Red Hat partners with SiFive for a RISC-V developer preview of RHEL 10",
    "url": "https://www.redhat.com/en/blog/red-hat-partners-with-sifive-for-risc-v-developer-preview-for-red-hat-enterprise-linux-10",
    "summary": "Red Hat and SiFive are partnering to release a developer preview of Red Hat Enterprise Linux (RHEL) 10 on the SiFive HiFive Premier P550 platform, a popular choice for RISC-V development, with downloads available starting June 1, 2025. RISC-V is an open-source instruction set architecture (ISA) that allows anyone to build microprocessors, and Red Hat's early adoption signifies their commitment to open hardware and supporting market choices in open source.\n\nThe collaboration aims to allow customers evaluating RISC-V to assess its suitability for IT infrastructure, workloads, and edge/embedded applications. Red Hat will also release the binary image, the source code of their Linux 6.12 kernel implementation, and their upstream contributions, stimulating development of RISC-V based solutions. A CentOS Stream 10 version supporting RISC-V is also being released.\n\nThe SiFive HiFive Premier P550 development board is presented as the easiest way to run the RHEL 10 developer preview on RISC-V. This initiative aligns with Red Hat's commitment to open and transparent enterprise software development.\n",
    "chinese_title": "红帽与SiFive合作推出RHEL 10的RISC-V开发者预览版",
    "chinese_summary": "红帽与SiFive合作发布红帽企业Linux (RHEL) 10开发者预览版，该版本将在SiFive HiFive Premier P550平台上运行，这是RISC-V开发的热门选择。下载将于2025年6月1日开始提供。RISC-V是一种开源指令集架构(ISA)，允许任何人构建微处理器，红帽的早期采用标志着他们对开放硬件的承诺以及对开源市场选择的支持。\n\n此次合作旨在使评估RISC-V的客户能够评估其在IT基础设施、工作负载和边缘/嵌入式应用中的适用性。红帽还将发布二进制镜像、其Linux 6.12内核实现的源代码以及其上游贡献，从而刺激基于RISC-V的解决方案的开发。同时还将发布支持RISC-V的CentOS Stream 10版本。\n\nSiFive HiFive Premier P550开发板被认为是运行RISC-V上的RHEL 10开发者预览版的最简单方法。此举符合红帽对开放和透明的企业软件开发的承诺。"
  },
  {
    "id": "44026799",
    "title": "Show HN: Goboscript, text-based programming language, compiles to Scratch",
    "url": "https://github.com/aspizu/goboscript",
    "summary": "Goboscript is a text-based programming language that compiles to Scratch projects. It aims to provide a more efficient and feature-rich way to create Scratch projects compared to the visual block-based editor. Users can write code in a text editor, use version control, refactor code, and easily share or reuse code snippets.\n\nKey features include a concise syntax, integration with external tooling for tasks like costume generation, a powerful macro system, local variables for custom blocks, and optimizations/problem detection.\n\nGoboscript builds upon previous projects like Tosh and Boiga, and evolved from an earlier Python implementation called gobomatic. The current implementation is written in Rust.\n\nThe project is open-source and welcomes contributions. Development instructions are provided, including using `cargo run` and a `tools/run` script for compiling, validating, uncompiling, and patching Scratch projects. It also includes instructions for validating external sb3 files.\n\nGoboscript was a first-place winner at FOSS HACK 25, a hackathon organized by the FOSS United Foundation.\n",
    "chinese_title": "Show HN: Goboscript，文本编程语言，编译成Scratch",
    "chinese_summary": "Goboscript 是一种基于文本的编程语言，可编译为 Scratch 项目。它旨在提供一种比可视化块编辑器更高效、功能更丰富的 Scratch 项目创建方式。用户可以在文本编辑器中编写代码，使用版本控制，重构代码，并轻松共享或重用代码片段。\n\n主要功能包括简洁的语法、与外部工具的集成（用于服装生成等任务）、强大的宏系统、自定义块的局部变量以及优化/问题检测。\n\nGoboscript 基于 Tosh 和 Boiga 等之前的项目构建，并由早期名为 gobomatic 的 Python 实现演变而来。当前的实现是用 Rust 编写的。\n\n该项目是开源的，欢迎贡献。提供了开发说明，包括使用 `cargo run` 和 `tools/run` 脚本来编译、验证、反编译和修补 Scratch 项目。它还包括验证外部 sb3 文件的说明。\n\nGoboscript 在由 FOSS United Foundation 组织的黑客马拉松 FOSS HACK 25 中荣获第一名。"
  },
  {
    "id": "44039232",
    "title": "Show HN: Hover Effects TS – ASCII, Lego, and glitch hover effects using canvas",
    "url": "https://www.npmjs.com/package/hover-effects-ts",
    "summary": "\"Hover Effects TS\" is a TypeScript library offering a variety of performant, canvas-based hover effects for images. It features effects like ASCII art, zoom, particle dust, pixelation, Minecraft-style blocks, and Lego blocks. The library is customizable with interactive controls, allowing real-time parameter adjustments for each effect.\n\nInstallation is simple via npm, yarn, or pnpm.  Applying effects requires only a few lines of code, supporting single or multiple images, and CSS selectors. Users can import specific effects to minimize bundle size. The library emphasizes performance, recommending optimized image sizes, careful parameter tuning, and proper cleanup with `destroy()`.\n\nThe documentation provides example usage guides, best practices for initialization (loading images first, using UI control values), and debugging tips. Each effect includes demos, available controls, and parameter ranges. The library supports modern browsers and offers integration examples for React. The MIT license allows free usage. Recent versions include bug fixes, performance improvements, enhanced TypeScript support, and new features like the Lego effect.\n",
    "chinese_title": "Show HN: Hover Effects TS – 使用 Canvas 实现的 ASCII、乐高和故障悬停效果",
    "chinese_summary": "Hover Effects TS 是一个 TypeScript 库，为图像提供各种基于 canvas 的高性能悬停效果。它包括 ASCII 艺术、缩放、粒子尘埃、像素化、Minecraft 风格方块和乐高积木等效果。该库可定制，具有交互式控件，允许实时调整每个效果的参数。\n\n可以通过 npm、yarn 或 pnpm 轻松安装。应用效果只需几行代码，支持单个或多个图像以及 CSS 选择器。用户可以导入特定的效果以最大限度地减小捆绑包大小。该库强调性能，建议优化图像大小、仔细调整参数并使用 `destroy()` 进行适当的清理。\n\n文档提供了示例用法指南、初始化最佳实践（首先加载图像、使用 UI 控件值）和调试技巧。每种效果都包含演示、可用控件和参数范围。该库支持现代浏览器，并提供 React 的集成示例。MIT 许可证允许免费使用。最近的版本包括错误修复、性能改进、增强的 TypeScript 支持以及乐高效果等新功能。"
  },
  {
    "id": "44030873",
    "title": "23andMe Sells Gene-Testing Business to DNA Drug Maker Regeneron",
    "url": "https://www.bloomberg.com/news/articles/2025-05-19/23andme-sells-gene-testing-business-to-dna-drug-maker-regeneron",
    "summary": "Unable to access the article link.\n",
    "chinese_title": "23andMe将基因检测业务出售给DNA药物制造商Regeneron",
    "chinese_summary": "无法访问文章链接。"
  },
  {
    "id": "44032115",
    "title": "Go Cryptography Security Audit",
    "url": "https://go.dev/blog/tob-crypto-audit",
    "summary": "This Go blog post announces the results of an independent security audit conducted by Trail of Bits on Go's core cryptography packages. The audit, which covered key exchange, digital signatures, encryption, hashing, key derivation, authentication, and the cryptographic random number generator, revealed one low-severity finding and five informational findings.\n\nThe low-severity issue (TOB-GOCL-3) was related to memory management in the legacy and unsupported Go+BoringCrypto integration and has been fixed in Go 1.25. The informational findings included potential timing side-channels (TOB-GOCL-1, TOB-GOCL-2, TOB-GOCL-6), a misuse risk in an internal API (TOB-GOCL-4), and a missing check for an impractical limit (TOB-GOCL-5). All informational findings have also been addressed in Go 1.25. Notably, TOB-GOCL-2, a timing side-channel in P-256 ECDSA, was assigned CVE-2025-22866 and impacted Power ISA targets.\n\nThe post highlights Go's commitment to secure cryptography through complexity limitation, thorough testing, safe APIs, and code readability. The move towards a native FIPS 140-3 mode written in pure Go, replacing the Go+BoringCrypto integration, is emphasized. Future plans include implementing post-quantum cryptography and introducing easier-to-use high-level cryptography APIs.\n",
    "chinese_title": "Go 密码学安全审计",
    "chinese_summary": "这篇 Go 博客文章宣布了 Trail of Bits 对 Go 核心密码学软件包进行的独立安全审计结果。该审计涵盖了密钥交换、数字签名、加密、哈希、密钥派生、身份验证和密码学随机数生成器，发现了一项低危漏洞和五项信息性发现。\n\n该低危问题（TOB-GOCL-3）与遗留且不受支持的 Go+BoringCrypto 集成中的内存管理有关，已在 Go 1.25 中修复。信息性发现包括潜在的时序侧信道（TOB-GOCL-1、TOB-GOCL-2、TOB-GOCL-6）、内部 API 中的误用风险（TOB-GOCL-4）以及对不切实际限制的缺失检查（TOB-GOCL-5）。所有信息性发现也已在 Go 1.25 中得到解决。值得注意的是，P-256 ECDSA 中的时序侧信道 TOB-GOCL-2 被分配了 CVE-2025-22866，并影响了 Power ISA 目标。\n\n文章强调了 Go 通过限制复杂性、彻底测试、安全 API 和代码可读性，致力于安全密码学。文章强调了转向使用纯 Go 编写的原生 FIPS 140-3 模式，以取代 Go+BoringCrypto 集成。未来的计划包括实施后量子密码学并引入更易于使用的高级密码学 API。"
  }
]